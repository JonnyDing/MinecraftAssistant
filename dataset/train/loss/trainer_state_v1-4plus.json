{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 6970,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.007173601147776184,
      "grad_norm": 0.9194042682647705,
      "learning_rate": 9.99994921055553e-06,
      "loss": 2.5959,
      "step": 10
    },
    {
      "epoch": 0.014347202295552367,
      "grad_norm": 0.9801968932151794,
      "learning_rate": 9.999796843253935e-06,
      "loss": 2.6264,
      "step": 20
    },
    {
      "epoch": 0.021520803443328552,
      "grad_norm": 1.382014274597168,
      "learning_rate": 9.999542901190683e-06,
      "loss": 2.5168,
      "step": 30
    },
    {
      "epoch": 0.028694404591104734,
      "grad_norm": 1.4780302047729492,
      "learning_rate": 9.999187389524804e-06,
      "loss": 2.5466,
      "step": 40
    },
    {
      "epoch": 0.035868005738880916,
      "grad_norm": 1.454967737197876,
      "learning_rate": 9.998730315478793e-06,
      "loss": 2.4209,
      "step": 50
    },
    {
      "epoch": 0.043041606886657105,
      "grad_norm": 1.882156491279602,
      "learning_rate": 9.998171688338463e-06,
      "loss": 2.4935,
      "step": 60
    },
    {
      "epoch": 0.05021520803443329,
      "grad_norm": 1.3684985637664795,
      "learning_rate": 9.99751151945276e-06,
      "loss": 2.5063,
      "step": 70
    },
    {
      "epoch": 0.05738880918220947,
      "grad_norm": 1.410757303237915,
      "learning_rate": 9.99674982223353e-06,
      "loss": 2.4287,
      "step": 80
    },
    {
      "epoch": 0.06456241032998565,
      "grad_norm": 1.7888482809066772,
      "learning_rate": 9.995886612155243e-06,
      "loss": 2.2896,
      "step": 90
    },
    {
      "epoch": 0.07173601147776183,
      "grad_norm": 1.587293028831482,
      "learning_rate": 9.994921906754683e-06,
      "loss": 2.4114,
      "step": 100
    },
    {
      "epoch": 0.07890961262553801,
      "grad_norm": 1.3949594497680664,
      "learning_rate": 9.99385572563059e-06,
      "loss": 2.245,
      "step": 110
    },
    {
      "epoch": 0.08608321377331421,
      "grad_norm": 1.9184200763702393,
      "learning_rate": 9.992688090443263e-06,
      "loss": 2.2285,
      "step": 120
    },
    {
      "epoch": 0.09325681492109039,
      "grad_norm": 1.394659399986267,
      "learning_rate": 9.991419024914121e-06,
      "loss": 2.2262,
      "step": 130
    },
    {
      "epoch": 0.10043041606886657,
      "grad_norm": 2.1594996452331543,
      "learning_rate": 9.990048554825214e-06,
      "loss": 2.1958,
      "step": 140
    },
    {
      "epoch": 0.10760401721664276,
      "grad_norm": 1.5954833030700684,
      "learning_rate": 9.988576708018712e-06,
      "loss": 2.1859,
      "step": 150
    },
    {
      "epoch": 0.11477761836441894,
      "grad_norm": 2.0988643169403076,
      "learning_rate": 9.987003514396323e-06,
      "loss": 2.1667,
      "step": 160
    },
    {
      "epoch": 0.12195121951219512,
      "grad_norm": 2.02583646774292,
      "learning_rate": 9.985329005918702e-06,
      "loss": 2.0931,
      "step": 170
    },
    {
      "epoch": 0.1291248206599713,
      "grad_norm": 1.8307210206985474,
      "learning_rate": 9.983553216604792e-06,
      "loss": 2.0266,
      "step": 180
    },
    {
      "epoch": 0.13629842180774748,
      "grad_norm": 2.117523193359375,
      "learning_rate": 9.981676182531132e-06,
      "loss": 2.0328,
      "step": 190
    },
    {
      "epoch": 0.14347202295552366,
      "grad_norm": 1.9901212453842163,
      "learning_rate": 9.979697941831129e-06,
      "loss": 1.9242,
      "step": 200
    },
    {
      "epoch": 0.15064562410329985,
      "grad_norm": 1.8518065214157104,
      "learning_rate": 9.977618534694283e-06,
      "loss": 1.966,
      "step": 210
    },
    {
      "epoch": 0.15781922525107603,
      "grad_norm": 2.043804883956909,
      "learning_rate": 9.975438003365365e-06,
      "loss": 1.9933,
      "step": 220
    },
    {
      "epoch": 0.1649928263988522,
      "grad_norm": 1.5042188167572021,
      "learning_rate": 9.973156392143569e-06,
      "loss": 1.9537,
      "step": 230
    },
    {
      "epoch": 0.17216642754662842,
      "grad_norm": 1.5401605367660522,
      "learning_rate": 9.970773747381597e-06,
      "loss": 1.9588,
      "step": 240
    },
    {
      "epoch": 0.1793400286944046,
      "grad_norm": 1.6560314893722534,
      "learning_rate": 9.968290117484732e-06,
      "loss": 1.9414,
      "step": 250
    },
    {
      "epoch": 0.18651362984218078,
      "grad_norm": 1.7469406127929688,
      "learning_rate": 9.96570555290985e-06,
      "loss": 1.9115,
      "step": 260
    },
    {
      "epoch": 0.19368723098995697,
      "grad_norm": 2.298182725906372,
      "learning_rate": 9.963020106164386e-06,
      "loss": 1.9322,
      "step": 270
    },
    {
      "epoch": 0.20086083213773315,
      "grad_norm": 2.3198959827423096,
      "learning_rate": 9.960233831805284e-06,
      "loss": 1.916,
      "step": 280
    },
    {
      "epoch": 0.20803443328550933,
      "grad_norm": 2.4925947189331055,
      "learning_rate": 9.95734678643787e-06,
      "loss": 1.9543,
      "step": 290
    },
    {
      "epoch": 0.2152080344332855,
      "grad_norm": 2.0946388244628906,
      "learning_rate": 9.95435902871472e-06,
      "loss": 1.8435,
      "step": 300
    },
    {
      "epoch": 0.2223816355810617,
      "grad_norm": 2.1129002571105957,
      "learning_rate": 9.951270619334454e-06,
      "loss": 1.8265,
      "step": 310
    },
    {
      "epoch": 0.22955523672883787,
      "grad_norm": 2.456162929534912,
      "learning_rate": 9.94808162104051e-06,
      "loss": 1.8947,
      "step": 320
    },
    {
      "epoch": 0.23672883787661406,
      "grad_norm": 1.9610545635223389,
      "learning_rate": 9.944792098619871e-06,
      "loss": 1.9302,
      "step": 330
    },
    {
      "epoch": 0.24390243902439024,
      "grad_norm": 2.9943015575408936,
      "learning_rate": 9.941402118901743e-06,
      "loss": 1.8772,
      "step": 340
    },
    {
      "epoch": 0.25107604017216645,
      "grad_norm": 2.417701005935669,
      "learning_rate": 9.9379117507562e-06,
      "loss": 1.8578,
      "step": 350
    },
    {
      "epoch": 0.2582496413199426,
      "grad_norm": 1.93217933177948,
      "learning_rate": 9.934321065092786e-06,
      "loss": 1.9241,
      "step": 360
    },
    {
      "epoch": 0.2654232424677188,
      "grad_norm": 2.0827794075012207,
      "learning_rate": 9.930630134859071e-06,
      "loss": 1.8329,
      "step": 370
    },
    {
      "epoch": 0.27259684361549497,
      "grad_norm": 2.1711959838867188,
      "learning_rate": 9.926839035039178e-06,
      "loss": 1.7923,
      "step": 380
    },
    {
      "epoch": 0.2797704447632712,
      "grad_norm": 1.7758499383926392,
      "learning_rate": 9.922947842652243e-06,
      "loss": 1.8546,
      "step": 390
    },
    {
      "epoch": 0.28694404591104733,
      "grad_norm": 2.552518606185913,
      "learning_rate": 9.91895663675087e-06,
      "loss": 1.8338,
      "step": 400
    },
    {
      "epoch": 0.29411764705882354,
      "grad_norm": 2.0564823150634766,
      "learning_rate": 9.91486549841951e-06,
      "loss": 1.8048,
      "step": 410
    },
    {
      "epoch": 0.3012912482065997,
      "grad_norm": 2.3088417053222656,
      "learning_rate": 9.91067451077282e-06,
      "loss": 1.8428,
      "step": 420
    },
    {
      "epoch": 0.3084648493543759,
      "grad_norm": 2.23958158493042,
      "learning_rate": 9.906383758953973e-06,
      "loss": 1.7864,
      "step": 430
    },
    {
      "epoch": 0.31563845050215206,
      "grad_norm": 2.340747356414795,
      "learning_rate": 9.901993330132931e-06,
      "loss": 1.7824,
      "step": 440
    },
    {
      "epoch": 0.32281205164992827,
      "grad_norm": 2.4749553203582764,
      "learning_rate": 9.89750331350467e-06,
      "loss": 1.8358,
      "step": 450
    },
    {
      "epoch": 0.3299856527977044,
      "grad_norm": 2.4075350761413574,
      "learning_rate": 9.89291380028737e-06,
      "loss": 1.7958,
      "step": 460
    },
    {
      "epoch": 0.33715925394548063,
      "grad_norm": 2.2604832649230957,
      "learning_rate": 9.88822488372056e-06,
      "loss": 1.8149,
      "step": 470
    },
    {
      "epoch": 0.34433285509325684,
      "grad_norm": 2.3971612453460693,
      "learning_rate": 9.883436659063229e-06,
      "loss": 1.89,
      "step": 480
    },
    {
      "epoch": 0.351506456241033,
      "grad_norm": 2.579916477203369,
      "learning_rate": 9.878549223591884e-06,
      "loss": 1.8747,
      "step": 490
    },
    {
      "epoch": 0.3586800573888092,
      "grad_norm": 2.278756618499756,
      "learning_rate": 9.87356267659858e-06,
      "loss": 1.7718,
      "step": 500
    },
    {
      "epoch": 0.36585365853658536,
      "grad_norm": 2.1737446784973145,
      "learning_rate": 9.868477119388897e-06,
      "loss": 1.7847,
      "step": 510
    },
    {
      "epoch": 0.37302725968436157,
      "grad_norm": 2.22996187210083,
      "learning_rate": 9.863292655279883e-06,
      "loss": 1.7405,
      "step": 520
    },
    {
      "epoch": 0.3802008608321377,
      "grad_norm": 2.398470401763916,
      "learning_rate": 9.85800938959796e-06,
      "loss": 1.7623,
      "step": 530
    },
    {
      "epoch": 0.38737446197991393,
      "grad_norm": 2.5929014682769775,
      "learning_rate": 9.85262742967678e-06,
      "loss": 1.8174,
      "step": 540
    },
    {
      "epoch": 0.3945480631276901,
      "grad_norm": 2.64414381980896,
      "learning_rate": 9.847146884855045e-06,
      "loss": 1.6879,
      "step": 550
    },
    {
      "epoch": 0.4017216642754663,
      "grad_norm": 2.4452977180480957,
      "learning_rate": 9.841567866474287e-06,
      "loss": 1.7997,
      "step": 560
    },
    {
      "epoch": 0.40889526542324245,
      "grad_norm": 2.8674280643463135,
      "learning_rate": 9.8358904878766e-06,
      "loss": 1.7718,
      "step": 570
    },
    {
      "epoch": 0.41606886657101866,
      "grad_norm": 2.3206565380096436,
      "learning_rate": 9.830114864402348e-06,
      "loss": 1.7872,
      "step": 580
    },
    {
      "epoch": 0.4232424677187948,
      "grad_norm": 2.3305070400238037,
      "learning_rate": 9.824241113387815e-06,
      "loss": 1.8445,
      "step": 590
    },
    {
      "epoch": 0.430416068866571,
      "grad_norm": 2.178396463394165,
      "learning_rate": 9.818269354162821e-06,
      "loss": 1.7812,
      "step": 600
    },
    {
      "epoch": 0.4375896700143472,
      "grad_norm": 2.3362793922424316,
      "learning_rate": 9.8121997080483e-06,
      "loss": 1.7698,
      "step": 610
    },
    {
      "epoch": 0.4447632711621234,
      "grad_norm": 2.7436630725860596,
      "learning_rate": 9.806032298353832e-06,
      "loss": 1.7655,
      "step": 620
    },
    {
      "epoch": 0.4519368723098996,
      "grad_norm": 3.1327860355377197,
      "learning_rate": 9.79976725037514e-06,
      "loss": 1.8263,
      "step": 630
    },
    {
      "epoch": 0.45911047345767575,
      "grad_norm": 2.661855459213257,
      "learning_rate": 9.793404691391552e-06,
      "loss": 1.7404,
      "step": 640
    },
    {
      "epoch": 0.46628407460545196,
      "grad_norm": 2.6840646266937256,
      "learning_rate": 9.786944750663401e-06,
      "loss": 1.7419,
      "step": 650
    },
    {
      "epoch": 0.4734576757532281,
      "grad_norm": 2.642101287841797,
      "learning_rate": 9.780387559429405e-06,
      "loss": 1.7918,
      "step": 660
    },
    {
      "epoch": 0.4806312769010043,
      "grad_norm": 3.4534380435943604,
      "learning_rate": 9.773733250904005e-06,
      "loss": 1.7589,
      "step": 670
    },
    {
      "epoch": 0.4878048780487805,
      "grad_norm": 2.6870739459991455,
      "learning_rate": 9.767661449638702e-06,
      "loss": 1.7071,
      "step": 680
    },
    {
      "epoch": 0.4949784791965567,
      "grad_norm": 2.865540027618408,
      "learning_rate": 9.76082299232955e-06,
      "loss": 1.8139,
      "step": 690
    },
    {
      "epoch": 0.5021520803443329,
      "grad_norm": 2.796400785446167,
      "learning_rate": 9.753887815198392e-06,
      "loss": 1.7655,
      "step": 700
    },
    {
      "epoch": 0.509325681492109,
      "grad_norm": 2.746027708053589,
      "learning_rate": 9.746856059138749e-06,
      "loss": 1.7551,
      "step": 710
    },
    {
      "epoch": 0.5164992826398852,
      "grad_norm": 2.67175555229187,
      "learning_rate": 9.739727867006208e-06,
      "loss": 1.8155,
      "step": 720
    },
    {
      "epoch": 0.5236728837876614,
      "grad_norm": 2.4445736408233643,
      "learning_rate": 9.73250338361554e-06,
      "loss": 1.7573,
      "step": 730
    },
    {
      "epoch": 0.5308464849354376,
      "grad_norm": 2.621654748916626,
      "learning_rate": 9.725182755737744e-06,
      "loss": 1.805,
      "step": 740
    },
    {
      "epoch": 0.5380200860832137,
      "grad_norm": 2.668133497238159,
      "learning_rate": 9.717766132097069e-06,
      "loss": 1.7212,
      "step": 750
    },
    {
      "epoch": 0.5451936872309899,
      "grad_norm": 2.8676552772521973,
      "learning_rate": 9.710253663367992e-06,
      "loss": 1.7208,
      "step": 760
    },
    {
      "epoch": 0.5523672883787661,
      "grad_norm": 2.9542934894561768,
      "learning_rate": 9.70264550217216e-06,
      "loss": 1.6998,
      "step": 770
    },
    {
      "epoch": 0.5595408895265424,
      "grad_norm": 3.321131467819214,
      "learning_rate": 9.694941803075285e-06,
      "loss": 1.8191,
      "step": 780
    },
    {
      "epoch": 0.5667144906743186,
      "grad_norm": 2.621577501296997,
      "learning_rate": 9.687142722584003e-06,
      "loss": 1.7507,
      "step": 790
    },
    {
      "epoch": 0.5738880918220947,
      "grad_norm": 2.3909685611724854,
      "learning_rate": 9.679248419142704e-06,
      "loss": 1.736,
      "step": 800
    },
    {
      "epoch": 0.5810616929698709,
      "grad_norm": 3.0768346786499023,
      "learning_rate": 9.6712590531303e-06,
      "loss": 1.7124,
      "step": 810
    },
    {
      "epoch": 0.5882352941176471,
      "grad_norm": 2.7387430667877197,
      "learning_rate": 9.663174786856977e-06,
      "loss": 1.7115,
      "step": 820
    },
    {
      "epoch": 0.5954088952654233,
      "grad_norm": 2.7841992378234863,
      "learning_rate": 9.654995784560892e-06,
      "loss": 1.7755,
      "step": 830
    },
    {
      "epoch": 0.6025824964131994,
      "grad_norm": 2.5391898155212402,
      "learning_rate": 9.646722212404837e-06,
      "loss": 1.7053,
      "step": 840
    },
    {
      "epoch": 0.6097560975609756,
      "grad_norm": 2.634413719177246,
      "learning_rate": 9.638354238472868e-06,
      "loss": 1.6596,
      "step": 850
    },
    {
      "epoch": 0.6169296987087518,
      "grad_norm": 2.707380771636963,
      "learning_rate": 9.629892032766882e-06,
      "loss": 1.7903,
      "step": 860
    },
    {
      "epoch": 0.624103299856528,
      "grad_norm": 3.326389789581299,
      "learning_rate": 9.62133576720317e-06,
      "loss": 1.749,
      "step": 870
    },
    {
      "epoch": 0.6312769010043041,
      "grad_norm": 2.9529130458831787,
      "learning_rate": 9.612685615608922e-06,
      "loss": 1.7347,
      "step": 880
    },
    {
      "epoch": 0.6384505021520803,
      "grad_norm": 2.9795944690704346,
      "learning_rate": 9.603941753718695e-06,
      "loss": 1.7346,
      "step": 890
    },
    {
      "epoch": 0.6456241032998565,
      "grad_norm": 2.973815441131592,
      "learning_rate": 9.595104359170847e-06,
      "loss": 1.7666,
      "step": 900
    },
    {
      "epoch": 0.6527977044476327,
      "grad_norm": 3.298482656478882,
      "learning_rate": 9.586173611503918e-06,
      "loss": 1.6643,
      "step": 910
    },
    {
      "epoch": 0.6599713055954088,
      "grad_norm": 3.1382298469543457,
      "learning_rate": 9.577149692152994e-06,
      "loss": 1.8043,
      "step": 920
    },
    {
      "epoch": 0.667144906743185,
      "grad_norm": 2.7115135192871094,
      "learning_rate": 9.568032784446018e-06,
      "loss": 1.6779,
      "step": 930
    },
    {
      "epoch": 0.6743185078909613,
      "grad_norm": 3.0318357944488525,
      "learning_rate": 9.558823073600057e-06,
      "loss": 1.7056,
      "step": 940
    },
    {
      "epoch": 0.6814921090387375,
      "grad_norm": 2.9717860221862793,
      "learning_rate": 9.549520746717553e-06,
      "loss": 1.7389,
      "step": 950
    },
    {
      "epoch": 0.6886657101865137,
      "grad_norm": 2.763456106185913,
      "learning_rate": 9.540125992782512e-06,
      "loss": 1.6615,
      "step": 960
    },
    {
      "epoch": 0.6958393113342898,
      "grad_norm": 2.3551104068756104,
      "learning_rate": 9.530639002656665e-06,
      "loss": 1.7097,
      "step": 970
    },
    {
      "epoch": 0.703012912482066,
      "grad_norm": 2.8491570949554443,
      "learning_rate": 9.5210599690756e-06,
      "loss": 1.7625,
      "step": 980
    },
    {
      "epoch": 0.7101865136298422,
      "grad_norm": 3.279590368270874,
      "learning_rate": 9.511389086644828e-06,
      "loss": 1.7107,
      "step": 990
    },
    {
      "epoch": 0.7173601147776184,
      "grad_norm": 2.9236605167388916,
      "learning_rate": 9.501626551835851e-06,
      "loss": 1.6359,
      "step": 1000
    },
    {
      "epoch": 0.7245337159253945,
      "grad_norm": 2.6630795001983643,
      "learning_rate": 9.491772562982159e-06,
      "loss": 1.7038,
      "step": 1010
    },
    {
      "epoch": 0.7317073170731707,
      "grad_norm": 2.524345874786377,
      "learning_rate": 9.481827320275197e-06,
      "loss": 1.684,
      "step": 1020
    },
    {
      "epoch": 0.7388809182209469,
      "grad_norm": 2.735154390335083,
      "learning_rate": 9.471791025760307e-06,
      "loss": 1.7282,
      "step": 1030
    },
    {
      "epoch": 0.7460545193687231,
      "grad_norm": 3.5653905868530273,
      "learning_rate": 9.461663883332615e-06,
      "loss": 1.6882,
      "step": 1040
    },
    {
      "epoch": 0.7532281205164992,
      "grad_norm": 2.898277997970581,
      "learning_rate": 9.451446098732901e-06,
      "loss": 1.6789,
      "step": 1050
    },
    {
      "epoch": 0.7604017216642754,
      "grad_norm": 2.8000683784484863,
      "learning_rate": 9.441137879543405e-06,
      "loss": 1.7158,
      "step": 1060
    },
    {
      "epoch": 0.7675753228120517,
      "grad_norm": 2.6920292377471924,
      "learning_rate": 9.430739435183615e-06,
      "loss": 1.6955,
      "step": 1070
    },
    {
      "epoch": 0.7747489239598279,
      "grad_norm": 2.960892677307129,
      "learning_rate": 9.420250976906017e-06,
      "loss": 1.7134,
      "step": 1080
    },
    {
      "epoch": 0.7819225251076041,
      "grad_norm": 2.7545387744903564,
      "learning_rate": 9.409672717791802e-06,
      "loss": 1.804,
      "step": 1090
    },
    {
      "epoch": 0.7890961262553802,
      "grad_norm": 3.0832555294036865,
      "learning_rate": 9.399004872746528e-06,
      "loss": 1.6946,
      "step": 1100
    },
    {
      "epoch": 0.7962697274031564,
      "grad_norm": 3.2083687782287598,
      "learning_rate": 9.388247658495766e-06,
      "loss": 1.6926,
      "step": 1110
    },
    {
      "epoch": 0.8034433285509326,
      "grad_norm": 2.8609795570373535,
      "learning_rate": 9.37740129358069e-06,
      "loss": 1.6841,
      "step": 1120
    },
    {
      "epoch": 0.8106169296987088,
      "grad_norm": 3.0739948749542236,
      "learning_rate": 9.366465998353639e-06,
      "loss": 1.6406,
      "step": 1130
    },
    {
      "epoch": 0.8177905308464849,
      "grad_norm": 3.2946834564208984,
      "learning_rate": 9.355441994973639e-06,
      "loss": 1.6717,
      "step": 1140
    },
    {
      "epoch": 0.8249641319942611,
      "grad_norm": 2.962939977645874,
      "learning_rate": 9.344329507401898e-06,
      "loss": 1.5966,
      "step": 1150
    },
    {
      "epoch": 0.8321377331420373,
      "grad_norm": 3.4961791038513184,
      "learning_rate": 9.33312876139724e-06,
      "loss": 1.7371,
      "step": 1160
    },
    {
      "epoch": 0.8393113342898135,
      "grad_norm": 2.982367753982544,
      "learning_rate": 9.321839984511534e-06,
      "loss": 1.6786,
      "step": 1170
    },
    {
      "epoch": 0.8464849354375896,
      "grad_norm": 2.5306308269500732,
      "learning_rate": 9.310463406085061e-06,
      "loss": 1.6994,
      "step": 1180
    },
    {
      "epoch": 0.8536585365853658,
      "grad_norm": 3.3410990238189697,
      "learning_rate": 9.298999257241862e-06,
      "loss": 1.6965,
      "step": 1190
    },
    {
      "epoch": 0.860832137733142,
      "grad_norm": 3.217726707458496,
      "learning_rate": 9.287447770885038e-06,
      "loss": 1.652,
      "step": 1200
    },
    {
      "epoch": 0.8680057388809183,
      "grad_norm": 3.2681081295013428,
      "learning_rate": 9.275809181692016e-06,
      "loss": 1.7253,
      "step": 1210
    },
    {
      "epoch": 0.8751793400286944,
      "grad_norm": 3.2594058513641357,
      "learning_rate": 9.264083726109789e-06,
      "loss": 1.7003,
      "step": 1220
    },
    {
      "epoch": 0.8823529411764706,
      "grad_norm": 2.6243631839752197,
      "learning_rate": 9.25227164235011e-06,
      "loss": 1.6956,
      "step": 1230
    },
    {
      "epoch": 0.8895265423242468,
      "grad_norm": 3.372919797897339,
      "learning_rate": 9.240373170384643e-06,
      "loss": 1.6924,
      "step": 1240
    },
    {
      "epoch": 0.896700143472023,
      "grad_norm": 3.2882370948791504,
      "learning_rate": 9.228388551940104e-06,
      "loss": 1.7165,
      "step": 1250
    },
    {
      "epoch": 0.9038737446197992,
      "grad_norm": 2.9154160022735596,
      "learning_rate": 9.216318030493338e-06,
      "loss": 1.7892,
      "step": 1260
    },
    {
      "epoch": 0.9110473457675753,
      "grad_norm": 3.197059392929077,
      "learning_rate": 9.204161851266376e-06,
      "loss": 1.7177,
      "step": 1270
    },
    {
      "epoch": 0.9182209469153515,
      "grad_norm": 3.1669254302978516,
      "learning_rate": 9.191920261221451e-06,
      "loss": 1.8187,
      "step": 1280
    },
    {
      "epoch": 0.9253945480631277,
      "grad_norm": 3.7641584873199463,
      "learning_rate": 9.17959350905599e-06,
      "loss": 1.7001,
      "step": 1290
    },
    {
      "epoch": 0.9325681492109039,
      "grad_norm": 3.2701101303100586,
      "learning_rate": 9.16718184519755e-06,
      "loss": 1.6759,
      "step": 1300
    },
    {
      "epoch": 0.93974175035868,
      "grad_norm": 3.310391426086426,
      "learning_rate": 9.154685521798736e-06,
      "loss": 1.6809,
      "step": 1310
    },
    {
      "epoch": 0.9469153515064562,
      "grad_norm": 3.147019147872925,
      "learning_rate": 9.142104792732078e-06,
      "loss": 1.704,
      "step": 1320
    },
    {
      "epoch": 0.9540889526542324,
      "grad_norm": 3.0734636783599854,
      "learning_rate": 9.129439913584869e-06,
      "loss": 1.612,
      "step": 1330
    },
    {
      "epoch": 0.9612625538020086,
      "grad_norm": 3.127837657928467,
      "learning_rate": 9.11669114165398e-06,
      "loss": 1.717,
      "step": 1340
    },
    {
      "epoch": 0.9684361549497847,
      "grad_norm": 3.3490240573883057,
      "learning_rate": 9.103858735940635e-06,
      "loss": 1.6266,
      "step": 1350
    },
    {
      "epoch": 0.975609756097561,
      "grad_norm": 2.8164584636688232,
      "learning_rate": 9.090942957145131e-06,
      "loss": 1.6425,
      "step": 1360
    },
    {
      "epoch": 0.9827833572453372,
      "grad_norm": 3.606072187423706,
      "learning_rate": 9.07794406766156e-06,
      "loss": 1.6473,
      "step": 1370
    },
    {
      "epoch": 0.9899569583931134,
      "grad_norm": 2.9579975605010986,
      "learning_rate": 9.064862331572472e-06,
      "loss": 1.7284,
      "step": 1380
    },
    {
      "epoch": 0.9971305595408895,
      "grad_norm": 3.6344897747039795,
      "learning_rate": 9.051698014643514e-06,
      "loss": 1.7172,
      "step": 1390
    },
    {
      "epoch": 1.0043041606886658,
      "grad_norm": 3.430784225463867,
      "learning_rate": 9.038451384318019e-06,
      "loss": 1.6962,
      "step": 1400
    },
    {
      "epoch": 1.011477761836442,
      "grad_norm": 2.9303646087646484,
      "learning_rate": 9.02512270971159e-06,
      "loss": 1.6398,
      "step": 1410
    },
    {
      "epoch": 1.018651362984218,
      "grad_norm": 3.5780439376831055,
      "learning_rate": 9.011712261606615e-06,
      "loss": 1.5988,
      "step": 1420
    },
    {
      "epoch": 1.0258249641319943,
      "grad_norm": 3.461416482925415,
      "learning_rate": 8.998220312446779e-06,
      "loss": 1.7059,
      "step": 1430
    },
    {
      "epoch": 1.0329985652797704,
      "grad_norm": 3.215670585632324,
      "learning_rate": 8.984647136331524e-06,
      "loss": 1.6497,
      "step": 1440
    },
    {
      "epoch": 1.0401721664275467,
      "grad_norm": 4.490140914916992,
      "learning_rate": 8.970993009010476e-06,
      "loss": 1.6671,
      "step": 1450
    },
    {
      "epoch": 1.0473457675753228,
      "grad_norm": 3.519808530807495,
      "learning_rate": 8.957258207877855e-06,
      "loss": 1.6173,
      "step": 1460
    },
    {
      "epoch": 1.054519368723099,
      "grad_norm": 3.54024338722229,
      "learning_rate": 8.94344301196683e-06,
      "loss": 1.6946,
      "step": 1470
    },
    {
      "epoch": 1.0616929698708752,
      "grad_norm": 3.6467039585113525,
      "learning_rate": 8.929547701943849e-06,
      "loss": 1.6899,
      "step": 1480
    },
    {
      "epoch": 1.0688665710186513,
      "grad_norm": 3.115145683288574,
      "learning_rate": 8.915572560102942e-06,
      "loss": 1.6569,
      "step": 1490
    },
    {
      "epoch": 1.0760401721664274,
      "grad_norm": 3.5412967205047607,
      "learning_rate": 8.901517870359987e-06,
      "loss": 1.6129,
      "step": 1500
    },
    {
      "epoch": 1.0832137733142038,
      "grad_norm": 3.212592363357544,
      "learning_rate": 8.887383918246936e-06,
      "loss": 1.619,
      "step": 1510
    },
    {
      "epoch": 1.0903873744619799,
      "grad_norm": 3.992544412612915,
      "learning_rate": 8.873170990906021e-06,
      "loss": 1.7198,
      "step": 1520
    },
    {
      "epoch": 1.0975609756097562,
      "grad_norm": 3.328711748123169,
      "learning_rate": 8.858879377083915e-06,
      "loss": 1.613,
      "step": 1530
    },
    {
      "epoch": 1.1047345767575323,
      "grad_norm": 3.3810648918151855,
      "learning_rate": 8.844509367125868e-06,
      "loss": 1.6236,
      "step": 1540
    },
    {
      "epoch": 1.1119081779053084,
      "grad_norm": 3.943138599395752,
      "learning_rate": 8.830061252969812e-06,
      "loss": 1.595,
      "step": 1550
    },
    {
      "epoch": 1.1190817790530847,
      "grad_norm": 3.1200098991394043,
      "learning_rate": 8.81553532814042e-06,
      "loss": 1.6529,
      "step": 1560
    },
    {
      "epoch": 1.1262553802008608,
      "grad_norm": 3.0979514122009277,
      "learning_rate": 8.800931887743154e-06,
      "loss": 1.6585,
      "step": 1570
    },
    {
      "epoch": 1.133428981348637,
      "grad_norm": 3.2608706951141357,
      "learning_rate": 8.786251228458264e-06,
      "loss": 1.6285,
      "step": 1580
    },
    {
      "epoch": 1.1406025824964132,
      "grad_norm": 3.183708667755127,
      "learning_rate": 8.771493648534765e-06,
      "loss": 1.6342,
      "step": 1590
    },
    {
      "epoch": 1.1477761836441893,
      "grad_norm": 3.1953697204589844,
      "learning_rate": 8.756659447784367e-06,
      "loss": 1.6109,
      "step": 1600
    },
    {
      "epoch": 1.1549497847919656,
      "grad_norm": 2.3765017986297607,
      "learning_rate": 8.741748927575399e-06,
      "loss": 1.7166,
      "step": 1610
    },
    {
      "epoch": 1.1621233859397417,
      "grad_norm": 3.361921548843384,
      "learning_rate": 8.726762390826674e-06,
      "loss": 1.6443,
      "step": 1620
    },
    {
      "epoch": 1.169296987087518,
      "grad_norm": 3.7625572681427,
      "learning_rate": 8.711700142001345e-06,
      "loss": 1.6706,
      "step": 1630
    },
    {
      "epoch": 1.1764705882352942,
      "grad_norm": 3.140669345855713,
      "learning_rate": 8.69656248710071e-06,
      "loss": 1.6747,
      "step": 1640
    },
    {
      "epoch": 1.1836441893830703,
      "grad_norm": 3.3284435272216797,
      "learning_rate": 8.681349733658002e-06,
      "loss": 1.6214,
      "step": 1650
    },
    {
      "epoch": 1.1908177905308466,
      "grad_norm": 2.9470176696777344,
      "learning_rate": 8.66606219073214e-06,
      "loss": 1.6245,
      "step": 1660
    },
    {
      "epoch": 1.1979913916786227,
      "grad_norm": 3.5023341178894043,
      "learning_rate": 8.650700168901453e-06,
      "loss": 1.7131,
      "step": 1670
    },
    {
      "epoch": 1.2051649928263988,
      "grad_norm": 3.873821973800659,
      "learning_rate": 8.635263980257356e-06,
      "loss": 1.6813,
      "step": 1680
    },
    {
      "epoch": 1.212338593974175,
      "grad_norm": 2.8494021892547607,
      "learning_rate": 8.619753938398034e-06,
      "loss": 1.6663,
      "step": 1690
    },
    {
      "epoch": 1.2195121951219512,
      "grad_norm": 3.300187110900879,
      "learning_rate": 8.604170358422045e-06,
      "loss": 1.6144,
      "step": 1700
    },
    {
      "epoch": 1.2266857962697273,
      "grad_norm": 3.532813310623169,
      "learning_rate": 8.58851355692194e-06,
      "loss": 1.7031,
      "step": 1710
    },
    {
      "epoch": 1.2338593974175036,
      "grad_norm": 3.4297521114349365,
      "learning_rate": 8.572783851977821e-06,
      "loss": 1.6424,
      "step": 1720
    },
    {
      "epoch": 1.2410329985652797,
      "grad_norm": 5.2142558097839355,
      "learning_rate": 8.556981563150874e-06,
      "loss": 1.6311,
      "step": 1730
    },
    {
      "epoch": 1.248206599713056,
      "grad_norm": 4.312285423278809,
      "learning_rate": 8.541107011476891e-06,
      "loss": 1.6334,
      "step": 1740
    },
    {
      "epoch": 1.2553802008608321,
      "grad_norm": 3.317035675048828,
      "learning_rate": 8.525160519459735e-06,
      "loss": 1.5938,
      "step": 1750
    },
    {
      "epoch": 1.2625538020086085,
      "grad_norm": 3.973345994949341,
      "learning_rate": 8.509142411064792e-06,
      "loss": 1.7113,
      "step": 1760
    },
    {
      "epoch": 1.2697274031563845,
      "grad_norm": 4.215405464172363,
      "learning_rate": 8.493053011712397e-06,
      "loss": 1.6902,
      "step": 1770
    },
    {
      "epoch": 1.2769010043041606,
      "grad_norm": 3.773895025253296,
      "learning_rate": 8.47689264827121e-06,
      "loss": 1.6749,
      "step": 1780
    },
    {
      "epoch": 1.284074605451937,
      "grad_norm": 3.682049036026001,
      "learning_rate": 8.460661649051583e-06,
      "loss": 1.6184,
      "step": 1790
    },
    {
      "epoch": 1.291248206599713,
      "grad_norm": 2.924461603164673,
      "learning_rate": 8.444360343798892e-06,
      "loss": 1.6876,
      "step": 1800
    },
    {
      "epoch": 1.2984218077474892,
      "grad_norm": 4.323392391204834,
      "learning_rate": 8.427989063686828e-06,
      "loss": 1.5844,
      "step": 1810
    },
    {
      "epoch": 1.3055954088952655,
      "grad_norm": 3.7296600341796875,
      "learning_rate": 8.411548141310683e-06,
      "loss": 1.6221,
      "step": 1820
    },
    {
      "epoch": 1.3127690100430416,
      "grad_norm": 3.5767481327056885,
      "learning_rate": 8.395037910680581e-06,
      "loss": 1.6147,
      "step": 1830
    },
    {
      "epoch": 1.3199426111908177,
      "grad_norm": 3.899414539337158,
      "learning_rate": 8.3784587072147e-06,
      "loss": 1.6196,
      "step": 1840
    },
    {
      "epoch": 1.327116212338594,
      "grad_norm": 3.6762821674346924,
      "learning_rate": 8.361810867732455e-06,
      "loss": 1.6538,
      "step": 1850
    },
    {
      "epoch": 1.33428981348637,
      "grad_norm": 3.703944444656372,
      "learning_rate": 8.345094730447649e-06,
      "loss": 1.6612,
      "step": 1860
    },
    {
      "epoch": 1.3414634146341464,
      "grad_norm": 3.5118725299835205,
      "learning_rate": 8.328310634961617e-06,
      "loss": 1.5703,
      "step": 1870
    },
    {
      "epoch": 1.3486370157819225,
      "grad_norm": 3.8196804523468018,
      "learning_rate": 8.311458922256309e-06,
      "loss": 1.7051,
      "step": 1880
    },
    {
      "epoch": 1.3558106169296988,
      "grad_norm": 3.4614179134368896,
      "learning_rate": 8.29453993468738e-06,
      "loss": 1.6457,
      "step": 1890
    },
    {
      "epoch": 1.362984218077475,
      "grad_norm": 3.706263780593872,
      "learning_rate": 8.277554015977221e-06,
      "loss": 1.7179,
      "step": 1900
    },
    {
      "epoch": 1.370157819225251,
      "grad_norm": 3.959934711456299,
      "learning_rate": 8.26050151120798e-06,
      "loss": 1.6627,
      "step": 1910
    },
    {
      "epoch": 1.3773314203730274,
      "grad_norm": 3.4681637287139893,
      "learning_rate": 8.243382766814555e-06,
      "loss": 1.706,
      "step": 1920
    },
    {
      "epoch": 1.3845050215208035,
      "grad_norm": 3.4048144817352295,
      "learning_rate": 8.226198130577556e-06,
      "loss": 1.6357,
      "step": 1930
    },
    {
      "epoch": 1.3916786226685796,
      "grad_norm": 3.7829625606536865,
      "learning_rate": 8.208947951616231e-06,
      "loss": 1.6607,
      "step": 1940
    },
    {
      "epoch": 1.3988522238163559,
      "grad_norm": 3.9566149711608887,
      "learning_rate": 8.191632580381384e-06,
      "loss": 1.642,
      "step": 1950
    },
    {
      "epoch": 1.406025824964132,
      "grad_norm": 3.754624843597412,
      "learning_rate": 8.174252368648249e-06,
      "loss": 1.6364,
      "step": 1960
    },
    {
      "epoch": 1.413199426111908,
      "grad_norm": 4.153691291809082,
      "learning_rate": 8.156807669509346e-06,
      "loss": 1.6488,
      "step": 1970
    },
    {
      "epoch": 1.4203730272596844,
      "grad_norm": 3.9793808460235596,
      "learning_rate": 8.139298837367305e-06,
      "loss": 1.5212,
      "step": 1980
    },
    {
      "epoch": 1.4275466284074605,
      "grad_norm": 4.473745822906494,
      "learning_rate": 8.12172622792767e-06,
      "loss": 1.7147,
      "step": 1990
    },
    {
      "epoch": 1.4347202295552366,
      "grad_norm": 3.779081344604492,
      "learning_rate": 8.10409019819167e-06,
      "loss": 1.6124,
      "step": 2000
    },
    {
      "epoch": 1.441893830703013,
      "grad_norm": 3.6823196411132812,
      "learning_rate": 8.086391106448965e-06,
      "loss": 1.7568,
      "step": 2010
    },
    {
      "epoch": 1.4490674318507892,
      "grad_norm": 3.419132947921753,
      "learning_rate": 8.068629312270372e-06,
      "loss": 1.5846,
      "step": 2020
    },
    {
      "epoch": 1.4562410329985653,
      "grad_norm": 3.6497838497161865,
      "learning_rate": 8.050805176500554e-06,
      "loss": 1.6299,
      "step": 2030
    },
    {
      "epoch": 1.4634146341463414,
      "grad_norm": 4.701443672180176,
      "learning_rate": 8.03291906125069e-06,
      "loss": 1.7703,
      "step": 2040
    },
    {
      "epoch": 1.4705882352941178,
      "grad_norm": 3.8372514247894287,
      "learning_rate": 8.014971329891126e-06,
      "loss": 1.7229,
      "step": 2050
    },
    {
      "epoch": 1.4777618364418939,
      "grad_norm": 3.7496984004974365,
      "learning_rate": 7.996962347043982e-06,
      "loss": 1.6741,
      "step": 2060
    },
    {
      "epoch": 1.48493543758967,
      "grad_norm": 3.7477941513061523,
      "learning_rate": 7.978892478575752e-06,
      "loss": 1.6042,
      "step": 2070
    },
    {
      "epoch": 1.4921090387374463,
      "grad_norm": 3.8436877727508545,
      "learning_rate": 7.96076209158987e-06,
      "loss": 1.5666,
      "step": 2080
    },
    {
      "epoch": 1.4992826398852224,
      "grad_norm": 4.1554694175720215,
      "learning_rate": 7.94257155441925e-06,
      "loss": 1.6624,
      "step": 2090
    },
    {
      "epoch": 1.5064562410329985,
      "grad_norm": 4.3105621337890625,
      "learning_rate": 7.9243212366188e-06,
      "loss": 1.6091,
      "step": 2100
    },
    {
      "epoch": 1.5136298421807748,
      "grad_norm": 3.1761374473571777,
      "learning_rate": 7.906011508957922e-06,
      "loss": 1.6096,
      "step": 2110
    },
    {
      "epoch": 1.5208034433285509,
      "grad_norm": 4.182857513427734,
      "learning_rate": 7.887642743412978e-06,
      "loss": 1.5962,
      "step": 2120
    },
    {
      "epoch": 1.527977044476327,
      "grad_norm": 4.191015243530273,
      "learning_rate": 7.86921531315972e-06,
      "loss": 1.6535,
      "step": 2130
    },
    {
      "epoch": 1.5351506456241033,
      "grad_norm": 4.101140975952148,
      "learning_rate": 7.850729592565734e-06,
      "loss": 1.6337,
      "step": 2140
    },
    {
      "epoch": 1.5423242467718796,
      "grad_norm": 3.7427709102630615,
      "learning_rate": 7.832185957182806e-06,
      "loss": 1.6006,
      "step": 2150
    },
    {
      "epoch": 1.5494978479196555,
      "grad_norm": 3.917590618133545,
      "learning_rate": 7.813584783739314e-06,
      "loss": 1.578,
      "step": 2160
    },
    {
      "epoch": 1.5566714490674318,
      "grad_norm": 3.8046553134918213,
      "learning_rate": 7.794926450132565e-06,
      "loss": 1.585,
      "step": 2170
    },
    {
      "epoch": 1.5638450502152081,
      "grad_norm": 4.225083827972412,
      "learning_rate": 7.776211335421117e-06,
      "loss": 1.647,
      "step": 2180
    },
    {
      "epoch": 1.5710186513629842,
      "grad_norm": 4.122983932495117,
      "learning_rate": 7.757439819817084e-06,
      "loss": 1.6376,
      "step": 2190
    },
    {
      "epoch": 1.5781922525107603,
      "grad_norm": 3.5825135707855225,
      "learning_rate": 7.738612284678404e-06,
      "loss": 1.6277,
      "step": 2200
    },
    {
      "epoch": 1.5853658536585367,
      "grad_norm": 4.454635143280029,
      "learning_rate": 7.719729112501098e-06,
      "loss": 1.6254,
      "step": 2210
    },
    {
      "epoch": 1.5925394548063128,
      "grad_norm": 4.191830158233643,
      "learning_rate": 7.700790686911494e-06,
      "loss": 1.6557,
      "step": 2220
    },
    {
      "epoch": 1.5997130559540889,
      "grad_norm": 4.0196990966796875,
      "learning_rate": 7.68179739265844e-06,
      "loss": 1.6249,
      "step": 2230
    },
    {
      "epoch": 1.6068866571018652,
      "grad_norm": 5.068940162658691,
      "learning_rate": 7.662749615605483e-06,
      "loss": 1.6595,
      "step": 2240
    },
    {
      "epoch": 1.6140602582496413,
      "grad_norm": 5.262932777404785,
      "learning_rate": 7.643647742723025e-06,
      "loss": 1.6574,
      "step": 2250
    },
    {
      "epoch": 1.6212338593974174,
      "grad_norm": 4.6926679611206055,
      "learning_rate": 7.624492162080472e-06,
      "loss": 1.6203,
      "step": 2260
    },
    {
      "epoch": 1.6284074605451937,
      "grad_norm": 3.5984044075012207,
      "learning_rate": 7.605283262838345e-06,
      "loss": 1.6511,
      "step": 2270
    },
    {
      "epoch": 1.63558106169297,
      "grad_norm": 4.157629489898682,
      "learning_rate": 7.586021435240374e-06,
      "loss": 1.6729,
      "step": 2280
    },
    {
      "epoch": 1.642754662840746,
      "grad_norm": 4.090473651885986,
      "learning_rate": 7.5667070706055645e-06,
      "loss": 1.5994,
      "step": 2290
    },
    {
      "epoch": 1.6499282639885222,
      "grad_norm": 3.849058151245117,
      "learning_rate": 7.5473405613202596e-06,
      "loss": 1.6257,
      "step": 2300
    },
    {
      "epoch": 1.6571018651362985,
      "grad_norm": 4.415526866912842,
      "learning_rate": 7.527922300830156e-06,
      "loss": 1.6039,
      "step": 2310
    },
    {
      "epoch": 1.6642754662840746,
      "grad_norm": 4.275269031524658,
      "learning_rate": 7.508452683632321e-06,
      "loss": 1.6875,
      "step": 2320
    },
    {
      "epoch": 1.6714490674318507,
      "grad_norm": 4.5427470207214355,
      "learning_rate": 7.488932105267171e-06,
      "loss": 1.6553,
      "step": 2330
    },
    {
      "epoch": 1.678622668579627,
      "grad_norm": 3.43648362159729,
      "learning_rate": 7.469360962310438e-06,
      "loss": 1.6071,
      "step": 2340
    },
    {
      "epoch": 1.6857962697274032,
      "grad_norm": 4.416233062744141,
      "learning_rate": 7.449739652365112e-06,
      "loss": 1.6199,
      "step": 2350
    },
    {
      "epoch": 1.6929698708751793,
      "grad_norm": 4.646688938140869,
      "learning_rate": 7.430068574053368e-06,
      "loss": 1.6819,
      "step": 2360
    },
    {
      "epoch": 1.7001434720229556,
      "grad_norm": 4.458006381988525,
      "learning_rate": 7.410348127008462e-06,
      "loss": 1.6469,
      "step": 2370
    },
    {
      "epoch": 1.7073170731707317,
      "grad_norm": 4.031156063079834,
      "learning_rate": 7.390578711866612e-06,
      "loss": 1.5756,
      "step": 2380
    },
    {
      "epoch": 1.7144906743185078,
      "grad_norm": 4.446554183959961,
      "learning_rate": 7.370760730258862e-06,
      "loss": 1.5738,
      "step": 2390
    },
    {
      "epoch": 1.721664275466284,
      "grad_norm": 3.382572889328003,
      "learning_rate": 7.350894584802928e-06,
      "loss": 1.6448,
      "step": 2400
    },
    {
      "epoch": 1.7288378766140604,
      "grad_norm": 4.41314697265625,
      "learning_rate": 7.330980679095e-06,
      "loss": 1.5619,
      "step": 2410
    },
    {
      "epoch": 1.7360114777618363,
      "grad_norm": 4.554559230804443,
      "learning_rate": 7.311019417701567e-06,
      "loss": 1.7062,
      "step": 2420
    },
    {
      "epoch": 1.7431850789096126,
      "grad_norm": 4.765127658843994,
      "learning_rate": 7.291011206151176e-06,
      "loss": 1.5651,
      "step": 2430
    },
    {
      "epoch": 1.750358680057389,
      "grad_norm": 3.4387612342834473,
      "learning_rate": 7.270956450926209e-06,
      "loss": 1.6356,
      "step": 2440
    },
    {
      "epoch": 1.757532281205165,
      "grad_norm": 4.319148540496826,
      "learning_rate": 7.250855559454615e-06,
      "loss": 1.6028,
      "step": 2450
    },
    {
      "epoch": 1.7647058823529411,
      "grad_norm": 3.935108184814453,
      "learning_rate": 7.2307089401016405e-06,
      "loss": 1.6229,
      "step": 2460
    },
    {
      "epoch": 1.7718794835007174,
      "grad_norm": 4.795163154602051,
      "learning_rate": 7.210517002161525e-06,
      "loss": 1.6268,
      "step": 2470
    },
    {
      "epoch": 1.7790530846484935,
      "grad_norm": 3.7808284759521484,
      "learning_rate": 7.190280155849196e-06,
      "loss": 1.6357,
      "step": 2480
    },
    {
      "epoch": 1.7862266857962696,
      "grad_norm": 4.101686954498291,
      "learning_rate": 7.169998812291923e-06,
      "loss": 1.672,
      "step": 2490
    },
    {
      "epoch": 1.793400286944046,
      "grad_norm": 4.233961582183838,
      "learning_rate": 7.149673383520978e-06,
      "loss": 1.5993,
      "step": 2500
    },
    {
      "epoch": 1.800573888091822,
      "grad_norm": 4.073063373565674,
      "learning_rate": 7.129304282463253e-06,
      "loss": 1.5757,
      "step": 2510
    },
    {
      "epoch": 1.8077474892395982,
      "grad_norm": 4.351341247558594,
      "learning_rate": 7.10889192293288e-06,
      "loss": 1.6747,
      "step": 2520
    },
    {
      "epoch": 1.8149210903873745,
      "grad_norm": 3.651437282562256,
      "learning_rate": 7.088436719622819e-06,
      "loss": 1.5985,
      "step": 2530
    },
    {
      "epoch": 1.8220946915351508,
      "grad_norm": 5.168068885803223,
      "learning_rate": 7.0679390880964364e-06,
      "loss": 1.5975,
      "step": 2540
    },
    {
      "epoch": 1.8292682926829267,
      "grad_norm": 4.336892127990723,
      "learning_rate": 7.047399444779055e-06,
      "loss": 1.6785,
      "step": 2550
    },
    {
      "epoch": 1.836441893830703,
      "grad_norm": 4.25896692276001,
      "learning_rate": 7.026818206949512e-06,
      "loss": 1.6602,
      "step": 2560
    },
    {
      "epoch": 1.8436154949784793,
      "grad_norm": 4.245326995849609,
      "learning_rate": 7.0061957927316534e-06,
      "loss": 1.6699,
      "step": 2570
    },
    {
      "epoch": 1.8507890961262554,
      "grad_norm": 4.327492713928223,
      "learning_rate": 6.985532621085871e-06,
      "loss": 1.5761,
      "step": 2580
    },
    {
      "epoch": 1.8579626972740315,
      "grad_norm": 3.8036532402038574,
      "learning_rate": 6.964829111800564e-06,
      "loss": 1.6718,
      "step": 2590
    },
    {
      "epoch": 1.8651362984218078,
      "grad_norm": 3.960731029510498,
      "learning_rate": 6.944085685483628e-06,
      "loss": 1.5995,
      "step": 2600
    },
    {
      "epoch": 1.872309899569584,
      "grad_norm": 4.787394046783447,
      "learning_rate": 6.923302763553903e-06,
      "loss": 1.6293,
      "step": 2610
    },
    {
      "epoch": 1.87948350071736,
      "grad_norm": 4.759507656097412,
      "learning_rate": 6.9024807682326114e-06,
      "loss": 1.6318,
      "step": 2620
    },
    {
      "epoch": 1.8866571018651364,
      "grad_norm": 4.086320877075195,
      "learning_rate": 6.881620122534785e-06,
      "loss": 1.6583,
      "step": 2630
    },
    {
      "epoch": 1.8938307030129125,
      "grad_norm": 4.67735481262207,
      "learning_rate": 6.860721250260664e-06,
      "loss": 1.5833,
      "step": 2640
    },
    {
      "epoch": 1.9010043041606886,
      "grad_norm": 4.130511283874512,
      "learning_rate": 6.839784575987096e-06,
      "loss": 1.595,
      "step": 2650
    },
    {
      "epoch": 1.9081779053084649,
      "grad_norm": 3.7440178394317627,
      "learning_rate": 6.8188105250589025e-06,
      "loss": 1.5726,
      "step": 2660
    },
    {
      "epoch": 1.9153515064562412,
      "grad_norm": 3.4837217330932617,
      "learning_rate": 6.79779952358024e-06,
      "loss": 1.6688,
      "step": 2670
    },
    {
      "epoch": 1.922525107604017,
      "grad_norm": 3.78692889213562,
      "learning_rate": 6.776751998405948e-06,
      "loss": 1.6546,
      "step": 2680
    },
    {
      "epoch": 1.9296987087517934,
      "grad_norm": 4.166765213012695,
      "learning_rate": 6.755668377132869e-06,
      "loss": 1.5996,
      "step": 2690
    },
    {
      "epoch": 1.9368723098995697,
      "grad_norm": 4.081962585449219,
      "learning_rate": 6.734549088091169e-06,
      "loss": 1.5417,
      "step": 2700
    },
    {
      "epoch": 1.9440459110473458,
      "grad_norm": 4.335911273956299,
      "learning_rate": 6.71339456033563e-06,
      "loss": 1.6831,
      "step": 2710
    },
    {
      "epoch": 1.951219512195122,
      "grad_norm": 5.010943412780762,
      "learning_rate": 6.692205223636939e-06,
      "loss": 1.5195,
      "step": 2720
    },
    {
      "epoch": 1.9583931133428982,
      "grad_norm": 4.185887813568115,
      "learning_rate": 6.67098150847295e-06,
      "loss": 1.5488,
      "step": 2730
    },
    {
      "epoch": 1.9655667144906743,
      "grad_norm": 3.8426084518432617,
      "learning_rate": 6.6497238460199454e-06,
      "loss": 1.538,
      "step": 2740
    },
    {
      "epoch": 1.9727403156384504,
      "grad_norm": 3.925285577774048,
      "learning_rate": 6.628432668143871e-06,
      "loss": 1.5495,
      "step": 2750
    },
    {
      "epoch": 1.9799139167862267,
      "grad_norm": 5.025732040405273,
      "learning_rate": 6.607108407391567e-06,
      "loss": 1.6993,
      "step": 2760
    },
    {
      "epoch": 1.9870875179340028,
      "grad_norm": 4.247812747955322,
      "learning_rate": 6.585751496981974e-06,
      "loss": 1.6227,
      "step": 2770
    },
    {
      "epoch": 1.994261119081779,
      "grad_norm": 3.4113566875457764,
      "learning_rate": 6.56436237079734e-06,
      "loss": 1.5672,
      "step": 2780
    },
    {
      "epoch": 2.0014347202295553,
      "grad_norm": 3.6480178833007812,
      "learning_rate": 6.542941463374397e-06,
      "loss": 1.6574,
      "step": 2790
    },
    {
      "epoch": 2.0086083213773316,
      "grad_norm": 5.171975135803223,
      "learning_rate": 6.521489209895546e-06,
      "loss": 1.6621,
      "step": 2800
    },
    {
      "epoch": 2.0157819225251075,
      "grad_norm": 4.774234771728516,
      "learning_rate": 6.500006046179995e-06,
      "loss": 1.5835,
      "step": 2810
    },
    {
      "epoch": 2.022955523672884,
      "grad_norm": 5.025129318237305,
      "learning_rate": 6.478492408674928e-06,
      "loss": 1.5968,
      "step": 2820
    },
    {
      "epoch": 2.03012912482066,
      "grad_norm": 4.36281681060791,
      "learning_rate": 6.456948734446624e-06,
      "loss": 1.6224,
      "step": 2830
    },
    {
      "epoch": 2.037302725968436,
      "grad_norm": 4.7408671379089355,
      "learning_rate": 6.43537546117158e-06,
      "loss": 1.5918,
      "step": 2840
    },
    {
      "epoch": 2.0444763271162123,
      "grad_norm": 4.659893035888672,
      "learning_rate": 6.413773027127623e-06,
      "loss": 1.6505,
      "step": 2850
    },
    {
      "epoch": 2.0516499282639886,
      "grad_norm": 5.467820167541504,
      "learning_rate": 6.392141871185004e-06,
      "loss": 1.5785,
      "step": 2860
    },
    {
      "epoch": 2.0588235294117645,
      "grad_norm": 4.07796049118042,
      "learning_rate": 6.370482432797476e-06,
      "loss": 1.6068,
      "step": 2870
    },
    {
      "epoch": 2.065997130559541,
      "grad_norm": 4.329823017120361,
      "learning_rate": 6.34879515199338e-06,
      "loss": 1.6593,
      "step": 2880
    },
    {
      "epoch": 2.073170731707317,
      "grad_norm": 4.083209991455078,
      "learning_rate": 6.327080469366694e-06,
      "loss": 1.6386,
      "step": 2890
    },
    {
      "epoch": 2.0803443328550935,
      "grad_norm": 4.150362491607666,
      "learning_rate": 6.305338826068082e-06,
      "loss": 1.5473,
      "step": 2900
    },
    {
      "epoch": 2.0875179340028693,
      "grad_norm": 4.753579616546631,
      "learning_rate": 6.283570663795943e-06,
      "loss": 1.5706,
      "step": 2910
    },
    {
      "epoch": 2.0946915351506457,
      "grad_norm": 4.7086944580078125,
      "learning_rate": 6.261776424787418e-06,
      "loss": 1.6254,
      "step": 2920
    },
    {
      "epoch": 2.101865136298422,
      "grad_norm": 4.923233985900879,
      "learning_rate": 6.23995655180943e-06,
      "loss": 1.6115,
      "step": 2930
    },
    {
      "epoch": 2.109038737446198,
      "grad_norm": 4.275697708129883,
      "learning_rate": 6.2181114881496665e-06,
      "loss": 1.5643,
      "step": 2940
    },
    {
      "epoch": 2.116212338593974,
      "grad_norm": 4.130299091339111,
      "learning_rate": 6.1962416776075864e-06,
      "loss": 1.5828,
      "step": 2950
    },
    {
      "epoch": 2.1233859397417505,
      "grad_norm": 3.8978536128997803,
      "learning_rate": 6.174347564485404e-06,
      "loss": 1.5797,
      "step": 2960
    },
    {
      "epoch": 2.1305595408895264,
      "grad_norm": 4.327828407287598,
      "learning_rate": 6.152429593579051e-06,
      "loss": 1.6265,
      "step": 2970
    },
    {
      "epoch": 2.1377331420373027,
      "grad_norm": 3.2009191513061523,
      "learning_rate": 6.130488210169159e-06,
      "loss": 1.632,
      "step": 2980
    },
    {
      "epoch": 2.144906743185079,
      "grad_norm": 4.210965633392334,
      "learning_rate": 6.1085238600119955e-06,
      "loss": 1.6722,
      "step": 2990
    },
    {
      "epoch": 2.152080344332855,
      "grad_norm": 4.629586219787598,
      "learning_rate": 6.086536989330417e-06,
      "loss": 1.5,
      "step": 3000
    },
    {
      "epoch": 2.159253945480631,
      "grad_norm": 3.74835467338562,
      "learning_rate": 6.064528044804805e-06,
      "loss": 1.54,
      "step": 3010
    },
    {
      "epoch": 2.1664275466284075,
      "grad_norm": 4.027993679046631,
      "learning_rate": 6.042497473563984e-06,
      "loss": 1.6397,
      "step": 3020
    },
    {
      "epoch": 2.173601147776184,
      "grad_norm": 3.9106898307800293,
      "learning_rate": 6.020445723176144e-06,
      "loss": 1.6342,
      "step": 3030
    },
    {
      "epoch": 2.1807747489239597,
      "grad_norm": 4.205453395843506,
      "learning_rate": 5.998373241639745e-06,
      "loss": 1.5694,
      "step": 3040
    },
    {
      "epoch": 2.187948350071736,
      "grad_norm": 4.477112293243408,
      "learning_rate": 5.97628047737442e-06,
      "loss": 1.5453,
      "step": 3050
    },
    {
      "epoch": 2.1951219512195124,
      "grad_norm": 4.558213233947754,
      "learning_rate": 5.954167879211854e-06,
      "loss": 1.6145,
      "step": 3060
    },
    {
      "epoch": 2.2022955523672882,
      "grad_norm": 3.5908589363098145,
      "learning_rate": 5.932035896386683e-06,
      "loss": 1.6056,
      "step": 3070
    },
    {
      "epoch": 2.2094691535150646,
      "grad_norm": 4.468430995941162,
      "learning_rate": 5.909884978527349e-06,
      "loss": 1.6614,
      "step": 3080
    },
    {
      "epoch": 2.216642754662841,
      "grad_norm": 5.002074718475342,
      "learning_rate": 5.887715575646976e-06,
      "loss": 1.6524,
      "step": 3090
    },
    {
      "epoch": 2.2238163558106168,
      "grad_norm": 4.667896270751953,
      "learning_rate": 5.865528138134229e-06,
      "loss": 1.6076,
      "step": 3100
    },
    {
      "epoch": 2.230989956958393,
      "grad_norm": 4.269671440124512,
      "learning_rate": 5.8433231167441574e-06,
      "loss": 1.6086,
      "step": 3110
    },
    {
      "epoch": 2.2381635581061694,
      "grad_norm": 4.375303268432617,
      "learning_rate": 5.821100962589041e-06,
      "loss": 1.5903,
      "step": 3120
    },
    {
      "epoch": 2.2453371592539453,
      "grad_norm": 3.6031107902526855,
      "learning_rate": 5.798862127129228e-06,
      "loss": 1.554,
      "step": 3130
    },
    {
      "epoch": 2.2525107604017216,
      "grad_norm": 4.405447959899902,
      "learning_rate": 5.776607062163953e-06,
      "loss": 1.6054,
      "step": 3140
    },
    {
      "epoch": 2.259684361549498,
      "grad_norm": 4.474941253662109,
      "learning_rate": 5.754336219822176e-06,
      "loss": 1.5854,
      "step": 3150
    },
    {
      "epoch": 2.266857962697274,
      "grad_norm": 4.63859748840332,
      "learning_rate": 5.732050052553377e-06,
      "loss": 1.6289,
      "step": 3160
    },
    {
      "epoch": 2.27403156384505,
      "grad_norm": 4.265592575073242,
      "learning_rate": 5.709749013118381e-06,
      "loss": 1.5863,
      "step": 3170
    },
    {
      "epoch": 2.2812051649928264,
      "grad_norm": 4.398319244384766,
      "learning_rate": 5.687433554580148e-06,
      "loss": 1.6214,
      "step": 3180
    },
    {
      "epoch": 2.2883787661406028,
      "grad_norm": 4.61810302734375,
      "learning_rate": 5.665104130294574e-06,
      "loss": 1.6352,
      "step": 3190
    },
    {
      "epoch": 2.2955523672883786,
      "grad_norm": 3.856745481491089,
      "learning_rate": 5.6427611939012834e-06,
      "loss": 1.6752,
      "step": 3200
    },
    {
      "epoch": 2.302725968436155,
      "grad_norm": 4.372683525085449,
      "learning_rate": 5.620405199314407e-06,
      "loss": 1.5983,
      "step": 3210
    },
    {
      "epoch": 2.3098995695839313,
      "grad_norm": 4.271259784698486,
      "learning_rate": 5.598036600713359e-06,
      "loss": 1.6765,
      "step": 3220
    },
    {
      "epoch": 2.317073170731707,
      "grad_norm": 4.324578762054443,
      "learning_rate": 5.575655852533625e-06,
      "loss": 1.5732,
      "step": 3230
    },
    {
      "epoch": 2.3242467718794835,
      "grad_norm": 5.443700790405273,
      "learning_rate": 5.553263409457504e-06,
      "loss": 1.5889,
      "step": 3240
    },
    {
      "epoch": 2.33142037302726,
      "grad_norm": 4.347670078277588,
      "learning_rate": 5.5308597264049e-06,
      "loss": 1.6189,
      "step": 3250
    },
    {
      "epoch": 2.338593974175036,
      "grad_norm": 4.7325005531311035,
      "learning_rate": 5.508445258524057e-06,
      "loss": 1.5651,
      "step": 3260
    },
    {
      "epoch": 2.345767575322812,
      "grad_norm": 4.769260406494141,
      "learning_rate": 5.486020461182323e-06,
      "loss": 1.5643,
      "step": 3270
    },
    {
      "epoch": 2.3529411764705883,
      "grad_norm": 4.687795162200928,
      "learning_rate": 5.463585789956899e-06,
      "loss": 1.6216,
      "step": 3280
    },
    {
      "epoch": 2.3601147776183646,
      "grad_norm": 5.211471080780029,
      "learning_rate": 5.44114170062558e-06,
      "loss": 1.6848,
      "step": 3290
    },
    {
      "epoch": 2.3672883787661405,
      "grad_norm": 4.001917362213135,
      "learning_rate": 5.4186886491574966e-06,
      "loss": 1.5608,
      "step": 3300
    },
    {
      "epoch": 2.374461979913917,
      "grad_norm": 4.999955177307129,
      "learning_rate": 5.396227091703856e-06,
      "loss": 1.6118,
      "step": 3310
    },
    {
      "epoch": 2.381635581061693,
      "grad_norm": 5.1315131187438965,
      "learning_rate": 5.373757484588664e-06,
      "loss": 1.5825,
      "step": 3320
    },
    {
      "epoch": 2.388809182209469,
      "grad_norm": 4.094560146331787,
      "learning_rate": 5.351280284299469e-06,
      "loss": 1.623,
      "step": 3330
    },
    {
      "epoch": 2.3959827833572453,
      "grad_norm": 4.049377918243408,
      "learning_rate": 5.328795947478077e-06,
      "loss": 1.6059,
      "step": 3340
    },
    {
      "epoch": 2.4031563845050217,
      "grad_norm": 4.680478096008301,
      "learning_rate": 5.306304930911278e-06,
      "loss": 1.5584,
      "step": 3350
    },
    {
      "epoch": 2.4103299856527975,
      "grad_norm": 3.9925215244293213,
      "learning_rate": 5.283807691521567e-06,
      "loss": 1.5721,
      "step": 3360
    },
    {
      "epoch": 2.417503586800574,
      "grad_norm": 4.742248058319092,
      "learning_rate": 5.261304686357859e-06,
      "loss": 1.637,
      "step": 3370
    },
    {
      "epoch": 2.42467718794835,
      "grad_norm": 4.040034294128418,
      "learning_rate": 5.238796372586207e-06,
      "loss": 1.5351,
      "step": 3380
    },
    {
      "epoch": 2.431850789096126,
      "grad_norm": 5.521610736846924,
      "learning_rate": 5.216283207480516e-06,
      "loss": 1.5446,
      "step": 3390
    },
    {
      "epoch": 2.4390243902439024,
      "grad_norm": 5.095225811004639,
      "learning_rate": 5.193765648413238e-06,
      "loss": 1.6123,
      "step": 3400
    },
    {
      "epoch": 2.4461979913916787,
      "grad_norm": 4.060489654541016,
      "learning_rate": 5.171244152846107e-06,
      "loss": 1.5664,
      "step": 3410
    },
    {
      "epoch": 2.4533715925394546,
      "grad_norm": 3.9281005859375,
      "learning_rate": 5.148719178320818e-06,
      "loss": 1.5962,
      "step": 3420
    },
    {
      "epoch": 2.460545193687231,
      "grad_norm": 4.570250034332275,
      "learning_rate": 5.126191182449749e-06,
      "loss": 1.548,
      "step": 3430
    },
    {
      "epoch": 2.4677187948350072,
      "grad_norm": 5.6110992431640625,
      "learning_rate": 5.103660622906659e-06,
      "loss": 1.5879,
      "step": 3440
    },
    {
      "epoch": 2.4748923959827835,
      "grad_norm": 4.34107780456543,
      "learning_rate": 5.081127957417387e-06,
      "loss": 1.5857,
      "step": 3450
    },
    {
      "epoch": 2.4820659971305594,
      "grad_norm": 4.172976016998291,
      "learning_rate": 5.05859364375056e-06,
      "loss": 1.5943,
      "step": 3460
    },
    {
      "epoch": 2.4892395982783357,
      "grad_norm": 3.377927780151367,
      "learning_rate": 5.03605813970829e-06,
      "loss": 1.6087,
      "step": 3470
    },
    {
      "epoch": 2.496413199426112,
      "grad_norm": 4.80830192565918,
      "learning_rate": 5.013521903116862e-06,
      "loss": 1.5942,
      "step": 3480
    },
    {
      "epoch": 2.503586800573888,
      "grad_norm": 3.5339808464050293,
      "learning_rate": 4.9909853918174565e-06,
      "loss": 1.5772,
      "step": 3490
    },
    {
      "epoch": 2.5107604017216643,
      "grad_norm": 3.645095109939575,
      "learning_rate": 4.968449063656829e-06,
      "loss": 1.5697,
      "step": 3500
    },
    {
      "epoch": 2.5179340028694406,
      "grad_norm": 5.039391040802002,
      "learning_rate": 4.945913376478011e-06,
      "loss": 1.5369,
      "step": 3510
    },
    {
      "epoch": 2.525107604017217,
      "grad_norm": 4.61375617980957,
      "learning_rate": 4.923378788111019e-06,
      "loss": 1.5834,
      "step": 3520
    },
    {
      "epoch": 2.5322812051649928,
      "grad_norm": 4.470226287841797,
      "learning_rate": 4.900845756363541e-06,
      "loss": 1.5755,
      "step": 3530
    },
    {
      "epoch": 2.539454806312769,
      "grad_norm": 4.5518879890441895,
      "learning_rate": 4.878314739011646e-06,
      "loss": 1.522,
      "step": 3540
    },
    {
      "epoch": 2.5466284074605454,
      "grad_norm": 4.295494079589844,
      "learning_rate": 4.855786193790471e-06,
      "loss": 1.6064,
      "step": 3550
    },
    {
      "epoch": 2.5538020086083213,
      "grad_norm": 4.549643516540527,
      "learning_rate": 4.8332605783849366e-06,
      "loss": 1.5963,
      "step": 3560
    },
    {
      "epoch": 2.5609756097560976,
      "grad_norm": 4.887781620025635,
      "learning_rate": 4.810738350420442e-06,
      "loss": 1.5344,
      "step": 3570
    },
    {
      "epoch": 2.568149210903874,
      "grad_norm": 4.554413795471191,
      "learning_rate": 4.788219967453565e-06,
      "loss": 1.6128,
      "step": 3580
    },
    {
      "epoch": 2.57532281205165,
      "grad_norm": 4.4911088943481445,
      "learning_rate": 4.765705886962767e-06,
      "loss": 1.5734,
      "step": 3590
    },
    {
      "epoch": 2.582496413199426,
      "grad_norm": 3.9210941791534424,
      "learning_rate": 4.743196566339108e-06,
      "loss": 1.4737,
      "step": 3600
    },
    {
      "epoch": 2.5896700143472025,
      "grad_norm": 4.929757595062256,
      "learning_rate": 4.720692462876942e-06,
      "loss": 1.5953,
      "step": 3610
    },
    {
      "epoch": 2.5968436154949783,
      "grad_norm": 5.522550106048584,
      "learning_rate": 4.698194033764638e-06,
      "loss": 1.6455,
      "step": 3620
    },
    {
      "epoch": 2.6040172166427547,
      "grad_norm": 5.086490154266357,
      "learning_rate": 4.675701736075277e-06,
      "loss": 1.5794,
      "step": 3630
    },
    {
      "epoch": 2.611190817790531,
      "grad_norm": 4.802802562713623,
      "learning_rate": 4.653216026757383e-06,
      "loss": 1.6637,
      "step": 3640
    },
    {
      "epoch": 2.618364418938307,
      "grad_norm": 4.139135360717773,
      "learning_rate": 4.630737362625631e-06,
      "loss": 1.5673,
      "step": 3650
    },
    {
      "epoch": 2.625538020086083,
      "grad_norm": 4.72154426574707,
      "learning_rate": 4.608266200351564e-06,
      "loss": 1.5771,
      "step": 3660
    },
    {
      "epoch": 2.6327116212338595,
      "grad_norm": 4.340199947357178,
      "learning_rate": 4.585802996454327e-06,
      "loss": 1.564,
      "step": 3670
    },
    {
      "epoch": 2.6398852223816354,
      "grad_norm": 5.623989582061768,
      "learning_rate": 4.563348207291373e-06,
      "loss": 1.6147,
      "step": 3680
    },
    {
      "epoch": 2.6470588235294117,
      "grad_norm": 5.085665702819824,
      "learning_rate": 4.54090228904921e-06,
      "loss": 1.6191,
      "step": 3690
    },
    {
      "epoch": 2.654232424677188,
      "grad_norm": 4.414031982421875,
      "learning_rate": 4.518465697734126e-06,
      "loss": 1.5833,
      "step": 3700
    },
    {
      "epoch": 2.661406025824964,
      "grad_norm": 5.087235450744629,
      "learning_rate": 4.496038889162928e-06,
      "loss": 1.6422,
      "step": 3710
    },
    {
      "epoch": 2.66857962697274,
      "grad_norm": 4.981625080108643,
      "learning_rate": 4.473622318953669e-06,
      "loss": 1.5344,
      "step": 3720
    },
    {
      "epoch": 2.6757532281205165,
      "grad_norm": 4.593977928161621,
      "learning_rate": 4.451216442516413e-06,
      "loss": 1.6018,
      "step": 3730
    },
    {
      "epoch": 2.682926829268293,
      "grad_norm": 5.477519989013672,
      "learning_rate": 4.4288217150439635e-06,
      "loss": 1.5434,
      "step": 3740
    },
    {
      "epoch": 2.6901004304160687,
      "grad_norm": 5.098538875579834,
      "learning_rate": 4.406438591502632e-06,
      "loss": 1.5243,
      "step": 3750
    },
    {
      "epoch": 2.697274031563845,
      "grad_norm": 4.711662769317627,
      "learning_rate": 4.384067526622978e-06,
      "loss": 1.6024,
      "step": 3760
    },
    {
      "epoch": 2.7044476327116214,
      "grad_norm": 4.055436134338379,
      "learning_rate": 4.361708974890585e-06,
      "loss": 1.511,
      "step": 3770
    },
    {
      "epoch": 2.7116212338593977,
      "grad_norm": 4.575531959533691,
      "learning_rate": 4.339363390536825e-06,
      "loss": 1.57,
      "step": 3780
    },
    {
      "epoch": 2.7187948350071736,
      "grad_norm": 4.232922077178955,
      "learning_rate": 4.317031227529623e-06,
      "loss": 1.5704,
      "step": 3790
    },
    {
      "epoch": 2.72596843615495,
      "grad_norm": 4.285029411315918,
      "learning_rate": 4.294712939564238e-06,
      "loss": 1.5333,
      "step": 3800
    },
    {
      "epoch": 2.733142037302726,
      "grad_norm": 5.382098197937012,
      "learning_rate": 4.27240898005405e-06,
      "loss": 1.6704,
      "step": 3810
    },
    {
      "epoch": 2.740315638450502,
      "grad_norm": 5.523699760437012,
      "learning_rate": 4.250119802121344e-06,
      "loss": 1.6044,
      "step": 3820
    },
    {
      "epoch": 2.7474892395982784,
      "grad_norm": 5.353147983551025,
      "learning_rate": 4.227845858588109e-06,
      "loss": 1.4982,
      "step": 3830
    },
    {
      "epoch": 2.7546628407460547,
      "grad_norm": 4.986764907836914,
      "learning_rate": 4.207812708827856e-06,
      "loss": 1.5282,
      "step": 3840
    },
    {
      "epoch": 2.7618364418938306,
      "grad_norm": 4.34943962097168,
      "learning_rate": 4.1855689570623045e-06,
      "loss": 1.5824,
      "step": 3850
    },
    {
      "epoch": 2.769010043041607,
      "grad_norm": 4.544363498687744,
      "learning_rate": 4.163341751096846e-06,
      "loss": 1.5475,
      "step": 3860
    },
    {
      "epoch": 2.7761836441893832,
      "grad_norm": 4.723612308502197,
      "learning_rate": 4.141131542494457e-06,
      "loss": 1.6123,
      "step": 3870
    },
    {
      "epoch": 2.783357245337159,
      "grad_norm": 4.292191982269287,
      "learning_rate": 4.118938782472801e-06,
      "loss": 1.576,
      "step": 3880
    },
    {
      "epoch": 2.7905308464849354,
      "grad_norm": 4.396968841552734,
      "learning_rate": 4.096763921895058e-06,
      "loss": 1.5365,
      "step": 3890
    },
    {
      "epoch": 2.7977044476327118,
      "grad_norm": 4.8770952224731445,
      "learning_rate": 4.074607411260772e-06,
      "loss": 1.6285,
      "step": 3900
    },
    {
      "epoch": 2.8048780487804876,
      "grad_norm": 4.614500045776367,
      "learning_rate": 4.052469700696685e-06,
      "loss": 1.5171,
      "step": 3910
    },
    {
      "epoch": 2.812051649928264,
      "grad_norm": 4.856694221496582,
      "learning_rate": 4.030351239947606e-06,
      "loss": 1.4856,
      "step": 3920
    },
    {
      "epoch": 2.8192252510760403,
      "grad_norm": 4.708069801330566,
      "learning_rate": 4.00825247836727e-06,
      "loss": 1.6181,
      "step": 3930
    },
    {
      "epoch": 2.826398852223816,
      "grad_norm": 4.426767349243164,
      "learning_rate": 3.9861738649092095e-06,
      "loss": 1.5186,
      "step": 3940
    },
    {
      "epoch": 2.8335724533715925,
      "grad_norm": 4.7677741050720215,
      "learning_rate": 3.964115848117623e-06,
      "loss": 1.5406,
      "step": 3950
    },
    {
      "epoch": 2.840746054519369,
      "grad_norm": 4.711234092712402,
      "learning_rate": 3.942078876118283e-06,
      "loss": 1.6106,
      "step": 3960
    },
    {
      "epoch": 2.8479196556671447,
      "grad_norm": 5.483160495758057,
      "learning_rate": 3.920063396609414e-06,
      "loss": 1.6093,
      "step": 3970
    },
    {
      "epoch": 2.855093256814921,
      "grad_norm": 4.5186004638671875,
      "learning_rate": 3.898069856852607e-06,
      "loss": 1.6152,
      "step": 3980
    },
    {
      "epoch": 2.8622668579626973,
      "grad_norm": 3.765223979949951,
      "learning_rate": 3.876098703663727e-06,
      "loss": 1.6138,
      "step": 3990
    },
    {
      "epoch": 2.869440459110473,
      "grad_norm": 6.776340484619141,
      "learning_rate": 3.854150383403839e-06,
      "loss": 1.5753,
      "step": 4000
    },
    {
      "epoch": 2.8766140602582495,
      "grad_norm": 3.8319530487060547,
      "learning_rate": 3.832225341970143e-06,
      "loss": 1.5731,
      "step": 4010
    },
    {
      "epoch": 2.883787661406026,
      "grad_norm": 7.7807416915893555,
      "learning_rate": 3.8103240247869077e-06,
      "loss": 1.6089,
      "step": 4020
    },
    {
      "epoch": 2.890961262553802,
      "grad_norm": 4.956257343292236,
      "learning_rate": 3.7884468767964242e-06,
      "loss": 1.5275,
      "step": 4030
    },
    {
      "epoch": 2.8981348637015785,
      "grad_norm": 4.5545973777771,
      "learning_rate": 3.7665943424499736e-06,
      "loss": 1.5825,
      "step": 4040
    },
    {
      "epoch": 2.9053084648493543,
      "grad_norm": 4.277187347412109,
      "learning_rate": 3.744766865698784e-06,
      "loss": 1.4575,
      "step": 4050
    },
    {
      "epoch": 2.9124820659971307,
      "grad_norm": 5.118729591369629,
      "learning_rate": 3.7229648899850267e-06,
      "loss": 1.5861,
      "step": 4060
    },
    {
      "epoch": 2.919655667144907,
      "grad_norm": 5.576146602630615,
      "learning_rate": 3.7011888582327925e-06,
      "loss": 1.5741,
      "step": 4070
    },
    {
      "epoch": 2.926829268292683,
      "grad_norm": 4.842383861541748,
      "learning_rate": 3.6794392128391033e-06,
      "loss": 1.5742,
      "step": 4080
    },
    {
      "epoch": 2.934002869440459,
      "grad_norm": 4.534346103668213,
      "learning_rate": 3.6577163956649247e-06,
      "loss": 1.5906,
      "step": 4090
    },
    {
      "epoch": 2.9411764705882355,
      "grad_norm": 4.8551435470581055,
      "learning_rate": 3.636020848026183e-06,
      "loss": 1.5673,
      "step": 4100
    },
    {
      "epoch": 2.9483500717360114,
      "grad_norm": 5.310166358947754,
      "learning_rate": 3.6143530106848e-06,
      "loss": 1.5057,
      "step": 4110
    },
    {
      "epoch": 2.9555236728837877,
      "grad_norm": 5.193301677703857,
      "learning_rate": 3.5927133238397473e-06,
      "loss": 1.5722,
      "step": 4120
    },
    {
      "epoch": 2.962697274031564,
      "grad_norm": 4.950436115264893,
      "learning_rate": 3.5711022271180918e-06,
      "loss": 1.5931,
      "step": 4130
    },
    {
      "epoch": 2.96987087517934,
      "grad_norm": 4.82004976272583,
      "learning_rate": 3.5495201595660755e-06,
      "loss": 1.5832,
      "step": 4140
    },
    {
      "epoch": 2.977044476327116,
      "grad_norm": 5.156952857971191,
      "learning_rate": 3.5279675596401846e-06,
      "loss": 1.6045,
      "step": 4150
    },
    {
      "epoch": 2.9842180774748925,
      "grad_norm": 5.17929744720459,
      "learning_rate": 3.5064448651982498e-06,
      "loss": 1.6699,
      "step": 4160
    },
    {
      "epoch": 2.9913916786226684,
      "grad_norm": 5.571254253387451,
      "learning_rate": 3.4849525134905503e-06,
      "loss": 1.5295,
      "step": 4170
    },
    {
      "epoch": 2.9985652797704447,
      "grad_norm": 5.106192111968994,
      "learning_rate": 3.463490941150928e-06,
      "loss": 1.5733,
      "step": 4180
    },
    {
      "epoch": 3.005738880918221,
      "grad_norm": 4.98599910736084,
      "learning_rate": 3.4420605841879145e-06,
      "loss": 1.6235,
      "step": 4190
    },
    {
      "epoch": 3.012912482065997,
      "grad_norm": 3.6945202350616455,
      "learning_rate": 3.4206618779758826e-06,
      "loss": 1.5349,
      "step": 4200
    },
    {
      "epoch": 3.0200860832137733,
      "grad_norm": 4.957498073577881,
      "learning_rate": 3.3992952572461912e-06,
      "loss": 1.5463,
      "step": 4210
    },
    {
      "epoch": 3.0272596843615496,
      "grad_norm": 4.388752460479736,
      "learning_rate": 3.3779611560783605e-06,
      "loss": 1.5902,
      "step": 4220
    },
    {
      "epoch": 3.034433285509326,
      "grad_norm": 4.970491886138916,
      "learning_rate": 3.356660007891249e-06,
      "loss": 1.6759,
      "step": 4230
    },
    {
      "epoch": 3.0416068866571018,
      "grad_norm": 5.330323219299316,
      "learning_rate": 3.335392245434248e-06,
      "loss": 1.5794,
      "step": 4240
    },
    {
      "epoch": 3.048780487804878,
      "grad_norm": 5.6723151206970215,
      "learning_rate": 3.314158300778496e-06,
      "loss": 1.5722,
      "step": 4250
    },
    {
      "epoch": 3.0559540889526544,
      "grad_norm": 5.204591751098633,
      "learning_rate": 3.292958605308092e-06,
      "loss": 1.6041,
      "step": 4260
    },
    {
      "epoch": 3.0631276901004303,
      "grad_norm": 5.039647579193115,
      "learning_rate": 3.2717935897113418e-06,
      "loss": 1.5349,
      "step": 4270
    },
    {
      "epoch": 3.0703012912482066,
      "grad_norm": 4.964313983917236,
      "learning_rate": 3.250663683971996e-06,
      "loss": 1.606,
      "step": 4280
    },
    {
      "epoch": 3.077474892395983,
      "grad_norm": 3.8116466999053955,
      "learning_rate": 3.2295693173605234e-06,
      "loss": 1.5322,
      "step": 4290
    },
    {
      "epoch": 3.084648493543759,
      "grad_norm": 4.780685901641846,
      "learning_rate": 3.2085109184253925e-06,
      "loss": 1.5661,
      "step": 4300
    },
    {
      "epoch": 3.091822094691535,
      "grad_norm": 5.638141632080078,
      "learning_rate": 3.187488914984356e-06,
      "loss": 1.586,
      "step": 4310
    },
    {
      "epoch": 3.0989956958393114,
      "grad_norm": 5.581665515899658,
      "learning_rate": 3.1665037341157605e-06,
      "loss": 1.5655,
      "step": 4320
    },
    {
      "epoch": 3.1061692969870873,
      "grad_norm": 4.704768657684326,
      "learning_rate": 3.145555802149881e-06,
      "loss": 1.492,
      "step": 4330
    },
    {
      "epoch": 3.1133428981348636,
      "grad_norm": 5.256568431854248,
      "learning_rate": 3.1246455446602475e-06,
      "loss": 1.6036,
      "step": 4340
    },
    {
      "epoch": 3.12051649928264,
      "grad_norm": 4.658392429351807,
      "learning_rate": 3.103773386455006e-06,
      "loss": 1.5651,
      "step": 4350
    },
    {
      "epoch": 3.1276901004304163,
      "grad_norm": 4.463980197906494,
      "learning_rate": 3.0829397515682825e-06,
      "loss": 1.6081,
      "step": 4360
    },
    {
      "epoch": 3.134863701578192,
      "grad_norm": 4.1332831382751465,
      "learning_rate": 3.062145063251574e-06,
      "loss": 1.6852,
      "step": 4370
    },
    {
      "epoch": 3.1420373027259685,
      "grad_norm": 5.186557769775391,
      "learning_rate": 3.0413897439651487e-06,
      "loss": 1.5986,
      "step": 4380
    },
    {
      "epoch": 3.149210903873745,
      "grad_norm": 4.409435272216797,
      "learning_rate": 3.0206742153694617e-06,
      "loss": 1.5196,
      "step": 4390
    },
    {
      "epoch": 3.1563845050215207,
      "grad_norm": 4.217126846313477,
      "learning_rate": 2.9999988983165863e-06,
      "loss": 1.5215,
      "step": 4400
    },
    {
      "epoch": 3.163558106169297,
      "grad_norm": 4.19410514831543,
      "learning_rate": 2.9793642128416716e-06,
      "loss": 1.5684,
      "step": 4410
    },
    {
      "epoch": 3.1707317073170733,
      "grad_norm": 5.336087703704834,
      "learning_rate": 2.9587705781544007e-06,
      "loss": 1.5931,
      "step": 4420
    },
    {
      "epoch": 3.177905308464849,
      "grad_norm": 4.636111259460449,
      "learning_rate": 2.9382184126304834e-06,
      "loss": 1.5354,
      "step": 4430
    },
    {
      "epoch": 3.1850789096126255,
      "grad_norm": 4.585898399353027,
      "learning_rate": 2.9177081338031437e-06,
      "loss": 1.5215,
      "step": 4440
    },
    {
      "epoch": 3.192252510760402,
      "grad_norm": 4.8940582275390625,
      "learning_rate": 2.8972401583546495e-06,
      "loss": 1.5747,
      "step": 4450
    },
    {
      "epoch": 3.1994261119081777,
      "grad_norm": 4.767278671264648,
      "learning_rate": 2.876814902107842e-06,
      "loss": 1.5259,
      "step": 4460
    },
    {
      "epoch": 3.206599713055954,
      "grad_norm": 5.463029384613037,
      "learning_rate": 2.8564327800176905e-06,
      "loss": 1.6067,
      "step": 4470
    },
    {
      "epoch": 3.2137733142037304,
      "grad_norm": 4.554108142852783,
      "learning_rate": 2.836094206162856e-06,
      "loss": 1.5775,
      "step": 4480
    },
    {
      "epoch": 3.2209469153515062,
      "grad_norm": 4.334529876708984,
      "learning_rate": 2.815799593737285e-06,
      "loss": 1.4881,
      "step": 4490
    },
    {
      "epoch": 3.2281205164992826,
      "grad_norm": 4.633631229400635,
      "learning_rate": 2.7955493550418144e-06,
      "loss": 1.5282,
      "step": 4500
    },
    {
      "epoch": 3.235294117647059,
      "grad_norm": 5.7441020011901855,
      "learning_rate": 2.7753439014757956e-06,
      "loss": 1.5701,
      "step": 4510
    },
    {
      "epoch": 3.242467718794835,
      "grad_norm": 5.130382537841797,
      "learning_rate": 2.7551836435287306e-06,
      "loss": 1.5804,
      "step": 4520
    },
    {
      "epoch": 3.249641319942611,
      "grad_norm": 4.696423530578613,
      "learning_rate": 2.735068990771943e-06,
      "loss": 1.5826,
      "step": 4530
    },
    {
      "epoch": 3.2568149210903874,
      "grad_norm": 5.125661849975586,
      "learning_rate": 2.7150003518502453e-06,
      "loss": 1.565,
      "step": 4540
    },
    {
      "epoch": 3.2639885222381637,
      "grad_norm": 5.408405780792236,
      "learning_rate": 2.694978134473649e-06,
      "loss": 1.5571,
      "step": 4550
    },
    {
      "epoch": 3.2711621233859396,
      "grad_norm": 5.13258695602417,
      "learning_rate": 2.675002745409071e-06,
      "loss": 1.5786,
      "step": 4560
    },
    {
      "epoch": 3.278335724533716,
      "grad_norm": 4.730387210845947,
      "learning_rate": 2.6550745904720774e-06,
      "loss": 1.6551,
      "step": 4570
    },
    {
      "epoch": 3.2855093256814922,
      "grad_norm": 4.820046424865723,
      "learning_rate": 2.6351940745186384e-06,
      "loss": 1.4968,
      "step": 4580
    },
    {
      "epoch": 3.292682926829268,
      "grad_norm": 5.0762739181518555,
      "learning_rate": 2.615361601436896e-06,
      "loss": 1.5626,
      "step": 4590
    },
    {
      "epoch": 3.2998565279770444,
      "grad_norm": 5.274113178253174,
      "learning_rate": 2.5955775741389646e-06,
      "loss": 1.5517,
      "step": 4600
    },
    {
      "epoch": 3.3070301291248207,
      "grad_norm": 4.58626651763916,
      "learning_rate": 2.5758423945527485e-06,
      "loss": 1.4916,
      "step": 4610
    },
    {
      "epoch": 3.314203730272597,
      "grad_norm": 5.414462566375732,
      "learning_rate": 2.55615646361377e-06,
      "loss": 1.6356,
      "step": 4620
    },
    {
      "epoch": 3.321377331420373,
      "grad_norm": 4.502676486968994,
      "learning_rate": 2.5365201812570318e-06,
      "loss": 1.4966,
      "step": 4630
    },
    {
      "epoch": 3.3285509325681493,
      "grad_norm": 5.824394702911377,
      "learning_rate": 2.516933946408878e-06,
      "loss": 1.5698,
      "step": 4640
    },
    {
      "epoch": 3.3357245337159256,
      "grad_norm": 5.289314270019531,
      "learning_rate": 2.4973981569789042e-06,
      "loss": 1.5215,
      "step": 4650
    },
    {
      "epoch": 3.3428981348637015,
      "grad_norm": 4.284488677978516,
      "learning_rate": 2.477913209851868e-06,
      "loss": 1.566,
      "step": 4660
    },
    {
      "epoch": 3.350071736011478,
      "grad_norm": 5.290041923522949,
      "learning_rate": 2.458479500879627e-06,
      "loss": 1.5467,
      "step": 4670
    },
    {
      "epoch": 3.357245337159254,
      "grad_norm": 5.2476091384887695,
      "learning_rate": 2.4390974248730907e-06,
      "loss": 1.5542,
      "step": 4680
    },
    {
      "epoch": 3.36441893830703,
      "grad_norm": 5.671301364898682,
      "learning_rate": 2.4197673755942135e-06,
      "loss": 1.6222,
      "step": 4690
    },
    {
      "epoch": 3.3715925394548063,
      "grad_norm": 4.909127235412598,
      "learning_rate": 2.400489745747976e-06,
      "loss": 1.5464,
      "step": 4700
    },
    {
      "epoch": 3.3787661406025826,
      "grad_norm": 4.422154903411865,
      "learning_rate": 2.3812649269744263e-06,
      "loss": 1.5883,
      "step": 4710
    },
    {
      "epoch": 3.3859397417503585,
      "grad_norm": 4.894751071929932,
      "learning_rate": 2.3620933098407077e-06,
      "loss": 1.594,
      "step": 4720
    },
    {
      "epoch": 3.393113342898135,
      "grad_norm": 5.899623394012451,
      "learning_rate": 2.342975283833135e-06,
      "loss": 1.6227,
      "step": 4730
    },
    {
      "epoch": 3.400286944045911,
      "grad_norm": 4.897150039672852,
      "learning_rate": 2.323911237349278e-06,
      "loss": 1.6003,
      "step": 4740
    },
    {
      "epoch": 3.407460545193687,
      "grad_norm": 4.9380316734313965,
      "learning_rate": 2.3049015576900678e-06,
      "loss": 1.5328,
      "step": 4750
    },
    {
      "epoch": 3.4146341463414633,
      "grad_norm": 5.834519386291504,
      "learning_rate": 2.2859466310519298e-06,
      "loss": 1.648,
      "step": 4760
    },
    {
      "epoch": 3.4218077474892397,
      "grad_norm": 4.8633527755737305,
      "learning_rate": 2.2670468425189427e-06,
      "loss": 1.5735,
      "step": 4770
    },
    {
      "epoch": 3.4289813486370155,
      "grad_norm": 5.286941051483154,
      "learning_rate": 2.2482025760550126e-06,
      "loss": 1.6808,
      "step": 4780
    },
    {
      "epoch": 3.436154949784792,
      "grad_norm": 5.844997406005859,
      "learning_rate": 2.2294142144960694e-06,
      "loss": 1.6354,
      "step": 4790
    },
    {
      "epoch": 3.443328550932568,
      "grad_norm": 4.110740661621094,
      "learning_rate": 2.2106821395422912e-06,
      "loss": 1.5817,
      "step": 4800
    },
    {
      "epoch": 3.4505021520803445,
      "grad_norm": 5.434201240539551,
      "learning_rate": 2.1920067317503474e-06,
      "loss": 1.5484,
      "step": 4810
    },
    {
      "epoch": 3.4576757532281204,
      "grad_norm": 5.193006992340088,
      "learning_rate": 2.1733883705256748e-06,
      "loss": 1.5997,
      "step": 4820
    },
    {
      "epoch": 3.4648493543758967,
      "grad_norm": 4.356771945953369,
      "learning_rate": 2.1548274341147652e-06,
      "loss": 1.509,
      "step": 4830
    },
    {
      "epoch": 3.472022955523673,
      "grad_norm": 4.091918468475342,
      "learning_rate": 2.136324299597474e-06,
      "loss": 1.5643,
      "step": 4840
    },
    {
      "epoch": 3.479196556671449,
      "grad_norm": 5.2305097579956055,
      "learning_rate": 2.1178793428793747e-06,
      "loss": 1.5008,
      "step": 4850
    },
    {
      "epoch": 3.486370157819225,
      "grad_norm": 5.039559841156006,
      "learning_rate": 2.0994929386841057e-06,
      "loss": 1.528,
      "step": 4860
    },
    {
      "epoch": 3.4935437589670015,
      "grad_norm": 5.642120361328125,
      "learning_rate": 2.0811654605457723e-06,
      "loss": 1.6247,
      "step": 4870
    },
    {
      "epoch": 3.500717360114778,
      "grad_norm": 5.08212423324585,
      "learning_rate": 2.062897280801344e-06,
      "loss": 1.5799,
      "step": 4880
    },
    {
      "epoch": 3.5078909612625537,
      "grad_norm": 3.2159550189971924,
      "learning_rate": 2.044688770583103e-06,
      "loss": 1.5451,
      "step": 4890
    },
    {
      "epoch": 3.51506456241033,
      "grad_norm": 4.61575174331665,
      "learning_rate": 2.026540299811099e-06,
      "loss": 1.5885,
      "step": 4900
    },
    {
      "epoch": 3.5222381635581064,
      "grad_norm": 6.853714466094971,
      "learning_rate": 2.008452237185629e-06,
      "loss": 1.574,
      "step": 4910
    },
    {
      "epoch": 3.5294117647058822,
      "grad_norm": 4.772297382354736,
      "learning_rate": 1.990424950179753e-06,
      "loss": 1.5926,
      "step": 4920
    },
    {
      "epoch": 3.5365853658536586,
      "grad_norm": 4.5937299728393555,
      "learning_rate": 1.9724588050318287e-06,
      "loss": 1.559,
      "step": 4930
    },
    {
      "epoch": 3.543758967001435,
      "grad_norm": 5.633677959442139,
      "learning_rate": 1.9545541667380692e-06,
      "loss": 1.4889,
      "step": 4940
    },
    {
      "epoch": 3.5509325681492108,
      "grad_norm": 5.533020973205566,
      "learning_rate": 1.936711399045129e-06,
      "loss": 1.5627,
      "step": 4950
    },
    {
      "epoch": 3.558106169296987,
      "grad_norm": 5.060328006744385,
      "learning_rate": 1.9189308644427108e-06,
      "loss": 1.5719,
      "step": 4960
    },
    {
      "epoch": 3.5652797704447634,
      "grad_norm": 4.445728778839111,
      "learning_rate": 1.9012129241562015e-06,
      "loss": 1.6033,
      "step": 4970
    },
    {
      "epoch": 3.5724533715925393,
      "grad_norm": 4.857860088348389,
      "learning_rate": 1.8835579381393415e-06,
      "loss": 1.546,
      "step": 4980
    },
    {
      "epoch": 3.5796269727403156,
      "grad_norm": 5.62244987487793,
      "learning_rate": 1.8659662650669037e-06,
      "loss": 1.5444,
      "step": 4990
    },
    {
      "epoch": 3.586800573888092,
      "grad_norm": 6.1219563484191895,
      "learning_rate": 1.84843826232741e-06,
      "loss": 1.6225,
      "step": 5000
    },
    {
      "epoch": 3.593974175035868,
      "grad_norm": 4.305948734283447,
      "learning_rate": 1.830974286015868e-06,
      "loss": 1.5381,
      "step": 5010
    },
    {
      "epoch": 3.601147776183644,
      "grad_norm": 5.382129192352295,
      "learning_rate": 1.8135746909265378e-06,
      "loss": 1.4818,
      "step": 5020
    },
    {
      "epoch": 3.6083213773314204,
      "grad_norm": 4.511855125427246,
      "learning_rate": 1.796239830545729e-06,
      "loss": 1.5812,
      "step": 5030
    },
    {
      "epoch": 3.6154949784791963,
      "grad_norm": 5.188858985900879,
      "learning_rate": 1.778970057044615e-06,
      "loss": 1.493,
      "step": 5040
    },
    {
      "epoch": 3.6226685796269726,
      "grad_norm": 5.078686714172363,
      "learning_rate": 1.7617657212720724e-06,
      "loss": 1.5827,
      "step": 5050
    },
    {
      "epoch": 3.629842180774749,
      "grad_norm": 3.890846014022827,
      "learning_rate": 1.7446271727475679e-06,
      "loss": 1.5357,
      "step": 5060
    },
    {
      "epoch": 3.637015781922525,
      "grad_norm": 5.869763374328613,
      "learning_rate": 1.727554759654041e-06,
      "loss": 1.5551,
      "step": 5070
    },
    {
      "epoch": 3.644189383070301,
      "grad_norm": 5.052447319030762,
      "learning_rate": 1.7105488288308458e-06,
      "loss": 1.5341,
      "step": 5080
    },
    {
      "epoch": 3.6513629842180775,
      "grad_norm": 4.311800956726074,
      "learning_rate": 1.6936097257666917e-06,
      "loss": 1.4998,
      "step": 5090
    },
    {
      "epoch": 3.658536585365854,
      "grad_norm": 4.6900153160095215,
      "learning_rate": 1.6767377945926332e-06,
      "loss": 1.5996,
      "step": 5100
    },
    {
      "epoch": 3.6657101865136297,
      "grad_norm": 5.323581218719482,
      "learning_rate": 1.6599333780750765e-06,
      "loss": 1.6039,
      "step": 5110
    },
    {
      "epoch": 3.672883787661406,
      "grad_norm": 4.80101203918457,
      "learning_rate": 1.6431968176088125e-06,
      "loss": 1.5708,
      "step": 5120
    },
    {
      "epoch": 3.6800573888091823,
      "grad_norm": 4.646844387054443,
      "learning_rate": 1.6265284532100822e-06,
      "loss": 1.5943,
      "step": 5130
    },
    {
      "epoch": 3.6872309899569586,
      "grad_norm": 4.231383323669434,
      "learning_rate": 1.609928623509675e-06,
      "loss": 1.5112,
      "step": 5140
    },
    {
      "epoch": 3.6944045911047345,
      "grad_norm": 5.30320930480957,
      "learning_rate": 1.5933976657460427e-06,
      "loss": 1.5601,
      "step": 5150
    },
    {
      "epoch": 3.701578192252511,
      "grad_norm": 5.305302143096924,
      "learning_rate": 1.57693591575845e-06,
      "loss": 1.4539,
      "step": 5160
    },
    {
      "epoch": 3.708751793400287,
      "grad_norm": 5.135618209838867,
      "learning_rate": 1.5605437079801522e-06,
      "loss": 1.5616,
      "step": 5170
    },
    {
      "epoch": 3.715925394548063,
      "grad_norm": 6.054445743560791,
      "learning_rate": 1.5442213754315966e-06,
      "loss": 1.5196,
      "step": 5180
    },
    {
      "epoch": 3.7230989956958394,
      "grad_norm": 5.193083763122559,
      "learning_rate": 1.5279692497136667e-06,
      "loss": 1.5286,
      "step": 5190
    },
    {
      "epoch": 3.7302725968436157,
      "grad_norm": 5.020390033721924,
      "learning_rate": 1.5117876610009386e-06,
      "loss": 1.4729,
      "step": 5200
    },
    {
      "epoch": 3.7374461979913915,
      "grad_norm": 4.597160339355469,
      "learning_rate": 1.49567693803497e-06,
      "loss": 1.5908,
      "step": 5210
    },
    {
      "epoch": 3.744619799139168,
      "grad_norm": 4.895615577697754,
      "learning_rate": 1.4796374081176312e-06,
      "loss": 1.5825,
      "step": 5220
    },
    {
      "epoch": 3.751793400286944,
      "grad_norm": 4.443509101867676,
      "learning_rate": 1.4636693971044452e-06,
      "loss": 1.6151,
      "step": 5230
    },
    {
      "epoch": 3.75896700143472,
      "grad_norm": 4.813259124755859,
      "learning_rate": 1.4477732293979785e-06,
      "loss": 1.6279,
      "step": 5240
    },
    {
      "epoch": 3.7661406025824964,
      "grad_norm": 4.843585968017578,
      "learning_rate": 1.4319492279412388e-06,
      "loss": 1.5365,
      "step": 5250
    },
    {
      "epoch": 3.7733142037302727,
      "grad_norm": 4.828671932220459,
      "learning_rate": 1.4161977142111256e-06,
      "loss": 1.5997,
      "step": 5260
    },
    {
      "epoch": 3.7804878048780486,
      "grad_norm": 5.076778888702393,
      "learning_rate": 1.4005190082118903e-06,
      "loss": 1.4816,
      "step": 5270
    },
    {
      "epoch": 3.787661406025825,
      "grad_norm": 5.268861770629883,
      "learning_rate": 1.384913428468644e-06,
      "loss": 1.579,
      "step": 5280
    },
    {
      "epoch": 3.7948350071736012,
      "grad_norm": 4.943966388702393,
      "learning_rate": 1.369381292020871e-06,
      "loss": 1.6173,
      "step": 5290
    },
    {
      "epoch": 3.802008608321377,
      "grad_norm": 5.388318061828613,
      "learning_rate": 1.3539229144160064e-06,
      "loss": 1.5597,
      "step": 5300
    },
    {
      "epoch": 3.8091822094691534,
      "grad_norm": 5.0746307373046875,
      "learning_rate": 1.338538609703015e-06,
      "loss": 1.4397,
      "step": 5310
    },
    {
      "epoch": 3.8163558106169297,
      "grad_norm": 4.911769866943359,
      "learning_rate": 1.3232286904260144e-06,
      "loss": 1.542,
      "step": 5320
    },
    {
      "epoch": 3.8235294117647056,
      "grad_norm": 5.338705062866211,
      "learning_rate": 1.3079934676179213e-06,
      "loss": 1.4935,
      "step": 5330
    },
    {
      "epoch": 3.830703012912482,
      "grad_norm": 5.367548942565918,
      "learning_rate": 1.2928332507941343e-06,
      "loss": 1.5855,
      "step": 5340
    },
    {
      "epoch": 3.8378766140602583,
      "grad_norm": 4.736767292022705,
      "learning_rate": 1.2777483479462527e-06,
      "loss": 1.6157,
      "step": 5350
    },
    {
      "epoch": 3.8450502152080346,
      "grad_norm": 5.312751293182373,
      "learning_rate": 1.2627390655358107e-06,
      "loss": 1.433,
      "step": 5360
    },
    {
      "epoch": 3.8522238163558105,
      "grad_norm": 5.255646705627441,
      "learning_rate": 1.2478057084880524e-06,
      "loss": 1.6055,
      "step": 5370
    },
    {
      "epoch": 3.859397417503587,
      "grad_norm": 4.396463394165039,
      "learning_rate": 1.2329485801857443e-06,
      "loss": 1.5391,
      "step": 5380
    },
    {
      "epoch": 3.866571018651363,
      "grad_norm": 5.263279914855957,
      "learning_rate": 1.218167982463e-06,
      "loss": 1.588,
      "step": 5390
    },
    {
      "epoch": 3.8737446197991394,
      "grad_norm": 5.802994728088379,
      "learning_rate": 1.2034642155991616e-06,
      "loss": 1.6133,
      "step": 5400
    },
    {
      "epoch": 3.8809182209469153,
      "grad_norm": 4.286766529083252,
      "learning_rate": 1.1888375783126872e-06,
      "loss": 1.5979,
      "step": 5410
    },
    {
      "epoch": 3.8880918220946916,
      "grad_norm": 5.927639007568359,
      "learning_rate": 1.1742883677550898e-06,
      "loss": 1.5707,
      "step": 5420
    },
    {
      "epoch": 3.895265423242468,
      "grad_norm": 4.695727825164795,
      "learning_rate": 1.1598168795048992e-06,
      "loss": 1.5972,
      "step": 5430
    },
    {
      "epoch": 3.902439024390244,
      "grad_norm": 4.653537273406982,
      "learning_rate": 1.1454234075616571e-06,
      "loss": 1.5528,
      "step": 5440
    },
    {
      "epoch": 3.90961262553802,
      "grad_norm": 5.233863353729248,
      "learning_rate": 1.1311082443399356e-06,
      "loss": 1.6008,
      "step": 5450
    },
    {
      "epoch": 3.9167862266857965,
      "grad_norm": 5.6193623542785645,
      "learning_rate": 1.1168716806634134e-06,
      "loss": 1.4883,
      "step": 5460
    },
    {
      "epoch": 3.9239598278335723,
      "grad_norm": 4.463745594024658,
      "learning_rate": 1.1027140057589535e-06,
      "loss": 1.5948,
      "step": 5470
    },
    {
      "epoch": 3.9311334289813487,
      "grad_norm": 4.90359354019165,
      "learning_rate": 1.0886355072507355e-06,
      "loss": 1.5842,
      "step": 5480
    },
    {
      "epoch": 3.938307030129125,
      "grad_norm": 4.931387901306152,
      "learning_rate": 1.0746364711544045e-06,
      "loss": 1.5554,
      "step": 5490
    },
    {
      "epoch": 3.945480631276901,
      "grad_norm": 5.381626129150391,
      "learning_rate": 1.0607171818712664e-06,
      "loss": 1.622,
      "step": 5500
    },
    {
      "epoch": 3.952654232424677,
      "grad_norm": 4.98836088180542,
      "learning_rate": 1.0468779221825104e-06,
      "loss": 1.4978,
      "step": 5510
    },
    {
      "epoch": 3.9598278335724535,
      "grad_norm": 5.961474418640137,
      "learning_rate": 1.0331189732434627e-06,
      "loss": 1.5845,
      "step": 5520
    },
    {
      "epoch": 3.9670014347202294,
      "grad_norm": 5.234318733215332,
      "learning_rate": 1.019440614577869e-06,
      "loss": 1.5729,
      "step": 5530
    },
    {
      "epoch": 3.9741750358680057,
      "grad_norm": 5.937037944793701,
      "learning_rate": 1.0058431240722266e-06,
      "loss": 1.5523,
      "step": 5540
    },
    {
      "epoch": 3.981348637015782,
      "grad_norm": 5.235926151275635,
      "learning_rate": 9.923267779701323e-07,
      "loss": 1.5117,
      "step": 5550
    },
    {
      "epoch": 3.988522238163558,
      "grad_norm": 5.3702778816223145,
      "learning_rate": 9.78891850866669e-07,
      "loss": 1.5973,
      "step": 5560
    },
    {
      "epoch": 3.995695839311334,
      "grad_norm": 4.557044982910156,
      "learning_rate": 9.655386157028285e-07,
      "loss": 1.5537,
      "step": 5570
    },
    {
      "epoch": 4.0028694404591105,
      "grad_norm": 4.746025562286377,
      "learning_rate": 9.522673437599705e-07,
      "loss": 1.6159,
      "step": 5580
    },
    {
      "epoch": 4.010043041606886,
      "grad_norm": 5.432153224945068,
      "learning_rate": 9.390783046543072e-07,
      "loss": 1.5328,
      "step": 5590
    },
    {
      "epoch": 4.017216642754663,
      "grad_norm": 5.343461036682129,
      "learning_rate": 9.259717663314283e-07,
      "loss": 1.5352,
      "step": 5600
    },
    {
      "epoch": 4.024390243902439,
      "grad_norm": 4.174177646636963,
      "learning_rate": 9.129479950608494e-07,
      "loss": 1.5461,
      "step": 5610
    },
    {
      "epoch": 4.031563845050215,
      "grad_norm": 6.409130573272705,
      "learning_rate": 9.000072554306161e-07,
      "loss": 1.6151,
      "step": 5620
    },
    {
      "epoch": 4.038737446197992,
      "grad_norm": 5.173266410827637,
      "learning_rate": 8.871498103419185e-07,
      "loss": 1.5866,
      "step": 5630
    },
    {
      "epoch": 4.045911047345768,
      "grad_norm": 4.607354164123535,
      "learning_rate": 8.743759210037561e-07,
      "loss": 1.5731,
      "step": 5640
    },
    {
      "epoch": 4.053084648493543,
      "grad_norm": 5.430211067199707,
      "learning_rate": 8.616858469276246e-07,
      "loss": 1.5995,
      "step": 5650
    },
    {
      "epoch": 4.06025824964132,
      "grad_norm": 5.3946003913879395,
      "learning_rate": 8.490798459222477e-07,
      "loss": 1.6257,
      "step": 5660
    },
    {
      "epoch": 4.067431850789096,
      "grad_norm": 5.310644626617432,
      "learning_rate": 8.365581740883416e-07,
      "loss": 1.5279,
      "step": 5670
    },
    {
      "epoch": 4.074605451936872,
      "grad_norm": 4.790588855743408,
      "learning_rate": 8.241210858134107e-07,
      "loss": 1.601,
      "step": 5680
    },
    {
      "epoch": 4.081779053084649,
      "grad_norm": 4.948362827301025,
      "learning_rate": 8.117688337665735e-07,
      "loss": 1.5722,
      "step": 5690
    },
    {
      "epoch": 4.088952654232425,
      "grad_norm": 5.825928211212158,
      "learning_rate": 7.995016688934403e-07,
      "loss": 1.551,
      "step": 5700
    },
    {
      "epoch": 4.0961262553802005,
      "grad_norm": 6.415127754211426,
      "learning_rate": 7.873198404110066e-07,
      "loss": 1.6086,
      "step": 5710
    },
    {
      "epoch": 4.103299856527977,
      "grad_norm": 5.353518486022949,
      "learning_rate": 7.752235958025922e-07,
      "loss": 1.5184,
      "step": 5720
    },
    {
      "epoch": 4.110473457675753,
      "grad_norm": 5.465404987335205,
      "learning_rate": 7.63213180812814e-07,
      "loss": 1.5094,
      "step": 5730
    },
    {
      "epoch": 4.117647058823529,
      "grad_norm": 4.449034214019775,
      "learning_rate": 7.512888394425944e-07,
      "loss": 1.5218,
      "step": 5740
    },
    {
      "epoch": 4.124820659971306,
      "grad_norm": 3.6261439323425293,
      "learning_rate": 7.394508139442036e-07,
      "loss": 1.5479,
      "step": 5750
    },
    {
      "epoch": 4.131994261119082,
      "grad_norm": 5.498762607574463,
      "learning_rate": 7.276993448163377e-07,
      "loss": 1.5817,
      "step": 5760
    },
    {
      "epoch": 4.1391678622668575,
      "grad_norm": 4.524099826812744,
      "learning_rate": 7.160346707992316e-07,
      "loss": 1.5499,
      "step": 5770
    },
    {
      "epoch": 4.146341463414634,
      "grad_norm": 3.949864387512207,
      "learning_rate": 7.044570288698094e-07,
      "loss": 1.528,
      "step": 5780
    },
    {
      "epoch": 4.15351506456241,
      "grad_norm": 4.782485485076904,
      "learning_rate": 6.929666542368724e-07,
      "loss": 1.6078,
      "step": 5790
    },
    {
      "epoch": 4.160688665710187,
      "grad_norm": 4.779369354248047,
      "learning_rate": 6.81563780336319e-07,
      "loss": 1.5533,
      "step": 5800
    },
    {
      "epoch": 4.167862266857963,
      "grad_norm": 5.37333345413208,
      "learning_rate": 6.70248638826404e-07,
      "loss": 1.5552,
      "step": 5810
    },
    {
      "epoch": 4.175035868005739,
      "grad_norm": 5.332211017608643,
      "learning_rate": 6.590214595830224e-07,
      "loss": 1.5161,
      "step": 5820
    },
    {
      "epoch": 4.182209469153515,
      "grad_norm": 4.697267055511475,
      "learning_rate": 6.47882470695056e-07,
      "loss": 1.5238,
      "step": 5830
    },
    {
      "epoch": 4.189383070301291,
      "grad_norm": 5.4522175788879395,
      "learning_rate": 6.368318984597277e-07,
      "loss": 1.5852,
      "step": 5840
    },
    {
      "epoch": 4.196556671449067,
      "grad_norm": 4.7713847160339355,
      "learning_rate": 6.258699673780083e-07,
      "loss": 1.511,
      "step": 5850
    },
    {
      "epoch": 4.203730272596844,
      "grad_norm": 4.892825603485107,
      "learning_rate": 6.149969001500522e-07,
      "loss": 1.488,
      "step": 5860
    },
    {
      "epoch": 4.21090387374462,
      "grad_norm": 5.340258598327637,
      "learning_rate": 6.042129176706785e-07,
      "loss": 1.6398,
      "step": 5870
    },
    {
      "epoch": 4.218077474892396,
      "grad_norm": 4.875546455383301,
      "learning_rate": 5.935182390248773e-07,
      "loss": 1.5438,
      "step": 5880
    },
    {
      "epoch": 4.2252510760401725,
      "grad_norm": 5.290652275085449,
      "learning_rate": 5.829130814833656e-07,
      "loss": 1.5892,
      "step": 5890
    },
    {
      "epoch": 4.232424677187948,
      "grad_norm": 5.553080081939697,
      "learning_rate": 5.723976604981652e-07,
      "loss": 1.601,
      "step": 5900
    },
    {
      "epoch": 4.239598278335724,
      "grad_norm": 5.031517028808594,
      "learning_rate": 5.619721896982328e-07,
      "loss": 1.5763,
      "step": 5910
    },
    {
      "epoch": 4.246771879483501,
      "grad_norm": 6.090808391571045,
      "learning_rate": 5.516368808851186e-07,
      "loss": 1.5763,
      "step": 5920
    },
    {
      "epoch": 4.253945480631277,
      "grad_norm": 5.399481773376465,
      "learning_rate": 5.41391944028658e-07,
      "loss": 1.5347,
      "step": 5930
    },
    {
      "epoch": 4.261119081779053,
      "grad_norm": 5.439573764801025,
      "learning_rate": 5.312375872627107e-07,
      "loss": 1.5747,
      "step": 5940
    },
    {
      "epoch": 4.2682926829268295,
      "grad_norm": 4.504551410675049,
      "learning_rate": 5.211740168809337e-07,
      "loss": 1.5384,
      "step": 5950
    },
    {
      "epoch": 4.275466284074605,
      "grad_norm": 4.708096027374268,
      "learning_rate": 5.112014373325863e-07,
      "loss": 1.5173,
      "step": 5960
    },
    {
      "epoch": 4.282639885222381,
      "grad_norm": 4.908802509307861,
      "learning_rate": 5.013200512183797e-07,
      "loss": 1.563,
      "step": 5970
    },
    {
      "epoch": 4.289813486370158,
      "grad_norm": 4.66370964050293,
      "learning_rate": 4.915300592863576e-07,
      "loss": 1.5509,
      "step": 5980
    },
    {
      "epoch": 4.296987087517934,
      "grad_norm": 4.341823101043701,
      "learning_rate": 4.818316604278195e-07,
      "loss": 1.5724,
      "step": 5990
    },
    {
      "epoch": 4.30416068866571,
      "grad_norm": 4.796359062194824,
      "learning_rate": 4.722250516732824e-07,
      "loss": 1.6052,
      "step": 6000
    },
    {
      "epoch": 4.3113342898134865,
      "grad_norm": 6.70570707321167,
      "learning_rate": 4.6271042818847643e-07,
      "loss": 1.6172,
      "step": 6010
    },
    {
      "epoch": 4.318507890961262,
      "grad_norm": 5.226576805114746,
      "learning_rate": 4.532879832703757e-07,
      "loss": 1.6204,
      "step": 6020
    },
    {
      "epoch": 4.325681492109039,
      "grad_norm": 5.05267333984375,
      "learning_rate": 4.439579083432799e-07,
      "loss": 1.6071,
      "step": 6030
    },
    {
      "epoch": 4.332855093256815,
      "grad_norm": 4.941309928894043,
      "learning_rate": 4.347203929549149e-07,
      "loss": 1.5591,
      "step": 6040
    },
    {
      "epoch": 4.340028694404591,
      "grad_norm": 4.543044090270996,
      "learning_rate": 4.2557562477259393e-07,
      "loss": 1.5605,
      "step": 6050
    },
    {
      "epoch": 4.347202295552368,
      "grad_norm": 7.761125564575195,
      "learning_rate": 4.165237895793928e-07,
      "loss": 1.526,
      "step": 6060
    },
    {
      "epoch": 4.354375896700144,
      "grad_norm": 5.777158737182617,
      "learning_rate": 4.0756507127038494e-07,
      "loss": 1.5849,
      "step": 6070
    },
    {
      "epoch": 4.3615494978479195,
      "grad_norm": 4.4891357421875,
      "learning_rate": 3.986996518489011e-07,
      "loss": 1.5555,
      "step": 6080
    },
    {
      "epoch": 4.368723098995696,
      "grad_norm": 4.413408279418945,
      "learning_rate": 3.8992771142283224e-07,
      "loss": 1.5449,
      "step": 6090
    },
    {
      "epoch": 4.375896700143472,
      "grad_norm": 4.123354911804199,
      "learning_rate": 3.812494282009699e-07,
      "loss": 1.4759,
      "step": 6100
    },
    {
      "epoch": 4.383070301291248,
      "grad_norm": 5.59555721282959,
      "learning_rate": 3.7266497848938764e-07,
      "loss": 1.5669,
      "step": 6110
    },
    {
      "epoch": 4.390243902439025,
      "grad_norm": 4.094523906707764,
      "learning_rate": 3.6417453668785905e-07,
      "loss": 1.5955,
      "step": 6120
    },
    {
      "epoch": 4.397417503586801,
      "grad_norm": 4.499478816986084,
      "learning_rate": 3.557782752863137e-07,
      "loss": 1.5491,
      "step": 6130
    },
    {
      "epoch": 4.4045911047345765,
      "grad_norm": 4.974415302276611,
      "learning_rate": 3.4747636486133105e-07,
      "loss": 1.5264,
      "step": 6140
    },
    {
      "epoch": 4.411764705882353,
      "grad_norm": 5.325627326965332,
      "learning_rate": 3.39268974072679e-07,
      "loss": 1.5621,
      "step": 6150
    },
    {
      "epoch": 4.418938307030129,
      "grad_norm": 4.974119663238525,
      "learning_rate": 3.311562696598847e-07,
      "loss": 1.5421,
      "step": 6160
    },
    {
      "epoch": 4.426111908177905,
      "grad_norm": 4.543067932128906,
      "learning_rate": 3.2313841643884956e-07,
      "loss": 1.6673,
      "step": 6170
    },
    {
      "epoch": 4.433285509325682,
      "grad_norm": 5.34157657623291,
      "learning_rate": 3.152155772984966e-07,
      "loss": 1.5316,
      "step": 6180
    },
    {
      "epoch": 4.440459110473458,
      "grad_norm": 5.4170637130737305,
      "learning_rate": 3.073879131974661e-07,
      "loss": 1.5461,
      "step": 6190
    },
    {
      "epoch": 4.4476327116212335,
      "grad_norm": 5.147431373596191,
      "learning_rate": 2.9965558316084165e-07,
      "loss": 1.6311,
      "step": 6200
    },
    {
      "epoch": 4.45480631276901,
      "grad_norm": 4.811800956726074,
      "learning_rate": 2.9201874427692343e-07,
      "loss": 1.5159,
      "step": 6210
    },
    {
      "epoch": 4.461979913916786,
      "grad_norm": 5.096737384796143,
      "learning_rate": 2.8447755169403177e-07,
      "loss": 1.4974,
      "step": 6220
    },
    {
      "epoch": 4.469153515064562,
      "grad_norm": 5.664224147796631,
      "learning_rate": 2.770321586173597e-07,
      "loss": 1.4725,
      "step": 6230
    },
    {
      "epoch": 4.476327116212339,
      "grad_norm": 5.362559795379639,
      "learning_rate": 2.696827163058602e-07,
      "loss": 1.6102,
      "step": 6240
    },
    {
      "epoch": 4.483500717360115,
      "grad_norm": 5.520565986633301,
      "learning_rate": 2.6242937406916856e-07,
      "loss": 1.531,
      "step": 6250
    },
    {
      "epoch": 4.490674318507891,
      "grad_norm": 4.630412578582764,
      "learning_rate": 2.5527227926457343e-07,
      "loss": 1.5023,
      "step": 6260
    },
    {
      "epoch": 4.497847919655667,
      "grad_norm": 4.701235294342041,
      "learning_rate": 2.48211577294023e-07,
      "loss": 1.5922,
      "step": 6270
    },
    {
      "epoch": 4.505021520803443,
      "grad_norm": 5.180627822875977,
      "learning_rate": 2.4124741160116983e-07,
      "loss": 1.5584,
      "step": 6280
    },
    {
      "epoch": 4.512195121951219,
      "grad_norm": 5.71336555480957,
      "learning_rate": 2.343799236684574e-07,
      "loss": 1.5454,
      "step": 6290
    },
    {
      "epoch": 4.519368723098996,
      "grad_norm": 6.5490546226501465,
      "learning_rate": 2.2760925301424376e-07,
      "loss": 1.5235,
      "step": 6300
    },
    {
      "epoch": 4.526542324246772,
      "grad_norm": 4.815338611602783,
      "learning_rate": 2.2093553718996853e-07,
      "loss": 1.544,
      "step": 6310
    },
    {
      "epoch": 4.533715925394548,
      "grad_norm": 5.39247465133667,
      "learning_rate": 2.1435891177736035e-07,
      "loss": 1.6074,
      "step": 6320
    },
    {
      "epoch": 4.540889526542324,
      "grad_norm": 3.7526986598968506,
      "learning_rate": 2.078795103856801e-07,
      "loss": 1.5999,
      "step": 6330
    },
    {
      "epoch": 4.5480631276901,
      "grad_norm": 5.715826988220215,
      "learning_rate": 2.014974646490059e-07,
      "loss": 1.4557,
      "step": 6340
    },
    {
      "epoch": 4.555236728837876,
      "grad_norm": 4.657169342041016,
      "learning_rate": 1.952129042235612e-07,
      "loss": 1.6431,
      "step": 6350
    },
    {
      "epoch": 4.562410329985653,
      "grad_norm": 5.31453800201416,
      "learning_rate": 1.890259567850783e-07,
      "loss": 1.5483,
      "step": 6360
    },
    {
      "epoch": 4.569583931133429,
      "grad_norm": 5.223659038543701,
      "learning_rate": 1.8293674802620787e-07,
      "loss": 1.5009,
      "step": 6370
    },
    {
      "epoch": 4.5767575322812055,
      "grad_norm": 4.785393238067627,
      "learning_rate": 1.7694540165396068e-07,
      "loss": 1.4854,
      "step": 6380
    },
    {
      "epoch": 4.583931133428981,
      "grad_norm": 4.617883205413818,
      "learning_rate": 1.7105203938719895e-07,
      "loss": 1.5503,
      "step": 6390
    },
    {
      "epoch": 4.591104734576757,
      "grad_norm": 5.081192493438721,
      "learning_rate": 1.6525678095416166e-07,
      "loss": 1.6313,
      "step": 6400
    },
    {
      "epoch": 4.598278335724534,
      "grad_norm": 4.655367374420166,
      "learning_rate": 1.5955974409002984e-07,
      "loss": 1.5813,
      "step": 6410
    },
    {
      "epoch": 4.60545193687231,
      "grad_norm": 4.787787437438965,
      "learning_rate": 1.5396104453453908e-07,
      "loss": 1.5194,
      "step": 6420
    },
    {
      "epoch": 4.612625538020086,
      "grad_norm": 6.389397621154785,
      "learning_rate": 1.4846079602962582e-07,
      "loss": 1.5399,
      "step": 6430
    },
    {
      "epoch": 4.619799139167863,
      "grad_norm": 5.136475563049316,
      "learning_rate": 1.4305911031711583e-07,
      "loss": 1.5758,
      "step": 6440
    },
    {
      "epoch": 4.626972740315638,
      "grad_norm": 5.421079158782959,
      "learning_rate": 1.3775609713645665e-07,
      "loss": 1.5501,
      "step": 6450
    },
    {
      "epoch": 4.634146341463414,
      "grad_norm": 4.907922744750977,
      "learning_rate": 1.3255186422248545e-07,
      "loss": 1.625,
      "step": 6460
    },
    {
      "epoch": 4.641319942611191,
      "grad_norm": 4.085840702056885,
      "learning_rate": 1.2744651730324077e-07,
      "loss": 1.5282,
      "step": 6470
    },
    {
      "epoch": 4.648493543758967,
      "grad_norm": 5.269533157348633,
      "learning_rate": 1.22440160097817e-07,
      "loss": 1.6036,
      "step": 6480
    },
    {
      "epoch": 4.655667144906743,
      "grad_norm": 4.301773548126221,
      "learning_rate": 1.1753289431425451e-07,
      "loss": 1.5719,
      "step": 6490
    },
    {
      "epoch": 4.66284074605452,
      "grad_norm": 5.401717662811279,
      "learning_rate": 1.1272481964747395e-07,
      "loss": 1.5735,
      "step": 6500
    },
    {
      "epoch": 4.6700143472022955,
      "grad_norm": 5.364983558654785,
      "learning_rate": 1.080160337772529e-07,
      "loss": 1.5749,
      "step": 6510
    },
    {
      "epoch": 4.677187948350072,
      "grad_norm": 5.163378715515137,
      "learning_rate": 1.034066323662375e-07,
      "loss": 1.543,
      "step": 6520
    },
    {
      "epoch": 4.684361549497848,
      "grad_norm": 5.264856338500977,
      "learning_rate": 9.889670905800397e-08,
      "loss": 1.594,
      "step": 6530
    },
    {
      "epoch": 4.691535150645624,
      "grad_norm": 6.027789115905762,
      "learning_rate": 9.448635547515128e-08,
      "loss": 1.5493,
      "step": 6540
    },
    {
      "epoch": 4.698708751793401,
      "grad_norm": 4.557546138763428,
      "learning_rate": 9.017566121744204e-08,
      "loss": 1.5684,
      "step": 6550
    },
    {
      "epoch": 4.705882352941177,
      "grad_norm": 4.302761554718018,
      "learning_rate": 8.596471385998451e-08,
      "loss": 1.5826,
      "step": 6560
    },
    {
      "epoch": 4.7130559540889525,
      "grad_norm": 4.797915458679199,
      "learning_rate": 8.185359895144851e-08,
      "loss": 1.5148,
      "step": 6570
    },
    {
      "epoch": 4.720229555236729,
      "grad_norm": 4.826371669769287,
      "learning_rate": 7.784240001233124e-08,
      "loss": 1.5527,
      "step": 6580
    },
    {
      "epoch": 4.727403156384505,
      "grad_norm": 4.862820148468018,
      "learning_rate": 7.393119853325914e-08,
      "loss": 1.566,
      "step": 6590
    },
    {
      "epoch": 4.734576757532281,
      "grad_norm": 5.459691524505615,
      "learning_rate": 7.012007397333154e-08,
      "loss": 1.4991,
      "step": 6600
    },
    {
      "epoch": 4.741750358680058,
      "grad_norm": 5.859012126922607,
      "learning_rate": 6.640910375850907e-08,
      "loss": 1.5146,
      "step": 6610
    },
    {
      "epoch": 4.748923959827834,
      "grad_norm": 4.840340614318848,
      "learning_rate": 6.27983632800383e-08,
      "loss": 1.4839,
      "step": 6620
    },
    {
      "epoch": 4.7560975609756095,
      "grad_norm": 5.5150885581970215,
      "learning_rate": 5.92879258929191e-08,
      "loss": 1.5551,
      "step": 6630
    },
    {
      "epoch": 4.763271162123386,
      "grad_norm": 4.157032012939453,
      "learning_rate": 5.587786291441799e-08,
      "loss": 1.5973,
      "step": 6640
    },
    {
      "epoch": 4.770444763271162,
      "grad_norm": 5.224157810211182,
      "learning_rate": 5.2568243622617146e-08,
      "loss": 1.5988,
      "step": 6650
    },
    {
      "epoch": 4.777618364418938,
      "grad_norm": 4.945899963378906,
      "learning_rate": 4.935913525500602e-08,
      "loss": 1.5578,
      "step": 6660
    },
    {
      "epoch": 4.784791965566715,
      "grad_norm": 5.344091415405273,
      "learning_rate": 4.625060300711803e-08,
      "loss": 1.5344,
      "step": 6670
    },
    {
      "epoch": 4.791965566714491,
      "grad_norm": 5.366001605987549,
      "learning_rate": 4.324271003120217e-08,
      "loss": 1.5485,
      "step": 6680
    },
    {
      "epoch": 4.799139167862267,
      "grad_norm": 4.758662700653076,
      "learning_rate": 4.0335517434945125e-08,
      "loss": 1.5994,
      "step": 6690
    },
    {
      "epoch": 4.806312769010043,
      "grad_norm": 4.486518383026123,
      "learning_rate": 3.752908428022506e-08,
      "loss": 1.5565,
      "step": 6700
    },
    {
      "epoch": 4.813486370157819,
      "grad_norm": 4.721200466156006,
      "learning_rate": 3.4823467581913686e-08,
      "loss": 1.5343,
      "step": 6710
    },
    {
      "epoch": 4.820659971305595,
      "grad_norm": 5.135944843292236,
      "learning_rate": 3.2218722306719964e-08,
      "loss": 1.5413,
      "step": 6720
    },
    {
      "epoch": 4.827833572453372,
      "grad_norm": 6.162848949432373,
      "learning_rate": 2.9714901372069337e-08,
      "loss": 1.5165,
      "step": 6730
    },
    {
      "epoch": 4.835007173601148,
      "grad_norm": 5.137905120849609,
      "learning_rate": 2.7312055645031254e-08,
      "loss": 1.5421,
      "step": 6740
    },
    {
      "epoch": 4.842180774748924,
      "grad_norm": 5.922069072723389,
      "learning_rate": 2.50102339412861e-08,
      "loss": 1.5462,
      "step": 6750
    },
    {
      "epoch": 4.8493543758967,
      "grad_norm": 6.346323490142822,
      "learning_rate": 2.2809483024132105e-08,
      "loss": 1.5267,
      "step": 6760
    },
    {
      "epoch": 4.856527977044476,
      "grad_norm": 5.942778587341309,
      "learning_rate": 2.0709847603536114e-08,
      "loss": 1.5642,
      "step": 6770
    },
    {
      "epoch": 4.863701578192252,
      "grad_norm": 5.7872748374938965,
      "learning_rate": 1.8711370335223745e-08,
      "loss": 1.4988,
      "step": 6780
    },
    {
      "epoch": 4.870875179340029,
      "grad_norm": 4.585635662078857,
      "learning_rate": 1.6814091819815638e-08,
      "loss": 1.5555,
      "step": 6790
    },
    {
      "epoch": 4.878048780487805,
      "grad_norm": 3.7418084144592285,
      "learning_rate": 1.50180506020009e-08,
      "loss": 1.5872,
      "step": 6800
    },
    {
      "epoch": 4.885222381635581,
      "grad_norm": 4.022066593170166,
      "learning_rate": 1.3323283169753842e-08,
      "loss": 1.5062,
      "step": 6810
    },
    {
      "epoch": 4.892395982783357,
      "grad_norm": 5.654516696929932,
      "learning_rate": 1.1729823953592345e-08,
      "loss": 1.5884,
      "step": 6820
    },
    {
      "epoch": 4.899569583931133,
      "grad_norm": 3.199690580368042,
      "learning_rate": 1.0237705325880087e-08,
      "loss": 1.4935,
      "step": 6830
    },
    {
      "epoch": 4.906743185078909,
      "grad_norm": 5.2016425132751465,
      "learning_rate": 8.846957600167628e-09,
      "loss": 1.5674,
      "step": 6840
    },
    {
      "epoch": 4.913916786226686,
      "grad_norm": 4.3060994148254395,
      "learning_rate": 7.557609030576784e-09,
      "loss": 1.5078,
      "step": 6850
    },
    {
      "epoch": 4.921090387374462,
      "grad_norm": 5.022528648376465,
      "learning_rate": 6.369685811226101e-09,
      "loss": 1.509,
      "step": 6860
    },
    {
      "epoch": 4.928263988522238,
      "grad_norm": 4.524997711181641,
      "learning_rate": 5.283212075700706e-09,
      "loss": 1.5751,
      "step": 6870
    },
    {
      "epoch": 4.9354375896700144,
      "grad_norm": 4.877177715301514,
      "learning_rate": 4.298209896558825e-09,
      "loss": 1.6102,
      "step": 6880
    },
    {
      "epoch": 4.94261119081779,
      "grad_norm": 5.287437438964844,
      "learning_rate": 3.41469928488547e-09,
      "loss": 1.4988,
      "step": 6890
    },
    {
      "epoch": 4.949784791965567,
      "grad_norm": 5.605737686157227,
      "learning_rate": 2.6326981898872063e-09,
      "loss": 1.6041,
      "step": 6900
    },
    {
      "epoch": 4.956958393113343,
      "grad_norm": 4.921607971191406,
      "learning_rate": 1.952222498523004e-09,
      "loss": 1.5303,
      "step": 6910
    },
    {
      "epoch": 4.964131994261119,
      "grad_norm": 4.859786033630371,
      "learning_rate": 1.3732860351872711e-09,
      "loss": 1.5685,
      "step": 6920
    },
    {
      "epoch": 4.971305595408896,
      "grad_norm": 4.652737140655518,
      "learning_rate": 8.959005614234129e-10,
      "loss": 1.5477,
      "step": 6930
    },
    {
      "epoch": 4.9784791965566715,
      "grad_norm": 5.877262592315674,
      "learning_rate": 5.200757756890218e-10,
      "loss": 1.6465,
      "step": 6940
    },
    {
      "epoch": 4.985652797704447,
      "grad_norm": 3.621426582336426,
      "learning_rate": 2.4581931315714735e-10,
      "loss": 1.5201,
      "step": 6950
    },
    {
      "epoch": 4.992826398852224,
      "grad_norm": 5.250094890594482,
      "learning_rate": 7.313674556086447e-11,
      "loss": 1.5128,
      "step": 6960
    },
    {
      "epoch": 5.0,
      "grad_norm": 6.219424247741699,
      "learning_rate": 2.031581080585987e-12,
      "loss": 1.5607,
      "step": 6970
    },
    {
      "epoch": 5.0,
      "step": 6970,
      "total_flos": 2.3256068955144192e+17,
      "train_loss": 1.6372499714964945,
      "train_runtime": 1867.6929,
      "train_samples_per_second": 14.925,
      "train_steps_per_second": 3.732
    }
  ],
  "logging_steps": 10,
  "max_steps": 6970,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500.0,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": false,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.3256068955144192e+17,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
