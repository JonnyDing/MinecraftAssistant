{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 1782,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.016835016835016835,
      "grad_norm": 1.9361172914505005,
      "learning_rate": 4.9996853191846885e-05,
      "loss": 3.4224,
      "step": 10
    },
    {
      "epoch": 0.03367003367003367,
      "grad_norm": 1.993322491645813,
      "learning_rate": 4.9987413559579636e-05,
      "loss": 3.1125,
      "step": 20
    },
    {
      "epoch": 0.050505050505050504,
      "grad_norm": 2.233336925506592,
      "learning_rate": 4.9969547550398755e-05,
      "loss": 2.8342,
      "step": 30
    },
    {
      "epoch": 0.06734006734006734,
      "grad_norm": 3.761918306350708,
      "learning_rate": 4.994392114722845e-05,
      "loss": 2.5459,
      "step": 40
    },
    {
      "epoch": 0.08417508417508418,
      "grad_norm": 2.6633126735687256,
      "learning_rate": 4.9910542314609684e-05,
      "loss": 2.5473,
      "step": 50
    },
    {
      "epoch": 0.10101010101010101,
      "grad_norm": 3.1748156547546387,
      "learning_rate": 4.986942142649465e-05,
      "loss": 2.2899,
      "step": 60
    },
    {
      "epoch": 0.11784511784511785,
      "grad_norm": 3.200826644897461,
      "learning_rate": 4.982057126302254e-05,
      "loss": 2.3689,
      "step": 70
    },
    {
      "epoch": 0.13468013468013468,
      "grad_norm": 3.9545986652374268,
      "learning_rate": 4.9764007006547516e-05,
      "loss": 2.2489,
      "step": 80
    },
    {
      "epoch": 0.15151515151515152,
      "grad_norm": 3.9337589740753174,
      "learning_rate": 4.969974623692023e-05,
      "loss": 2.2514,
      "step": 90
    },
    {
      "epoch": 0.16835016835016836,
      "grad_norm": 2.0633909702301025,
      "learning_rate": 4.962780892602398e-05,
      "loss": 2.3551,
      "step": 100
    },
    {
      "epoch": 0.18518518518518517,
      "grad_norm": 2.8814339637756348,
      "learning_rate": 4.9548217431567665e-05,
      "loss": 2.0978,
      "step": 110
    },
    {
      "epoch": 0.20202020202020202,
      "grad_norm": 5.306746959686279,
      "learning_rate": 4.946099649013708e-05,
      "loss": 2.22,
      "step": 120
    },
    {
      "epoch": 0.21885521885521886,
      "grad_norm": 2.772097110748291,
      "learning_rate": 4.936617320950693e-05,
      "loss": 2.1648,
      "step": 130
    },
    {
      "epoch": 0.2356902356902357,
      "grad_norm": 4.200642108917236,
      "learning_rate": 4.92637770602159e-05,
      "loss": 2.1084,
      "step": 140
    },
    {
      "epoch": 0.25252525252525254,
      "grad_norm": 3.3180630207061768,
      "learning_rate": 4.915383986640738e-05,
      "loss": 2.1075,
      "step": 150
    },
    {
      "epoch": 0.26936026936026936,
      "grad_norm": 3.6734814643859863,
      "learning_rate": 4.90363957959387e-05,
      "loss": 2.2369,
      "step": 160
    },
    {
      "epoch": 0.28619528619528617,
      "grad_norm": 3.107510805130005,
      "learning_rate": 4.89114813497619e-05,
      "loss": 2.0231,
      "step": 170
    },
    {
      "epoch": 0.30303030303030304,
      "grad_norm": 3.5727992057800293,
      "learning_rate": 4.8779135350579575e-05,
      "loss": 2.1481,
      "step": 180
    },
    {
      "epoch": 0.31986531986531985,
      "grad_norm": 4.047451972961426,
      "learning_rate": 4.863939893077883e-05,
      "loss": 2.1437,
      "step": 190
    },
    {
      "epoch": 0.3367003367003367,
      "grad_norm": 3.7398104667663574,
      "learning_rate": 4.849231551964771e-05,
      "loss": 2.1299,
      "step": 200
    },
    {
      "epoch": 0.35353535353535354,
      "grad_norm": 4.882070541381836,
      "learning_rate": 4.8337930829877534e-05,
      "loss": 2.1748,
      "step": 210
    },
    {
      "epoch": 0.37037037037037035,
      "grad_norm": 4.168272972106934,
      "learning_rate": 4.817629284335567e-05,
      "loss": 2.1443,
      "step": 220
    },
    {
      "epoch": 0.3872053872053872,
      "grad_norm": 4.170114517211914,
      "learning_rate": 4.8007451796253075e-05,
      "loss": 2.0676,
      "step": 230
    },
    {
      "epoch": 0.40404040404040403,
      "grad_norm": 4.207018852233887,
      "learning_rate": 4.7831460163411136e-05,
      "loss": 1.9006,
      "step": 240
    },
    {
      "epoch": 0.4208754208754209,
      "grad_norm": 2.880298137664795,
      "learning_rate": 4.764837264203278e-05,
      "loss": 2.2217,
      "step": 250
    },
    {
      "epoch": 0.4377104377104377,
      "grad_norm": 3.574801206588745,
      "learning_rate": 4.7458246134682926e-05,
      "loss": 2.1579,
      "step": 260
    },
    {
      "epoch": 0.45454545454545453,
      "grad_norm": 3.816235065460205,
      "learning_rate": 4.726113973160348e-05,
      "loss": 2.0936,
      "step": 270
    },
    {
      "epoch": 0.4713804713804714,
      "grad_norm": 3.2483744621276855,
      "learning_rate": 4.7057114692348406e-05,
      "loss": 2.1808,
      "step": 280
    },
    {
      "epoch": 0.4882154882154882,
      "grad_norm": 4.281609058380127,
      "learning_rate": 4.684623442674463e-05,
      "loss": 2.0325,
      "step": 290
    },
    {
      "epoch": 0.5050505050505051,
      "grad_norm": 4.061521530151367,
      "learning_rate": 4.662856447518458e-05,
      "loss": 1.9011,
      "step": 300
    },
    {
      "epoch": 0.5218855218855218,
      "grad_norm": 5.447611331939697,
      "learning_rate": 4.640417248825667e-05,
      "loss": 1.9803,
      "step": 310
    },
    {
      "epoch": 0.5387205387205387,
      "grad_norm": 4.667344093322754,
      "learning_rate": 4.61731282057198e-05,
      "loss": 2.1228,
      "step": 320
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 4.146225929260254,
      "learning_rate": 4.5935503434828743e-05,
      "loss": 2.1799,
      "step": 330
    },
    {
      "epoch": 0.5723905723905723,
      "grad_norm": 4.317255973815918,
      "learning_rate": 4.569137202801671e-05,
      "loss": 2.1215,
      "step": 340
    },
    {
      "epoch": 0.5892255892255892,
      "grad_norm": 5.996849060058594,
      "learning_rate": 4.544080985994258e-05,
      "loss": 2.0402,
      "step": 350
    },
    {
      "epoch": 0.6060606060606061,
      "grad_norm": 3.815424680709839,
      "learning_rate": 4.5183894803909365e-05,
      "loss": 1.8804,
      "step": 360
    },
    {
      "epoch": 0.622895622895623,
      "grad_norm": 4.076857566833496,
      "learning_rate": 4.492070670766173e-05,
      "loss": 2.0163,
      "step": 370
    },
    {
      "epoch": 0.6397306397306397,
      "grad_norm": 6.609031677246094,
      "learning_rate": 4.465132736856969e-05,
      "loss": 2.1748,
      "step": 380
    },
    {
      "epoch": 0.6565656565656566,
      "grad_norm": 4.432318210601807,
      "learning_rate": 4.437584050820647e-05,
      "loss": 2.1219,
      "step": 390
    },
    {
      "epoch": 0.6734006734006734,
      "grad_norm": 5.35256814956665,
      "learning_rate": 4.4094331746328354e-05,
      "loss": 1.9736,
      "step": 400
    },
    {
      "epoch": 0.6902356902356902,
      "grad_norm": 5.2005791664123535,
      "learning_rate": 4.3806888574264495e-05,
      "loss": 1.9909,
      "step": 410
    },
    {
      "epoch": 0.7070707070707071,
      "grad_norm": 4.184239864349365,
      "learning_rate": 4.351360032772512e-05,
      "loss": 2.0955,
      "step": 420
    },
    {
      "epoch": 0.7239057239057239,
      "grad_norm": 4.037359714508057,
      "learning_rate": 4.321455815903653e-05,
      "loss": 2.0205,
      "step": 430
    },
    {
      "epoch": 0.7407407407407407,
      "grad_norm": 6.231339931488037,
      "learning_rate": 4.290985500881143e-05,
      "loss": 2.0691,
      "step": 440
    },
    {
      "epoch": 0.7575757575757576,
      "grad_norm": 5.952918529510498,
      "learning_rate": 4.259958557706355e-05,
      "loss": 1.7926,
      "step": 450
    },
    {
      "epoch": 0.7744107744107744,
      "grad_norm": 5.576068878173828,
      "learning_rate": 4.228384629377539e-05,
      "loss": 1.9136,
      "step": 460
    },
    {
      "epoch": 0.7912457912457912,
      "grad_norm": 3.046609878540039,
      "learning_rate": 4.1962735288928305e-05,
      "loss": 2.0711,
      "step": 470
    },
    {
      "epoch": 0.8080808080808081,
      "grad_norm": 3.6076724529266357,
      "learning_rate": 4.163635236200435e-05,
      "loss": 1.9722,
      "step": 480
    },
    {
      "epoch": 0.8249158249158249,
      "grad_norm": 5.440877437591553,
      "learning_rate": 4.130479895096907e-05,
      "loss": 1.9659,
      "step": 490
    },
    {
      "epoch": 0.8417508417508418,
      "grad_norm": 7.086289882659912,
      "learning_rate": 4.09681781007452e-05,
      "loss": 2.2745,
      "step": 500
    },
    {
      "epoch": 0.8585858585858586,
      "grad_norm": 5.448435306549072,
      "learning_rate": 4.0626594431186904e-05,
      "loss": 2.0968,
      "step": 510
    },
    {
      "epoch": 0.8754208754208754,
      "grad_norm": 6.97867488861084,
      "learning_rate": 4.028015410456445e-05,
      "loss": 1.8963,
      "step": 520
    },
    {
      "epoch": 0.8922558922558923,
      "grad_norm": 4.68292760848999,
      "learning_rate": 3.9928964792569655e-05,
      "loss": 1.7577,
      "step": 530
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 4.739386558532715,
      "learning_rate": 3.95731356428521e-05,
      "loss": 2.0566,
      "step": 540
    },
    {
      "epoch": 0.9259259259259259,
      "grad_norm": 4.243803977966309,
      "learning_rate": 3.921277724509663e-05,
      "loss": 2.0318,
      "step": 550
    },
    {
      "epoch": 0.9427609427609428,
      "grad_norm": 6.421629428863525,
      "learning_rate": 3.884800159665276e-05,
      "loss": 2.1027,
      "step": 560
    },
    {
      "epoch": 0.9595959595959596,
      "grad_norm": 6.0716047286987305,
      "learning_rate": 3.84789220677265e-05,
      "loss": 2.1734,
      "step": 570
    },
    {
      "epoch": 0.9764309764309764,
      "grad_norm": 5.678934097290039,
      "learning_rate": 3.8105653366145524e-05,
      "loss": 1.9719,
      "step": 580
    },
    {
      "epoch": 0.9932659932659933,
      "grad_norm": 7.308315753936768,
      "learning_rate": 3.772831150170868e-05,
      "loss": 2.0994,
      "step": 590
    },
    {
      "epoch": 1.0101010101010102,
      "grad_norm": 5.857185363769531,
      "learning_rate": 3.7347013750130734e-05,
      "loss": 1.9649,
      "step": 600
    },
    {
      "epoch": 1.026936026936027,
      "grad_norm": 4.728305339813232,
      "learning_rate": 3.69618786165938e-05,
      "loss": 1.6969,
      "step": 610
    },
    {
      "epoch": 1.0437710437710437,
      "grad_norm": 6.207624912261963,
      "learning_rate": 3.657302579891657e-05,
      "loss": 1.9248,
      "step": 620
    },
    {
      "epoch": 1.0606060606060606,
      "grad_norm": 4.343847274780273,
      "learning_rate": 3.618057615035289e-05,
      "loss": 1.7637,
      "step": 630
    },
    {
      "epoch": 1.0774410774410774,
      "grad_norm": 6.621979236602783,
      "learning_rate": 3.578465164203134e-05,
      "loss": 1.9147,
      "step": 640
    },
    {
      "epoch": 1.0942760942760943,
      "grad_norm": 6.866629600524902,
      "learning_rate": 3.5385375325047166e-05,
      "loss": 1.8866,
      "step": 650
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 4.001997470855713,
      "learning_rate": 3.4982871292218805e-05,
      "loss": 1.8466,
      "step": 660
    },
    {
      "epoch": 1.127946127946128,
      "grad_norm": 7.133988857269287,
      "learning_rate": 3.4577264639520494e-05,
      "loss": 1.9493,
      "step": 670
    },
    {
      "epoch": 1.144781144781145,
      "grad_norm": 6.602786540985107,
      "learning_rate": 3.416868142720316e-05,
      "loss": 1.9226,
      "step": 680
    },
    {
      "epoch": 1.1616161616161615,
      "grad_norm": 9.323063850402832,
      "learning_rate": 3.37572486406156e-05,
      "loss": 1.9672,
      "step": 690
    },
    {
      "epoch": 1.1784511784511784,
      "grad_norm": 4.18539571762085,
      "learning_rate": 3.3343094150738166e-05,
      "loss": 2.0295,
      "step": 700
    },
    {
      "epoch": 1.1952861952861953,
      "grad_norm": 5.8379387855529785,
      "learning_rate": 3.292634667444117e-05,
      "loss": 1.9455,
      "step": 710
    },
    {
      "epoch": 1.2121212121212122,
      "grad_norm": 5.1855902671813965,
      "learning_rate": 3.250713573448044e-05,
      "loss": 1.9526,
      "step": 720
    },
    {
      "epoch": 1.228956228956229,
      "grad_norm": 5.360795974731445,
      "learning_rate": 3.208559161924232e-05,
      "loss": 1.705,
      "step": 730
    },
    {
      "epoch": 1.2457912457912457,
      "grad_norm": 7.308452606201172,
      "learning_rate": 3.166184534225087e-05,
      "loss": 2.0892,
      "step": 740
    },
    {
      "epoch": 1.2626262626262625,
      "grad_norm": 3.909184217453003,
      "learning_rate": 3.1236028601449534e-05,
      "loss": 1.9512,
      "step": 750
    },
    {
      "epoch": 1.2794612794612794,
      "grad_norm": 5.3987040519714355,
      "learning_rate": 3.080827373827016e-05,
      "loss": 1.9843,
      "step": 760
    },
    {
      "epoch": 1.2962962962962963,
      "grad_norm": 6.810695648193359,
      "learning_rate": 3.0378713696502097e-05,
      "loss": 1.9476,
      "step": 770
    },
    {
      "epoch": 1.3131313131313131,
      "grad_norm": 4.940785884857178,
      "learning_rate": 2.9947481980973934e-05,
      "loss": 1.8776,
      "step": 780
    },
    {
      "epoch": 1.32996632996633,
      "grad_norm": 3.6534156799316406,
      "learning_rate": 2.9514712616061045e-05,
      "loss": 1.9687,
      "step": 790
    },
    {
      "epoch": 1.3468013468013469,
      "grad_norm": 7.7403788566589355,
      "learning_rate": 2.9080540104031485e-05,
      "loss": 1.8009,
      "step": 800
    },
    {
      "epoch": 1.3636363636363638,
      "grad_norm": 6.852053165435791,
      "learning_rate": 2.8645099383243518e-05,
      "loss": 2.0693,
      "step": 810
    },
    {
      "epoch": 1.3804713804713804,
      "grad_norm": 7.782318592071533,
      "learning_rate": 2.8208525786207485e-05,
      "loss": 1.943,
      "step": 820
    },
    {
      "epoch": 1.3973063973063973,
      "grad_norm": 5.953694820404053,
      "learning_rate": 2.7770954997525277e-05,
      "loss": 1.9966,
      "step": 830
    },
    {
      "epoch": 1.4141414141414141,
      "grad_norm": 7.4925360679626465,
      "learning_rate": 2.7332523011720272e-05,
      "loss": 1.8169,
      "step": 840
    },
    {
      "epoch": 1.430976430976431,
      "grad_norm": 6.120523929595947,
      "learning_rate": 2.6893366090971067e-05,
      "loss": 1.6022,
      "step": 850
    },
    {
      "epoch": 1.4478114478114479,
      "grad_norm": 5.6537699699401855,
      "learning_rate": 2.6453620722761896e-05,
      "loss": 1.9344,
      "step": 860
    },
    {
      "epoch": 1.4646464646464645,
      "grad_norm": 8.110011100769043,
      "learning_rate": 2.6013423577463174e-05,
      "loss": 1.8895,
      "step": 870
    },
    {
      "epoch": 1.4814814814814814,
      "grad_norm": 6.937915325164795,
      "learning_rate": 2.557291146585506e-05,
      "loss": 1.8306,
      "step": 880
    },
    {
      "epoch": 1.4983164983164983,
      "grad_norm": 6.417893886566162,
      "learning_rate": 2.5132221296607445e-05,
      "loss": 1.7104,
      "step": 890
    },
    {
      "epoch": 1.5151515151515151,
      "grad_norm": 5.459097862243652,
      "learning_rate": 2.469149003372954e-05,
      "loss": 1.9575,
      "step": 900
    },
    {
      "epoch": 1.531986531986532,
      "grad_norm": 7.772295951843262,
      "learning_rate": 2.4250854654002232e-05,
      "loss": 1.9303,
      "step": 910
    },
    {
      "epoch": 1.5488215488215489,
      "grad_norm": 9.859537124633789,
      "learning_rate": 2.3810452104406444e-05,
      "loss": 2.0681,
      "step": 920
    },
    {
      "epoch": 1.5656565656565657,
      "grad_norm": 3.643014907836914,
      "learning_rate": 2.3370419259560837e-05,
      "loss": 1.9672,
      "step": 930
    },
    {
      "epoch": 1.5824915824915826,
      "grad_norm": 7.973616123199463,
      "learning_rate": 2.29308928791819e-05,
      "loss": 1.8944,
      "step": 940
    },
    {
      "epoch": 1.5993265993265995,
      "grad_norm": 8.799342155456543,
      "learning_rate": 2.2492009565579876e-05,
      "loss": 2.0704,
      "step": 950
    },
    {
      "epoch": 1.6161616161616161,
      "grad_norm": 9.770369529724121,
      "learning_rate": 2.2053905721203458e-05,
      "loss": 2.0266,
      "step": 960
    },
    {
      "epoch": 1.632996632996633,
      "grad_norm": 7.306504249572754,
      "learning_rate": 2.161671750624673e-05,
      "loss": 1.7485,
      "step": 970
    },
    {
      "epoch": 1.6498316498316499,
      "grad_norm": 7.927485942840576,
      "learning_rate": 2.1180580796331324e-05,
      "loss": 1.697,
      "step": 980
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 6.952017784118652,
      "learning_rate": 2.0745631140277094e-05,
      "loss": 1.926,
      "step": 990
    },
    {
      "epoch": 1.6835016835016834,
      "grad_norm": 7.3350396156311035,
      "learning_rate": 2.031200371797425e-05,
      "loss": 1.6561,
      "step": 1000
    },
    {
      "epoch": 1.7003367003367003,
      "grad_norm": 7.578257083892822,
      "learning_rate": 1.9879833298370238e-05,
      "loss": 1.9912,
      "step": 1010
    },
    {
      "epoch": 1.7171717171717171,
      "grad_norm": 7.0355753898620605,
      "learning_rate": 1.9492236680336485e-05,
      "loss": 1.7038,
      "step": 1020
    },
    {
      "epoch": 1.734006734006734,
      "grad_norm": 7.267971038818359,
      "learning_rate": 1.906320419886445e-05,
      "loss": 1.9112,
      "step": 1030
    },
    {
      "epoch": 1.7508417508417509,
      "grad_norm": 6.544770240783691,
      "learning_rate": 1.8636016839910497e-05,
      "loss": 1.7737,
      "step": 1040
    },
    {
      "epoch": 1.7676767676767677,
      "grad_norm": 5.931102752685547,
      "learning_rate": 1.821080737088685e-05,
      "loss": 1.8481,
      "step": 1050
    },
    {
      "epoch": 1.7845117845117846,
      "grad_norm": 5.879185676574707,
      "learning_rate": 1.778770794448877e-05,
      "loss": 1.9272,
      "step": 1060
    },
    {
      "epoch": 1.8013468013468015,
      "grad_norm": 8.077104568481445,
      "learning_rate": 1.7366850057622175e-05,
      "loss": 1.8372,
      "step": 1070
    },
    {
      "epoch": 1.8181818181818183,
      "grad_norm": 5.2831244468688965,
      "learning_rate": 1.694836451053522e-05,
      "loss": 1.7639,
      "step": 1080
    },
    {
      "epoch": 1.835016835016835,
      "grad_norm": 4.322843074798584,
      "learning_rate": 1.6532381366166264e-05,
      "loss": 1.8766,
      "step": 1090
    },
    {
      "epoch": 1.8518518518518519,
      "grad_norm": 5.8645548820495605,
      "learning_rate": 1.6119029909721106e-05,
      "loss": 2.0219,
      "step": 1100
    },
    {
      "epoch": 1.8686868686868687,
      "grad_norm": 5.842085361480713,
      "learning_rate": 1.5708438608491814e-05,
      "loss": 2.0644,
      "step": 1110
    },
    {
      "epoch": 1.8855218855218854,
      "grad_norm": 5.038226127624512,
      "learning_rate": 1.5300735071929845e-05,
      "loss": 1.9035,
      "step": 1120
    },
    {
      "epoch": 1.9023569023569022,
      "grad_norm": 6.452444076538086,
      "learning_rate": 1.4896046011985765e-05,
      "loss": 1.9606,
      "step": 1130
    },
    {
      "epoch": 1.9191919191919191,
      "grad_norm": 7.44296932220459,
      "learning_rate": 1.4494497203727844e-05,
      "loss": 1.7241,
      "step": 1140
    },
    {
      "epoch": 1.936026936026936,
      "grad_norm": 5.8304619789123535,
      "learning_rate": 1.4096213446251966e-05,
      "loss": 2.0278,
      "step": 1150
    },
    {
      "epoch": 1.9528619528619529,
      "grad_norm": 10.173737525939941,
      "learning_rate": 1.3701318523894656e-05,
      "loss": 1.8184,
      "step": 1160
    },
    {
      "epoch": 1.9696969696969697,
      "grad_norm": 7.054353713989258,
      "learning_rate": 1.3309935167761716e-05,
      "loss": 1.882,
      "step": 1170
    },
    {
      "epoch": 1.9865319865319866,
      "grad_norm": 6.652568340301514,
      "learning_rate": 1.2922185017584037e-05,
      "loss": 1.6882,
      "step": 1180
    },
    {
      "epoch": 2.0033670033670035,
      "grad_norm": 9.29315185546875,
      "learning_rate": 1.2538188583912674e-05,
      "loss": 1.8583,
      "step": 1190
    },
    {
      "epoch": 2.0202020202020203,
      "grad_norm": 6.203002452850342,
      "learning_rate": 1.2158065210664848e-05,
      "loss": 1.8807,
      "step": 1200
    },
    {
      "epoch": 2.037037037037037,
      "grad_norm": 5.840773105621338,
      "learning_rate": 1.1781933038032515e-05,
      "loss": 1.8674,
      "step": 1210
    },
    {
      "epoch": 2.053872053872054,
      "grad_norm": 7.845053195953369,
      "learning_rate": 1.14099089657651e-05,
      "loss": 1.74,
      "step": 1220
    },
    {
      "epoch": 2.0707070707070705,
      "grad_norm": 10.924101829528809,
      "learning_rate": 1.1042108616837692e-05,
      "loss": 2.0004,
      "step": 1230
    },
    {
      "epoch": 2.0875420875420874,
      "grad_norm": 6.253499984741211,
      "learning_rate": 1.067864630151608e-05,
      "loss": 1.7647,
      "step": 1240
    },
    {
      "epoch": 2.1043771043771042,
      "grad_norm": 6.9842939376831055,
      "learning_rate": 1.0319634981829782e-05,
      "loss": 1.6262,
      "step": 1250
    },
    {
      "epoch": 2.121212121212121,
      "grad_norm": 7.460921287536621,
      "learning_rate": 9.965186236464047e-06,
      "loss": 1.5536,
      "step": 1260
    },
    {
      "epoch": 2.138047138047138,
      "grad_norm": 7.046140193939209,
      "learning_rate": 9.615410226081891e-06,
      "loss": 1.9304,
      "step": 1270
    },
    {
      "epoch": 2.154882154882155,
      "grad_norm": 8.44336223602295,
      "learning_rate": 9.270415659086748e-06,
      "loss": 1.8702,
      "step": 1280
    },
    {
      "epoch": 2.1717171717171717,
      "grad_norm": 5.6424641609191895,
      "learning_rate": 8.930309757836517e-06,
      "loss": 1.6715,
      "step": 1290
    },
    {
      "epoch": 2.1885521885521886,
      "grad_norm": 12.148447036743164,
      "learning_rate": 8.595198225319478e-06,
      "loss": 1.7344,
      "step": 1300
    },
    {
      "epoch": 2.2053872053872055,
      "grad_norm": 7.99276065826416,
      "learning_rate": 8.265185212302355e-06,
      "loss": 1.7974,
      "step": 1310
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 8.078484535217285,
      "learning_rate": 7.940373284960934e-06,
      "loss": 1.7363,
      "step": 1320
    },
    {
      "epoch": 2.239057239057239,
      "grad_norm": 8.506318092346191,
      "learning_rate": 7.6208633930029805e-06,
      "loss": 1.8342,
      "step": 1330
    },
    {
      "epoch": 2.255892255892256,
      "grad_norm": 7.238159656524658,
      "learning_rate": 7.306754838293703e-06,
      "loss": 2.0333,
      "step": 1340
    },
    {
      "epoch": 2.2727272727272725,
      "grad_norm": 7.155613899230957,
      "learning_rate": 6.998145243993284e-06,
      "loss": 1.8313,
      "step": 1350
    },
    {
      "epoch": 2.28956228956229,
      "grad_norm": 5.4975762367248535,
      "learning_rate": 6.69513052421612e-06,
      "loss": 1.738,
      "step": 1360
    },
    {
      "epoch": 2.3063973063973062,
      "grad_norm": 7.5331597328186035,
      "learning_rate": 6.397804854221334e-06,
      "loss": 1.864,
      "step": 1370
    },
    {
      "epoch": 2.323232323232323,
      "grad_norm": 10.124072074890137,
      "learning_rate": 6.106260641143546e-06,
      "loss": 1.6441,
      "step": 1380
    },
    {
      "epoch": 2.34006734006734,
      "grad_norm": 6.610650062561035,
      "learning_rate": 5.820588495273324e-06,
      "loss": 1.9974,
      "step": 1390
    },
    {
      "epoch": 2.356902356902357,
      "grad_norm": 10.83629035949707,
      "learning_rate": 5.5408772018959995e-06,
      "loss": 1.788,
      "step": 1400
    },
    {
      "epoch": 2.3737373737373737,
      "grad_norm": 8.132287979125977,
      "learning_rate": 5.267213693697695e-06,
      "loss": 1.7337,
      "step": 1410
    },
    {
      "epoch": 2.3905723905723906,
      "grad_norm": 6.018608093261719,
      "learning_rate": 4.999683023747159e-06,
      "loss": 1.8088,
      "step": 1420
    },
    {
      "epoch": 2.4074074074074074,
      "grad_norm": 6.688351154327393,
      "learning_rate": 4.7383683390617104e-06,
      "loss": 1.8822,
      "step": 1430
    },
    {
      "epoch": 2.4242424242424243,
      "grad_norm": 6.652238845825195,
      "learning_rate": 4.483350854765672e-06,
      "loss": 1.7295,
      "step": 1440
    },
    {
      "epoch": 2.441077441077441,
      "grad_norm": 7.677175998687744,
      "learning_rate": 4.234709828849137e-06,
      "loss": 1.9172,
      "step": 1450
    },
    {
      "epoch": 2.457912457912458,
      "grad_norm": 6.691679954528809,
      "learning_rate": 3.9925225375350404e-06,
      "loss": 1.896,
      "step": 1460
    },
    {
      "epoch": 2.474747474747475,
      "grad_norm": 6.405922889709473,
      "learning_rate": 3.756864251262143e-06,
      "loss": 1.662,
      "step": 1470
    },
    {
      "epoch": 2.4915824915824913,
      "grad_norm": 9.106470108032227,
      "learning_rate": 3.5278082112914083e-06,
      "loss": 1.8358,
      "step": 1480
    },
    {
      "epoch": 2.5084175084175087,
      "grad_norm": 6.764180660247803,
      "learning_rate": 3.3054256069430383e-06,
      "loss": 1.7713,
      "step": 1490
    },
    {
      "epoch": 2.525252525252525,
      "grad_norm": 8.211374282836914,
      "learning_rate": 3.0897855534712333e-06,
      "loss": 1.7359,
      "step": 1500
    },
    {
      "epoch": 2.542087542087542,
      "grad_norm": 5.966109752655029,
      "learning_rate": 2.8809550705835548e-06,
      "loss": 1.79,
      "step": 1510
    },
    {
      "epoch": 2.558922558922559,
      "grad_norm": 11.352228164672852,
      "learning_rate": 2.678999061611595e-06,
      "loss": 1.7324,
      "step": 1520
    },
    {
      "epoch": 2.5757575757575757,
      "grad_norm": 9.940911293029785,
      "learning_rate": 2.4839802933393606e-06,
      "loss": 1.7529,
      "step": 1530
    },
    {
      "epoch": 2.5925925925925926,
      "grad_norm": 8.974912643432617,
      "learning_rate": 2.295959376495757e-06,
      "loss": 1.9429,
      "step": 1540
    },
    {
      "epoch": 2.6094276094276094,
      "grad_norm": 6.593918323516846,
      "learning_rate": 2.1149947469170676e-06,
      "loss": 1.5388,
      "step": 1550
    },
    {
      "epoch": 2.6262626262626263,
      "grad_norm": 7.941250324249268,
      "learning_rate": 1.9411426473854688e-06,
      "loss": 1.6763,
      "step": 1560
    },
    {
      "epoch": 2.643097643097643,
      "grad_norm": 7.069528579711914,
      "learning_rate": 1.7744571101490498e-06,
      "loss": 1.8412,
      "step": 1570
    },
    {
      "epoch": 2.65993265993266,
      "grad_norm": 7.123358726501465,
      "learning_rate": 1.6149899401289414e-06,
      "loss": 1.8719,
      "step": 1580
    },
    {
      "epoch": 2.676767676767677,
      "grad_norm": 8.62523078918457,
      "learning_rate": 1.4627906988186113e-06,
      "loss": 1.7191,
      "step": 1590
    },
    {
      "epoch": 2.6936026936026938,
      "grad_norm": 6.393489360809326,
      "learning_rate": 1.3179066888804298e-06,
      "loss": 2.0463,
      "step": 1600
    },
    {
      "epoch": 2.71043771043771,
      "grad_norm": 9.608689308166504,
      "learning_rate": 1.1803829394442855e-06,
      "loss": 1.8849,
      "step": 1610
    },
    {
      "epoch": 2.7272727272727275,
      "grad_norm": 6.896596431732178,
      "learning_rate": 1.0502621921127776e-06,
      "loss": 1.8083,
      "step": 1620
    },
    {
      "epoch": 2.744107744107744,
      "grad_norm": 6.67935037612915,
      "learning_rate": 9.275848876773796e-07,
      "loss": 1.7658,
      "step": 1630
    },
    {
      "epoch": 2.760942760942761,
      "grad_norm": 8.784103393554688,
      "learning_rate": 8.123891535496475e-07,
      "loss": 1.9462,
      "step": 1640
    },
    {
      "epoch": 2.7777777777777777,
      "grad_norm": 5.157577037811279,
      "learning_rate": 7.047107919114588e-07,
      "loss": 1.857,
      "step": 1650
    },
    {
      "epoch": 2.7946127946127945,
      "grad_norm": 7.425522804260254,
      "learning_rate": 6.045832685878828e-07,
      "loss": 1.7423,
      "step": 1660
    },
    {
      "epoch": 2.8114478114478114,
      "grad_norm": 9.610501289367676,
      "learning_rate": 5.120377026462037e-07,
      "loss": 1.8188,
      "step": 1670
    },
    {
      "epoch": 2.8282828282828283,
      "grad_norm": 5.929516315460205,
      "learning_rate": 4.271028567242819e-07,
      "loss": 1.5702,
      "step": 1680
    },
    {
      "epoch": 2.845117845117845,
      "grad_norm": 6.968934535980225,
      "learning_rate": 3.4980512809127954e-07,
      "loss": 1.5563,
      "step": 1690
    },
    {
      "epoch": 2.861952861952862,
      "grad_norm": 7.659050941467285,
      "learning_rate": 2.8016854044356775e-07,
      "loss": 1.8192,
      "step": 1700
    },
    {
      "epoch": 2.878787878787879,
      "grad_norm": 8.355278968811035,
      "learning_rate": 2.1821473643827138e-07,
      "loss": 1.8219,
      "step": 1710
    },
    {
      "epoch": 2.8956228956228958,
      "grad_norm": 9.674089431762695,
      "learning_rate": 1.639629709668633e-07,
      "loss": 1.7927,
      "step": 1720
    },
    {
      "epoch": 2.9124579124579126,
      "grad_norm": 5.425037384033203,
      "learning_rate": 1.1743010517085428e-07,
      "loss": 1.8128,
      "step": 1730
    },
    {
      "epoch": 2.929292929292929,
      "grad_norm": 8.92187786102295,
      "learning_rate": 7.863060120144317e-08,
      "loss": 1.7988,
      "step": 1740
    },
    {
      "epoch": 2.9461279461279464,
      "grad_norm": 7.1704864501953125,
      "learning_rate": 4.7576517724756685e-08,
      "loss": 1.8015,
      "step": 1750
    },
    {
      "epoch": 2.962962962962963,
      "grad_norm": 8.75033950805664,
      "learning_rate": 2.4277506174105735e-08,
      "loss": 1.916,
      "step": 1760
    },
    {
      "epoch": 2.9797979797979797,
      "grad_norm": 7.728853702545166,
      "learning_rate": 8.740807750345915e-09,
      "loss": 1.9084,
      "step": 1770
    },
    {
      "epoch": 2.9966329966329965,
      "grad_norm": 8.42664623260498,
      "learning_rate": 9.71251171369425e-10,
      "loss": 1.8771,
      "step": 1780
    },
    {
      "epoch": 3.0,
      "step": 1782,
      "total_flos": 3.073490876891136e+16,
      "train_loss": 1.9493836136378022,
      "train_runtime": 349.2384,
      "train_samples_per_second": 20.402,
      "train_steps_per_second": 5.103
    }
  ],
  "logging_steps": 10,
  "max_steps": 1782,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500.0,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": false,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.073490876891136e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
