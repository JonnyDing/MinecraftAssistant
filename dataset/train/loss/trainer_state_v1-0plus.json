{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 6970,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.007173601147776184,
      "grad_norm": 1.298170566558838,
      "learning_rate": 9.999949210555528e-05,
      "loss": 2.5106,
      "step": 10
    },
    {
      "epoch": 0.014347202295552367,
      "grad_norm": 2.0555217266082764,
      "learning_rate": 9.999796843253935e-05,
      "loss": 2.2759,
      "step": 20
    },
    {
      "epoch": 0.021520803443328552,
      "grad_norm": 1.8535711765289307,
      "learning_rate": 9.999542901190684e-05,
      "loss": 2.016,
      "step": 30
    },
    {
      "epoch": 0.028694404591104734,
      "grad_norm": 2.0995495319366455,
      "learning_rate": 9.999187389524805e-05,
      "loss": 1.9192,
      "step": 40
    },
    {
      "epoch": 0.035868005738880916,
      "grad_norm": 2.382140636444092,
      "learning_rate": 9.998730315478792e-05,
      "loss": 1.7824,
      "step": 50
    },
    {
      "epoch": 0.043041606886657105,
      "grad_norm": 2.2361385822296143,
      "learning_rate": 9.998171688338462e-05,
      "loss": 1.7727,
      "step": 60
    },
    {
      "epoch": 0.05021520803443329,
      "grad_norm": 2.113255262374878,
      "learning_rate": 9.99751151945276e-05,
      "loss": 1.9258,
      "step": 70
    },
    {
      "epoch": 0.05738880918220947,
      "grad_norm": 2.039304256439209,
      "learning_rate": 9.99674982223353e-05,
      "loss": 1.8441,
      "step": 80
    },
    {
      "epoch": 0.06456241032998565,
      "grad_norm": 2.3045413494110107,
      "learning_rate": 9.995886612155242e-05,
      "loss": 1.7628,
      "step": 90
    },
    {
      "epoch": 0.07173601147776183,
      "grad_norm": 1.8735686540603638,
      "learning_rate": 9.994921906754682e-05,
      "loss": 1.8377,
      "step": 100
    },
    {
      "epoch": 0.07890961262553801,
      "grad_norm": 2.355377435684204,
      "learning_rate": 9.993855725630588e-05,
      "loss": 1.7385,
      "step": 110
    },
    {
      "epoch": 0.08608321377331421,
      "grad_norm": 2.4928340911865234,
      "learning_rate": 9.992688090443263e-05,
      "loss": 1.7499,
      "step": 120
    },
    {
      "epoch": 0.09325681492109039,
      "grad_norm": 2.198096513748169,
      "learning_rate": 9.991419024914121e-05,
      "loss": 1.7619,
      "step": 130
    },
    {
      "epoch": 0.10043041606886657,
      "grad_norm": 2.415618896484375,
      "learning_rate": 9.990048554825215e-05,
      "loss": 1.741,
      "step": 140
    },
    {
      "epoch": 0.10760401721664276,
      "grad_norm": 2.26796817779541,
      "learning_rate": 9.988576708018711e-05,
      "loss": 1.7711,
      "step": 150
    },
    {
      "epoch": 0.11477761836441894,
      "grad_norm": 2.9681622982025146,
      "learning_rate": 9.987003514396322e-05,
      "loss": 1.7294,
      "step": 160
    },
    {
      "epoch": 0.12195121951219512,
      "grad_norm": 3.153125286102295,
      "learning_rate": 9.985329005918702e-05,
      "loss": 1.7273,
      "step": 170
    },
    {
      "epoch": 0.1291248206599713,
      "grad_norm": 2.276196241378784,
      "learning_rate": 9.983553216604792e-05,
      "loss": 1.6607,
      "step": 180
    },
    {
      "epoch": 0.13629842180774748,
      "grad_norm": 2.4939064979553223,
      "learning_rate": 9.981676182531131e-05,
      "loss": 1.6889,
      "step": 190
    },
    {
      "epoch": 0.14347202295552366,
      "grad_norm": 2.7025654315948486,
      "learning_rate": 9.979697941831128e-05,
      "loss": 1.6446,
      "step": 200
    },
    {
      "epoch": 0.15064562410329985,
      "grad_norm": 2.2409722805023193,
      "learning_rate": 9.977618534694282e-05,
      "loss": 1.6623,
      "step": 210
    },
    {
      "epoch": 0.15781922525107603,
      "grad_norm": 2.4798145294189453,
      "learning_rate": 9.975438003365366e-05,
      "loss": 1.6848,
      "step": 220
    },
    {
      "epoch": 0.1649928263988522,
      "grad_norm": 2.054076910018921,
      "learning_rate": 9.973156392143568e-05,
      "loss": 1.6779,
      "step": 230
    },
    {
      "epoch": 0.17216642754662842,
      "grad_norm": 2.2682740688323975,
      "learning_rate": 9.970773747381597e-05,
      "loss": 1.6695,
      "step": 240
    },
    {
      "epoch": 0.1793400286944046,
      "grad_norm": 2.2365875244140625,
      "learning_rate": 9.968290117484732e-05,
      "loss": 1.6603,
      "step": 250
    },
    {
      "epoch": 0.18651362984218078,
      "grad_norm": 2.398155689239502,
      "learning_rate": 9.96570555290985e-05,
      "loss": 1.6664,
      "step": 260
    },
    {
      "epoch": 0.19368723098995697,
      "grad_norm": 2.7290360927581787,
      "learning_rate": 9.963020106164386e-05,
      "loss": 1.6599,
      "step": 270
    },
    {
      "epoch": 0.20086083213773315,
      "grad_norm": 2.675737142562866,
      "learning_rate": 9.960233831805284e-05,
      "loss": 1.6702,
      "step": 280
    },
    {
      "epoch": 0.20803443328550933,
      "grad_norm": 3.1560027599334717,
      "learning_rate": 9.957346786437869e-05,
      "loss": 1.712,
      "step": 290
    },
    {
      "epoch": 0.2152080344332855,
      "grad_norm": 2.8875186443328857,
      "learning_rate": 9.95435902871472e-05,
      "loss": 1.6174,
      "step": 300
    },
    {
      "epoch": 0.2223816355810617,
      "grad_norm": 2.5566627979278564,
      "learning_rate": 9.951270619334454e-05,
      "loss": 1.5903,
      "step": 310
    },
    {
      "epoch": 0.22955523672883787,
      "grad_norm": 2.8374459743499756,
      "learning_rate": 9.948081621040511e-05,
      "loss": 1.6414,
      "step": 320
    },
    {
      "epoch": 0.23672883787661406,
      "grad_norm": 2.256355047225952,
      "learning_rate": 9.944792098619872e-05,
      "loss": 1.6978,
      "step": 330
    },
    {
      "epoch": 0.24390243902439024,
      "grad_norm": 2.5966529846191406,
      "learning_rate": 9.941402118901744e-05,
      "loss": 1.6577,
      "step": 340
    },
    {
      "epoch": 0.25107604017216645,
      "grad_norm": 2.9095821380615234,
      "learning_rate": 9.937911750756199e-05,
      "loss": 1.6139,
      "step": 350
    },
    {
      "epoch": 0.2582496413199426,
      "grad_norm": 2.2617950439453125,
      "learning_rate": 9.934321065092785e-05,
      "loss": 1.7024,
      "step": 360
    },
    {
      "epoch": 0.2654232424677188,
      "grad_norm": 2.526320219039917,
      "learning_rate": 9.930630134859071e-05,
      "loss": 1.6128,
      "step": 370
    },
    {
      "epoch": 0.27259684361549497,
      "grad_norm": 2.363527297973633,
      "learning_rate": 9.926839035039177e-05,
      "loss": 1.582,
      "step": 380
    },
    {
      "epoch": 0.2797704447632712,
      "grad_norm": 2.046205997467041,
      "learning_rate": 9.922947842652242e-05,
      "loss": 1.623,
      "step": 390
    },
    {
      "epoch": 0.28694404591104733,
      "grad_norm": 2.961033821105957,
      "learning_rate": 9.91895663675087e-05,
      "loss": 1.6147,
      "step": 400
    },
    {
      "epoch": 0.29411764705882354,
      "grad_norm": 2.524362087249756,
      "learning_rate": 9.91486549841951e-05,
      "loss": 1.606,
      "step": 410
    },
    {
      "epoch": 0.3012912482065997,
      "grad_norm": 2.474078416824341,
      "learning_rate": 9.91067451077282e-05,
      "loss": 1.6242,
      "step": 420
    },
    {
      "epoch": 0.3084648493543759,
      "grad_norm": 2.547837734222412,
      "learning_rate": 9.906383758953971e-05,
      "loss": 1.594,
      "step": 430
    },
    {
      "epoch": 0.31563845050215206,
      "grad_norm": 2.73883056640625,
      "learning_rate": 9.901993330132931e-05,
      "loss": 1.5854,
      "step": 440
    },
    {
      "epoch": 0.32281205164992827,
      "grad_norm": 2.5496866703033447,
      "learning_rate": 9.897503313504669e-05,
      "loss": 1.6164,
      "step": 450
    },
    {
      "epoch": 0.3299856527977044,
      "grad_norm": 2.651834487915039,
      "learning_rate": 9.892913800287368e-05,
      "loss": 1.6135,
      "step": 460
    },
    {
      "epoch": 0.33715925394548063,
      "grad_norm": 2.4376673698425293,
      "learning_rate": 9.88822488372056e-05,
      "loss": 1.6226,
      "step": 470
    },
    {
      "epoch": 0.34433285509325684,
      "grad_norm": 2.5227110385894775,
      "learning_rate": 9.883436659063229e-05,
      "loss": 1.6708,
      "step": 480
    },
    {
      "epoch": 0.351506456241033,
      "grad_norm": 2.9039149284362793,
      "learning_rate": 9.878549223591884e-05,
      "loss": 1.7026,
      "step": 490
    },
    {
      "epoch": 0.3586800573888092,
      "grad_norm": 2.6755917072296143,
      "learning_rate": 9.87356267659858e-05,
      "loss": 1.5681,
      "step": 500
    },
    {
      "epoch": 0.36585365853658536,
      "grad_norm": 2.323331356048584,
      "learning_rate": 9.868477119388896e-05,
      "loss": 1.6316,
      "step": 510
    },
    {
      "epoch": 0.37302725968436157,
      "grad_norm": 2.490771532058716,
      "learning_rate": 9.863292655279883e-05,
      "loss": 1.5567,
      "step": 520
    },
    {
      "epoch": 0.3802008608321377,
      "grad_norm": 2.4119927883148193,
      "learning_rate": 9.85800938959796e-05,
      "loss": 1.547,
      "step": 530
    },
    {
      "epoch": 0.38737446197991393,
      "grad_norm": 2.8178136348724365,
      "learning_rate": 9.852627429676779e-05,
      "loss": 1.6052,
      "step": 540
    },
    {
      "epoch": 0.3945480631276901,
      "grad_norm": 2.8599014282226562,
      "learning_rate": 9.847146884855045e-05,
      "loss": 1.5228,
      "step": 550
    },
    {
      "epoch": 0.4017216642754663,
      "grad_norm": 2.6402626037597656,
      "learning_rate": 9.841567866474286e-05,
      "loss": 1.5779,
      "step": 560
    },
    {
      "epoch": 0.40889526542324245,
      "grad_norm": 2.899078607559204,
      "learning_rate": 9.835890487876599e-05,
      "loss": 1.599,
      "step": 570
    },
    {
      "epoch": 0.41606886657101866,
      "grad_norm": 2.583547830581665,
      "learning_rate": 9.830114864402347e-05,
      "loss": 1.5901,
      "step": 580
    },
    {
      "epoch": 0.4232424677187948,
      "grad_norm": 2.500868558883667,
      "learning_rate": 9.824241113387815e-05,
      "loss": 1.6602,
      "step": 590
    },
    {
      "epoch": 0.430416068866571,
      "grad_norm": 2.242496967315674,
      "learning_rate": 9.818269354162821e-05,
      "loss": 1.5674,
      "step": 600
    },
    {
      "epoch": 0.4375896700143472,
      "grad_norm": 2.653169631958008,
      "learning_rate": 9.8121997080483e-05,
      "loss": 1.5692,
      "step": 610
    },
    {
      "epoch": 0.4447632711621234,
      "grad_norm": 2.8072593212127686,
      "learning_rate": 9.80603229835383e-05,
      "loss": 1.5687,
      "step": 620
    },
    {
      "epoch": 0.4519368723098996,
      "grad_norm": 3.2042555809020996,
      "learning_rate": 9.799767250375141e-05,
      "loss": 1.6127,
      "step": 630
    },
    {
      "epoch": 0.45911047345767575,
      "grad_norm": 2.7730493545532227,
      "learning_rate": 9.793404691391552e-05,
      "loss": 1.5405,
      "step": 640
    },
    {
      "epoch": 0.46628407460545196,
      "grad_norm": 2.593186378479004,
      "learning_rate": 9.7869447506634e-05,
      "loss": 1.563,
      "step": 650
    },
    {
      "epoch": 0.4734576757532281,
      "grad_norm": 2.784064531326294,
      "learning_rate": 9.780387559429405e-05,
      "loss": 1.5964,
      "step": 660
    },
    {
      "epoch": 0.4806312769010043,
      "grad_norm": 3.714168071746826,
      "learning_rate": 9.773733250904005e-05,
      "loss": 1.5671,
      "step": 670
    },
    {
      "epoch": 0.4878048780487805,
      "grad_norm": 2.833815336227417,
      "learning_rate": 9.766981960274653e-05,
      "loss": 1.52,
      "step": 680
    },
    {
      "epoch": 0.4949784791965567,
      "grad_norm": 2.8818602561950684,
      "learning_rate": 9.760133824699071e-05,
      "loss": 1.5878,
      "step": 690
    },
    {
      "epoch": 0.5021520803443329,
      "grad_norm": 2.9483187198638916,
      "learning_rate": 9.753188983302462e-05,
      "loss": 1.5713,
      "step": 700
    },
    {
      "epoch": 0.509325681492109,
      "grad_norm": 2.8911423683166504,
      "learning_rate": 9.746856059138747e-05,
      "loss": 1.5733,
      "step": 710
    },
    {
      "epoch": 0.5164992826398852,
      "grad_norm": 2.616671323776245,
      "learning_rate": 9.739727867006207e-05,
      "loss": 1.6175,
      "step": 720
    },
    {
      "epoch": 0.5236728837876614,
      "grad_norm": 2.349388360977173,
      "learning_rate": 9.73250338361554e-05,
      "loss": 1.5734,
      "step": 730
    },
    {
      "epoch": 0.5308464849354376,
      "grad_norm": 2.8104963302612305,
      "learning_rate": 9.725182755737744e-05,
      "loss": 1.6296,
      "step": 740
    },
    {
      "epoch": 0.5380200860832137,
      "grad_norm": 2.5167224407196045,
      "learning_rate": 9.717766132097068e-05,
      "loss": 1.5319,
      "step": 750
    },
    {
      "epoch": 0.5451936872309899,
      "grad_norm": 2.8860487937927246,
      "learning_rate": 9.710253663367992e-05,
      "loss": 1.5379,
      "step": 760
    },
    {
      "epoch": 0.5523672883787661,
      "grad_norm": 2.957317590713501,
      "learning_rate": 9.70264550217216e-05,
      "loss": 1.4989,
      "step": 770
    },
    {
      "epoch": 0.5595408895265424,
      "grad_norm": 2.9840502738952637,
      "learning_rate": 9.694941803075283e-05,
      "loss": 1.602,
      "step": 780
    },
    {
      "epoch": 0.5667144906743186,
      "grad_norm": 2.4364359378814697,
      "learning_rate": 9.687142722584002e-05,
      "loss": 1.58,
      "step": 790
    },
    {
      "epoch": 0.5738880918220947,
      "grad_norm": 2.2601335048675537,
      "learning_rate": 9.679248419142703e-05,
      "loss": 1.5334,
      "step": 800
    },
    {
      "epoch": 0.5810616929698709,
      "grad_norm": 3.148315191268921,
      "learning_rate": 9.6712590531303e-05,
      "loss": 1.5349,
      "step": 810
    },
    {
      "epoch": 0.5882352941176471,
      "grad_norm": 2.647977828979492,
      "learning_rate": 9.663174786856977e-05,
      "loss": 1.5301,
      "step": 820
    },
    {
      "epoch": 0.5954088952654233,
      "grad_norm": 2.6439216136932373,
      "learning_rate": 9.654995784560891e-05,
      "loss": 1.5749,
      "step": 830
    },
    {
      "epoch": 0.6025824964131994,
      "grad_norm": 2.718472957611084,
      "learning_rate": 9.646722212404837e-05,
      "loss": 1.5094,
      "step": 840
    },
    {
      "epoch": 0.6097560975609756,
      "grad_norm": 2.5974364280700684,
      "learning_rate": 9.638354238472867e-05,
      "loss": 1.4783,
      "step": 850
    },
    {
      "epoch": 0.6169296987087518,
      "grad_norm": 2.760101079940796,
      "learning_rate": 9.629892032766882e-05,
      "loss": 1.603,
      "step": 860
    },
    {
      "epoch": 0.624103299856528,
      "grad_norm": 3.409027099609375,
      "learning_rate": 9.62133576720317e-05,
      "loss": 1.5585,
      "step": 870
    },
    {
      "epoch": 0.6312769010043041,
      "grad_norm": 2.9583122730255127,
      "learning_rate": 9.612685615608921e-05,
      "loss": 1.5364,
      "step": 880
    },
    {
      "epoch": 0.6384505021520803,
      "grad_norm": 2.8054850101470947,
      "learning_rate": 9.603941753718695e-05,
      "loss": 1.5523,
      "step": 890
    },
    {
      "epoch": 0.6456241032998565,
      "grad_norm": 2.720571994781494,
      "learning_rate": 9.595104359170845e-05,
      "loss": 1.5673,
      "step": 900
    },
    {
      "epoch": 0.6527977044476327,
      "grad_norm": 3.3063747882843018,
      "learning_rate": 9.586173611503917e-05,
      "loss": 1.4913,
      "step": 910
    },
    {
      "epoch": 0.6599713055954088,
      "grad_norm": 2.8720548152923584,
      "learning_rate": 9.577149692152994e-05,
      "loss": 1.609,
      "step": 920
    },
    {
      "epoch": 0.667144906743185,
      "grad_norm": 2.58489727973938,
      "learning_rate": 9.568032784446018e-05,
      "loss": 1.4926,
      "step": 930
    },
    {
      "epoch": 0.6743185078909613,
      "grad_norm": 2.9971415996551514,
      "learning_rate": 9.558823073600056e-05,
      "loss": 1.5132,
      "step": 940
    },
    {
      "epoch": 0.6814921090387375,
      "grad_norm": 2.669046640396118,
      "learning_rate": 9.549520746717553e-05,
      "loss": 1.5398,
      "step": 950
    },
    {
      "epoch": 0.6886657101865137,
      "grad_norm": 2.463022470474243,
      "learning_rate": 9.540125992782512e-05,
      "loss": 1.4924,
      "step": 960
    },
    {
      "epoch": 0.6958393113342898,
      "grad_norm": 2.2842466831207275,
      "learning_rate": 9.530639002656665e-05,
      "loss": 1.5274,
      "step": 970
    },
    {
      "epoch": 0.703012912482066,
      "grad_norm": 2.6288959980010986,
      "learning_rate": 9.521059969075599e-05,
      "loss": 1.5772,
      "step": 980
    },
    {
      "epoch": 0.7101865136298422,
      "grad_norm": 3.1775739192962646,
      "learning_rate": 9.511389086644829e-05,
      "loss": 1.5329,
      "step": 990
    },
    {
      "epoch": 0.7173601147776184,
      "grad_norm": 2.6843059062957764,
      "learning_rate": 9.501626551835852e-05,
      "loss": 1.4436,
      "step": 1000
    },
    {
      "epoch": 0.7245337159253945,
      "grad_norm": 2.4255621433258057,
      "learning_rate": 9.491772562982158e-05,
      "loss": 1.5119,
      "step": 1010
    },
    {
      "epoch": 0.7317073170731707,
      "grad_norm": 2.623197078704834,
      "learning_rate": 9.481827320275197e-05,
      "loss": 1.5094,
      "step": 1020
    },
    {
      "epoch": 0.7388809182209469,
      "grad_norm": 2.4844207763671875,
      "learning_rate": 9.471791025760307e-05,
      "loss": 1.5473,
      "step": 1030
    },
    {
      "epoch": 0.7460545193687231,
      "grad_norm": 3.6124775409698486,
      "learning_rate": 9.461663883332614e-05,
      "loss": 1.5166,
      "step": 1040
    },
    {
      "epoch": 0.7532281205164992,
      "grad_norm": 2.5590381622314453,
      "learning_rate": 9.451446098732901e-05,
      "loss": 1.4787,
      "step": 1050
    },
    {
      "epoch": 0.7604017216642754,
      "grad_norm": 2.5246567726135254,
      "learning_rate": 9.441137879543404e-05,
      "loss": 1.518,
      "step": 1060
    },
    {
      "epoch": 0.7675753228120517,
      "grad_norm": 2.540229558944702,
      "learning_rate": 9.430739435183614e-05,
      "loss": 1.4916,
      "step": 1070
    },
    {
      "epoch": 0.7747489239598279,
      "grad_norm": 2.745563507080078,
      "learning_rate": 9.420250976906018e-05,
      "loss": 1.5602,
      "step": 1080
    },
    {
      "epoch": 0.7819225251076041,
      "grad_norm": 2.462373733520508,
      "learning_rate": 9.409672717791802e-05,
      "loss": 1.6048,
      "step": 1090
    },
    {
      "epoch": 0.7890961262553802,
      "grad_norm": 2.9209349155426025,
      "learning_rate": 9.399004872746529e-05,
      "loss": 1.5411,
      "step": 1100
    },
    {
      "epoch": 0.7962697274031564,
      "grad_norm": 3.0946247577667236,
      "learning_rate": 9.388247658495766e-05,
      "loss": 1.4932,
      "step": 1110
    },
    {
      "epoch": 0.8034433285509326,
      "grad_norm": 2.643073558807373,
      "learning_rate": 9.37740129358069e-05,
      "loss": 1.4931,
      "step": 1120
    },
    {
      "epoch": 0.8106169296987088,
      "grad_norm": 2.6170237064361572,
      "learning_rate": 9.366465998353637e-05,
      "loss": 1.4333,
      "step": 1130
    },
    {
      "epoch": 0.8177905308464849,
      "grad_norm": 2.998739004135132,
      "learning_rate": 9.35544199497364e-05,
      "loss": 1.4706,
      "step": 1140
    },
    {
      "epoch": 0.8249641319942611,
      "grad_norm": 2.6423611640930176,
      "learning_rate": 9.344329507401898e-05,
      "loss": 1.4368,
      "step": 1150
    },
    {
      "epoch": 0.8321377331420373,
      "grad_norm": 3.100240468978882,
      "learning_rate": 9.333128761397241e-05,
      "loss": 1.5443,
      "step": 1160
    },
    {
      "epoch": 0.8393113342898135,
      "grad_norm": 2.7220427989959717,
      "learning_rate": 9.321839984511534e-05,
      "loss": 1.4932,
      "step": 1170
    },
    {
      "epoch": 0.8464849354375896,
      "grad_norm": 2.170520544052124,
      "learning_rate": 9.310463406085061e-05,
      "loss": 1.5285,
      "step": 1180
    },
    {
      "epoch": 0.8536585365853658,
      "grad_norm": 3.0441653728485107,
      "learning_rate": 9.298999257241863e-05,
      "loss": 1.5177,
      "step": 1190
    },
    {
      "epoch": 0.860832137733142,
      "grad_norm": 2.585625648498535,
      "learning_rate": 9.287447770885037e-05,
      "loss": 1.4817,
      "step": 1200
    },
    {
      "epoch": 0.8680057388809183,
      "grad_norm": 2.8890604972839355,
      "learning_rate": 9.275809181692015e-05,
      "loss": 1.5232,
      "step": 1210
    },
    {
      "epoch": 0.8751793400286944,
      "grad_norm": 2.8181991577148438,
      "learning_rate": 9.264083726109789e-05,
      "loss": 1.5014,
      "step": 1220
    },
    {
      "epoch": 0.8823529411764706,
      "grad_norm": 2.193950891494751,
      "learning_rate": 9.25227164235011e-05,
      "loss": 1.4799,
      "step": 1230
    },
    {
      "epoch": 0.8895265423242468,
      "grad_norm": 2.9745285511016846,
      "learning_rate": 9.240373170384643e-05,
      "loss": 1.5024,
      "step": 1240
    },
    {
      "epoch": 0.896700143472023,
      "grad_norm": 2.8971333503723145,
      "learning_rate": 9.228388551940104e-05,
      "loss": 1.5379,
      "step": 1250
    },
    {
      "epoch": 0.9038737446197992,
      "grad_norm": 2.574913263320923,
      "learning_rate": 9.216318030493338e-05,
      "loss": 1.5507,
      "step": 1260
    },
    {
      "epoch": 0.9110473457675753,
      "grad_norm": 2.950390100479126,
      "learning_rate": 9.204161851266376e-05,
      "loss": 1.5381,
      "step": 1270
    },
    {
      "epoch": 0.9182209469153515,
      "grad_norm": 2.9500696659088135,
      "learning_rate": 9.191920261221452e-05,
      "loss": 1.6248,
      "step": 1280
    },
    {
      "epoch": 0.9253945480631277,
      "grad_norm": 3.2319960594177246,
      "learning_rate": 9.17959350905599e-05,
      "loss": 1.5241,
      "step": 1290
    },
    {
      "epoch": 0.9325681492109039,
      "grad_norm": 2.7698535919189453,
      "learning_rate": 9.16718184519755e-05,
      "loss": 1.4731,
      "step": 1300
    },
    {
      "epoch": 0.93974175035868,
      "grad_norm": 2.8813068866729736,
      "learning_rate": 9.154685521798736e-05,
      "loss": 1.508,
      "step": 1310
    },
    {
      "epoch": 0.9469153515064562,
      "grad_norm": 2.3836722373962402,
      "learning_rate": 9.142104792732078e-05,
      "loss": 1.4973,
      "step": 1320
    },
    {
      "epoch": 0.9540889526542324,
      "grad_norm": 2.597491502761841,
      "learning_rate": 9.129439913584869e-05,
      "loss": 1.4296,
      "step": 1330
    },
    {
      "epoch": 0.9612625538020086,
      "grad_norm": 2.7958295345306396,
      "learning_rate": 9.116691141653982e-05,
      "loss": 1.5156,
      "step": 1340
    },
    {
      "epoch": 0.9684361549497847,
      "grad_norm": 2.7601420879364014,
      "learning_rate": 9.103858735940634e-05,
      "loss": 1.4158,
      "step": 1350
    },
    {
      "epoch": 0.975609756097561,
      "grad_norm": 2.7396602630615234,
      "learning_rate": 9.090942957145129e-05,
      "loss": 1.4596,
      "step": 1360
    },
    {
      "epoch": 0.9827833572453372,
      "grad_norm": 3.0356502532958984,
      "learning_rate": 9.077944067661559e-05,
      "loss": 1.4588,
      "step": 1370
    },
    {
      "epoch": 0.9899569583931134,
      "grad_norm": 2.504120111465454,
      "learning_rate": 9.064862331572471e-05,
      "loss": 1.5496,
      "step": 1380
    },
    {
      "epoch": 0.9971305595408895,
      "grad_norm": 3.0737967491149902,
      "learning_rate": 9.051698014643512e-05,
      "loss": 1.5302,
      "step": 1390
    },
    {
      "epoch": 1.0043041606886658,
      "grad_norm": 2.706622838973999,
      "learning_rate": 9.038451384318018e-05,
      "loss": 1.4702,
      "step": 1400
    },
    {
      "epoch": 1.011477761836442,
      "grad_norm": 2.450814723968506,
      "learning_rate": 9.025122709711589e-05,
      "loss": 1.4137,
      "step": 1410
    },
    {
      "epoch": 1.018651362984218,
      "grad_norm": 2.854745864868164,
      "learning_rate": 9.011712261606615e-05,
      "loss": 1.3693,
      "step": 1420
    },
    {
      "epoch": 1.0258249641319943,
      "grad_norm": 2.8439221382141113,
      "learning_rate": 8.998220312446779e-05,
      "loss": 1.4558,
      "step": 1430
    },
    {
      "epoch": 1.0329985652797704,
      "grad_norm": 2.630324363708496,
      "learning_rate": 8.984647136331524e-05,
      "loss": 1.4154,
      "step": 1440
    },
    {
      "epoch": 1.0401721664275467,
      "grad_norm": 3.568255662918091,
      "learning_rate": 8.970993009010475e-05,
      "loss": 1.4041,
      "step": 1450
    },
    {
      "epoch": 1.0473457675753228,
      "grad_norm": 2.9109206199645996,
      "learning_rate": 8.957258207877856e-05,
      "loss": 1.4019,
      "step": 1460
    },
    {
      "epoch": 1.054519368723099,
      "grad_norm": 3.216482639312744,
      "learning_rate": 8.94344301196683e-05,
      "loss": 1.4665,
      "step": 1470
    },
    {
      "epoch": 1.0616929698708752,
      "grad_norm": 2.9954001903533936,
      "learning_rate": 8.929547701943848e-05,
      "loss": 1.4807,
      "step": 1480
    },
    {
      "epoch": 1.0688665710186513,
      "grad_norm": 2.5991709232330322,
      "learning_rate": 8.915572560102941e-05,
      "loss": 1.4193,
      "step": 1490
    },
    {
      "epoch": 1.0760401721664274,
      "grad_norm": 2.9832918643951416,
      "learning_rate": 8.901517870359985e-05,
      "loss": 1.3814,
      "step": 1500
    },
    {
      "epoch": 1.0832137733142038,
      "grad_norm": 2.6296141147613525,
      "learning_rate": 8.887383918246936e-05,
      "loss": 1.3956,
      "step": 1510
    },
    {
      "epoch": 1.0903873744619799,
      "grad_norm": 3.2225430011749268,
      "learning_rate": 8.873170990906021e-05,
      "loss": 1.4685,
      "step": 1520
    },
    {
      "epoch": 1.0975609756097562,
      "grad_norm": 2.8110101222991943,
      "learning_rate": 8.858879377083915e-05,
      "loss": 1.3601,
      "step": 1530
    },
    {
      "epoch": 1.1047345767575323,
      "grad_norm": 2.992262125015259,
      "learning_rate": 8.844509367125868e-05,
      "loss": 1.3712,
      "step": 1540
    },
    {
      "epoch": 1.1119081779053084,
      "grad_norm": 3.213188886642456,
      "learning_rate": 8.830061252969811e-05,
      "loss": 1.3457,
      "step": 1550
    },
    {
      "epoch": 1.1190817790530847,
      "grad_norm": 2.5247933864593506,
      "learning_rate": 8.81553532814042e-05,
      "loss": 1.4234,
      "step": 1560
    },
    {
      "epoch": 1.1262553802008608,
      "grad_norm": 2.7491555213928223,
      "learning_rate": 8.800931887743153e-05,
      "loss": 1.4426,
      "step": 1570
    },
    {
      "epoch": 1.133428981348637,
      "grad_norm": 2.6754984855651855,
      "learning_rate": 8.786251228458264e-05,
      "loss": 1.3909,
      "step": 1580
    },
    {
      "epoch": 1.1406025824964132,
      "grad_norm": 2.771118402481079,
      "learning_rate": 8.771493648534765e-05,
      "loss": 1.421,
      "step": 1590
    },
    {
      "epoch": 1.1477761836441893,
      "grad_norm": 2.7445571422576904,
      "learning_rate": 8.756659447784368e-05,
      "loss": 1.3443,
      "step": 1600
    },
    {
      "epoch": 1.1549497847919656,
      "grad_norm": 2.0330324172973633,
      "learning_rate": 8.741748927575398e-05,
      "loss": 1.4666,
      "step": 1610
    },
    {
      "epoch": 1.1621233859397417,
      "grad_norm": 2.937870979309082,
      "learning_rate": 8.726762390826674e-05,
      "loss": 1.4064,
      "step": 1620
    },
    {
      "epoch": 1.169296987087518,
      "grad_norm": 3.222107410430908,
      "learning_rate": 8.711700142001344e-05,
      "loss": 1.4408,
      "step": 1630
    },
    {
      "epoch": 1.1764705882352942,
      "grad_norm": 2.576213836669922,
      "learning_rate": 8.696562487100709e-05,
      "loss": 1.4326,
      "step": 1640
    },
    {
      "epoch": 1.1836441893830703,
      "grad_norm": 3.006030321121216,
      "learning_rate": 8.681349733658002e-05,
      "loss": 1.3936,
      "step": 1650
    },
    {
      "epoch": 1.1908177905308466,
      "grad_norm": 2.516413927078247,
      "learning_rate": 8.66606219073214e-05,
      "loss": 1.3851,
      "step": 1660
    },
    {
      "epoch": 1.1979913916786227,
      "grad_norm": 2.868436813354492,
      "learning_rate": 8.650700168901452e-05,
      "loss": 1.4678,
      "step": 1670
    },
    {
      "epoch": 1.2051649928263988,
      "grad_norm": 2.93217396736145,
      "learning_rate": 8.635263980257357e-05,
      "loss": 1.45,
      "step": 1680
    },
    {
      "epoch": 1.212338593974175,
      "grad_norm": 2.3920679092407227,
      "learning_rate": 8.619753938398033e-05,
      "loss": 1.4254,
      "step": 1690
    },
    {
      "epoch": 1.2195121951219512,
      "grad_norm": 2.9425272941589355,
      "learning_rate": 8.604170358422045e-05,
      "loss": 1.3804,
      "step": 1700
    },
    {
      "epoch": 1.2266857962697273,
      "grad_norm": 2.6863324642181396,
      "learning_rate": 8.588513556921939e-05,
      "loss": 1.4965,
      "step": 1710
    },
    {
      "epoch": 1.2338593974175036,
      "grad_norm": 2.813613176345825,
      "learning_rate": 8.57278385197782e-05,
      "loss": 1.3971,
      "step": 1720
    },
    {
      "epoch": 1.2410329985652797,
      "grad_norm": 3.194021463394165,
      "learning_rate": 8.556981563150874e-05,
      "loss": 1.4072,
      "step": 1730
    },
    {
      "epoch": 1.248206599713056,
      "grad_norm": 3.697157621383667,
      "learning_rate": 8.54110701147689e-05,
      "loss": 1.4168,
      "step": 1740
    },
    {
      "epoch": 1.2553802008608321,
      "grad_norm": 2.947761058807373,
      "learning_rate": 8.525160519459735e-05,
      "loss": 1.3604,
      "step": 1750
    },
    {
      "epoch": 1.2625538020086085,
      "grad_norm": 3.3520236015319824,
      "learning_rate": 8.509142411064792e-05,
      "loss": 1.436,
      "step": 1760
    },
    {
      "epoch": 1.2697274031563845,
      "grad_norm": 3.4812674522399902,
      "learning_rate": 8.493053011712397e-05,
      "loss": 1.4516,
      "step": 1770
    },
    {
      "epoch": 1.2769010043041606,
      "grad_norm": 3.102189064025879,
      "learning_rate": 8.47689264827121e-05,
      "loss": 1.4422,
      "step": 1780
    },
    {
      "epoch": 1.284074605451937,
      "grad_norm": 3.046910047531128,
      "learning_rate": 8.460661649051583e-05,
      "loss": 1.4016,
      "step": 1790
    },
    {
      "epoch": 1.291248206599713,
      "grad_norm": 2.706263780593872,
      "learning_rate": 8.44436034379889e-05,
      "loss": 1.4217,
      "step": 1800
    },
    {
      "epoch": 1.2984218077474892,
      "grad_norm": 3.7472827434539795,
      "learning_rate": 8.427989063686828e-05,
      "loss": 1.3708,
      "step": 1810
    },
    {
      "epoch": 1.3055954088952655,
      "grad_norm": 2.804635763168335,
      "learning_rate": 8.411548141310682e-05,
      "loss": 1.3936,
      "step": 1820
    },
    {
      "epoch": 1.3127690100430416,
      "grad_norm": 2.8117332458496094,
      "learning_rate": 8.395037910680581e-05,
      "loss": 1.3816,
      "step": 1830
    },
    {
      "epoch": 1.3199426111908177,
      "grad_norm": 3.1736600399017334,
      "learning_rate": 8.378458707214701e-05,
      "loss": 1.3576,
      "step": 1840
    },
    {
      "epoch": 1.327116212338594,
      "grad_norm": 3.014254093170166,
      "learning_rate": 8.361810867732453e-05,
      "loss": 1.3956,
      "step": 1850
    },
    {
      "epoch": 1.33428981348637,
      "grad_norm": 2.9291393756866455,
      "learning_rate": 8.345094730447649e-05,
      "loss": 1.413,
      "step": 1860
    },
    {
      "epoch": 1.3414634146341464,
      "grad_norm": 2.982708215713501,
      "learning_rate": 8.328310634961616e-05,
      "loss": 1.3483,
      "step": 1870
    },
    {
      "epoch": 1.3486370157819225,
      "grad_norm": 3.149256467819214,
      "learning_rate": 8.311458922256309e-05,
      "loss": 1.4716,
      "step": 1880
    },
    {
      "epoch": 1.3558106169296988,
      "grad_norm": 2.536743640899658,
      "learning_rate": 8.294539934687379e-05,
      "loss": 1.4338,
      "step": 1890
    },
    {
      "epoch": 1.362984218077475,
      "grad_norm": 2.7966835498809814,
      "learning_rate": 8.27755401597722e-05,
      "loss": 1.4778,
      "step": 1900
    },
    {
      "epoch": 1.370157819225251,
      "grad_norm": 3.369292736053467,
      "learning_rate": 8.26050151120798e-05,
      "loss": 1.4315,
      "step": 1910
    },
    {
      "epoch": 1.3773314203730274,
      "grad_norm": 2.9208297729492188,
      "learning_rate": 8.243382766814556e-05,
      "loss": 1.4664,
      "step": 1920
    },
    {
      "epoch": 1.3845050215208035,
      "grad_norm": 2.6625313758850098,
      "learning_rate": 8.226198130577556e-05,
      "loss": 1.3909,
      "step": 1930
    },
    {
      "epoch": 1.3916786226685796,
      "grad_norm": 3.2035138607025146,
      "learning_rate": 8.20894795161623e-05,
      "loss": 1.3892,
      "step": 1940
    },
    {
      "epoch": 1.3988522238163559,
      "grad_norm": 3.4210939407348633,
      "learning_rate": 8.191632580381383e-05,
      "loss": 1.4168,
      "step": 1950
    },
    {
      "epoch": 1.406025824964132,
      "grad_norm": 2.8167715072631836,
      "learning_rate": 8.174252368648249e-05,
      "loss": 1.3799,
      "step": 1960
    },
    {
      "epoch": 1.413199426111908,
      "grad_norm": 3.1416828632354736,
      "learning_rate": 8.156807669509345e-05,
      "loss": 1.3985,
      "step": 1970
    },
    {
      "epoch": 1.4203730272596844,
      "grad_norm": 3.264753818511963,
      "learning_rate": 8.139298837367304e-05,
      "loss": 1.2768,
      "step": 1980
    },
    {
      "epoch": 1.4275466284074605,
      "grad_norm": 3.6079304218292236,
      "learning_rate": 8.121726227927671e-05,
      "loss": 1.4704,
      "step": 1990
    },
    {
      "epoch": 1.4347202295552366,
      "grad_norm": 2.9245381355285645,
      "learning_rate": 8.10409019819167e-05,
      "loss": 1.3527,
      "step": 2000
    },
    {
      "epoch": 1.441893830703013,
      "grad_norm": 3.107102632522583,
      "learning_rate": 8.086391106448965e-05,
      "loss": 1.5086,
      "step": 2010
    },
    {
      "epoch": 1.4490674318507892,
      "grad_norm": 3.0396792888641357,
      "learning_rate": 8.068629312270371e-05,
      "loss": 1.3216,
      "step": 2020
    },
    {
      "epoch": 1.4562410329985653,
      "grad_norm": 2.908071756362915,
      "learning_rate": 8.050805176500553e-05,
      "loss": 1.3853,
      "step": 2030
    },
    {
      "epoch": 1.4634146341463414,
      "grad_norm": 3.8463714122772217,
      "learning_rate": 8.03291906125069e-05,
      "loss": 1.5165,
      "step": 2040
    },
    {
      "epoch": 1.4705882352941178,
      "grad_norm": 2.9803552627563477,
      "learning_rate": 8.014971329891125e-05,
      "loss": 1.4643,
      "step": 2050
    },
    {
      "epoch": 1.4777618364418939,
      "grad_norm": 3.294945478439331,
      "learning_rate": 7.996962347043982e-05,
      "loss": 1.4255,
      "step": 2060
    },
    {
      "epoch": 1.48493543758967,
      "grad_norm": 3.236440896987915,
      "learning_rate": 7.978892478575752e-05,
      "loss": 1.3614,
      "step": 2070
    },
    {
      "epoch": 1.4921090387374463,
      "grad_norm": 3.046417713165283,
      "learning_rate": 7.96076209158987e-05,
      "loss": 1.358,
      "step": 2080
    },
    {
      "epoch": 1.4992826398852224,
      "grad_norm": 3.108543634414673,
      "learning_rate": 7.942571554419251e-05,
      "loss": 1.4263,
      "step": 2090
    },
    {
      "epoch": 1.5064562410329985,
      "grad_norm": 3.290055274963379,
      "learning_rate": 7.9243212366188e-05,
      "loss": 1.3626,
      "step": 2100
    },
    {
      "epoch": 1.5136298421807748,
      "grad_norm": 2.789551258087158,
      "learning_rate": 7.906011508957923e-05,
      "loss": 1.3638,
      "step": 2110
    },
    {
      "epoch": 1.5208034433285509,
      "grad_norm": 3.1973395347595215,
      "learning_rate": 7.887642743412978e-05,
      "loss": 1.3542,
      "step": 2120
    },
    {
      "epoch": 1.527977044476327,
      "grad_norm": 3.411090135574341,
      "learning_rate": 7.86921531315972e-05,
      "loss": 1.3988,
      "step": 2130
    },
    {
      "epoch": 1.5351506456241033,
      "grad_norm": 3.3994874954223633,
      "learning_rate": 7.850729592565734e-05,
      "loss": 1.3966,
      "step": 2140
    },
    {
      "epoch": 1.5423242467718796,
      "grad_norm": 3.05942702293396,
      "learning_rate": 7.832185957182805e-05,
      "loss": 1.359,
      "step": 2150
    },
    {
      "epoch": 1.5494978479196555,
      "grad_norm": 3.1277618408203125,
      "learning_rate": 7.813584783739314e-05,
      "loss": 1.3309,
      "step": 2160
    },
    {
      "epoch": 1.5566714490674318,
      "grad_norm": 3.248868227005005,
      "learning_rate": 7.794926450132565e-05,
      "loss": 1.3585,
      "step": 2170
    },
    {
      "epoch": 1.5638450502152081,
      "grad_norm": 3.435115098953247,
      "learning_rate": 7.776211335421117e-05,
      "loss": 1.4032,
      "step": 2180
    },
    {
      "epoch": 1.5710186513629842,
      "grad_norm": 3.3702077865600586,
      "learning_rate": 7.757439819817083e-05,
      "loss": 1.3649,
      "step": 2190
    },
    {
      "epoch": 1.5781922525107603,
      "grad_norm": 2.8304779529571533,
      "learning_rate": 7.738612284678404e-05,
      "loss": 1.3912,
      "step": 2200
    },
    {
      "epoch": 1.5853658536585367,
      "grad_norm": 3.3419954776763916,
      "learning_rate": 7.719729112501098e-05,
      "loss": 1.3565,
      "step": 2210
    },
    {
      "epoch": 1.5925394548063128,
      "grad_norm": 3.6977500915527344,
      "learning_rate": 7.700790686911495e-05,
      "loss": 1.4151,
      "step": 2220
    },
    {
      "epoch": 1.5997130559540889,
      "grad_norm": 2.872711181640625,
      "learning_rate": 7.681797392658439e-05,
      "loss": 1.4061,
      "step": 2230
    },
    {
      "epoch": 1.6068866571018652,
      "grad_norm": 4.087683200836182,
      "learning_rate": 7.662749615605482e-05,
      "loss": 1.4289,
      "step": 2240
    },
    {
      "epoch": 1.6140602582496413,
      "grad_norm": 3.6216225624084473,
      "learning_rate": 7.643647742723023e-05,
      "loss": 1.4075,
      "step": 2250
    },
    {
      "epoch": 1.6212338593974174,
      "grad_norm": 3.373255729675293,
      "learning_rate": 7.624492162080472e-05,
      "loss": 1.3865,
      "step": 2260
    },
    {
      "epoch": 1.6284074605451937,
      "grad_norm": 2.9744205474853516,
      "learning_rate": 7.605283262838345e-05,
      "loss": 1.3998,
      "step": 2270
    },
    {
      "epoch": 1.63558106169297,
      "grad_norm": 3.4906039237976074,
      "learning_rate": 7.586021435240374e-05,
      "loss": 1.4255,
      "step": 2280
    },
    {
      "epoch": 1.642754662840746,
      "grad_norm": 3.4570727348327637,
      "learning_rate": 7.566707070605564e-05,
      "loss": 1.3811,
      "step": 2290
    },
    {
      "epoch": 1.6499282639885222,
      "grad_norm": 2.707740068435669,
      "learning_rate": 7.54734056132026e-05,
      "loss": 1.3599,
      "step": 2300
    },
    {
      "epoch": 1.6571018651362985,
      "grad_norm": 3.341099262237549,
      "learning_rate": 7.527922300830156e-05,
      "loss": 1.3391,
      "step": 2310
    },
    {
      "epoch": 1.6642754662840746,
      "grad_norm": 3.0339484214782715,
      "learning_rate": 7.508452683632321e-05,
      "loss": 1.4098,
      "step": 2320
    },
    {
      "epoch": 1.6714490674318507,
      "grad_norm": 3.38130784034729,
      "learning_rate": 7.48893210526717e-05,
      "loss": 1.4084,
      "step": 2330
    },
    {
      "epoch": 1.678622668579627,
      "grad_norm": 2.49716854095459,
      "learning_rate": 7.469360962310438e-05,
      "loss": 1.3733,
      "step": 2340
    },
    {
      "epoch": 1.6857962697274032,
      "grad_norm": 3.2822554111480713,
      "learning_rate": 7.449739652365111e-05,
      "loss": 1.3805,
      "step": 2350
    },
    {
      "epoch": 1.6929698708751793,
      "grad_norm": 3.403757333755493,
      "learning_rate": 7.430068574053368e-05,
      "loss": 1.402,
      "step": 2360
    },
    {
      "epoch": 1.7001434720229556,
      "grad_norm": 3.28905987739563,
      "learning_rate": 7.410348127008462e-05,
      "loss": 1.4297,
      "step": 2370
    },
    {
      "epoch": 1.7073170731707317,
      "grad_norm": 3.123530149459839,
      "learning_rate": 7.390578711866611e-05,
      "loss": 1.3247,
      "step": 2380
    },
    {
      "epoch": 1.7144906743185078,
      "grad_norm": 3.5005815029144287,
      "learning_rate": 7.370760730258862e-05,
      "loss": 1.3537,
      "step": 2390
    },
    {
      "epoch": 1.721664275466284,
      "grad_norm": 2.5729856491088867,
      "learning_rate": 7.350894584802928e-05,
      "loss": 1.4196,
      "step": 2400
    },
    {
      "epoch": 1.7288378766140604,
      "grad_norm": 3.524106979370117,
      "learning_rate": 7.330980679094999e-05,
      "loss": 1.2982,
      "step": 2410
    },
    {
      "epoch": 1.7360114777618363,
      "grad_norm": 3.5481936931610107,
      "learning_rate": 7.311019417701566e-05,
      "loss": 1.448,
      "step": 2420
    },
    {
      "epoch": 1.7431850789096126,
      "grad_norm": 3.7003495693206787,
      "learning_rate": 7.291011206151175e-05,
      "loss": 1.3616,
      "step": 2430
    },
    {
      "epoch": 1.750358680057389,
      "grad_norm": 2.5912623405456543,
      "learning_rate": 7.270956450926209e-05,
      "loss": 1.3908,
      "step": 2440
    },
    {
      "epoch": 1.757532281205165,
      "grad_norm": 2.92048978805542,
      "learning_rate": 7.250855559454615e-05,
      "loss": 1.3663,
      "step": 2450
    },
    {
      "epoch": 1.7647058823529411,
      "grad_norm": 3.0768134593963623,
      "learning_rate": 7.23070894010164e-05,
      "loss": 1.3285,
      "step": 2460
    },
    {
      "epoch": 1.7718794835007174,
      "grad_norm": 3.272902011871338,
      "learning_rate": 7.210517002161525e-05,
      "loss": 1.3793,
      "step": 2470
    },
    {
      "epoch": 1.7790530846484935,
      "grad_norm": 2.9417531490325928,
      "learning_rate": 7.190280155849196e-05,
      "loss": 1.3591,
      "step": 2480
    },
    {
      "epoch": 1.7862266857962696,
      "grad_norm": 2.990832805633545,
      "learning_rate": 7.169998812291923e-05,
      "loss": 1.4266,
      "step": 2490
    },
    {
      "epoch": 1.793400286944046,
      "grad_norm": 3.4651167392730713,
      "learning_rate": 7.149673383520977e-05,
      "loss": 1.3411,
      "step": 2500
    },
    {
      "epoch": 1.800573888091822,
      "grad_norm": 3.6041595935821533,
      "learning_rate": 7.129304282463252e-05,
      "loss": 1.3263,
      "step": 2510
    },
    {
      "epoch": 1.8077474892395982,
      "grad_norm": 3.330446720123291,
      "learning_rate": 7.10889192293288e-05,
      "loss": 1.4045,
      "step": 2520
    },
    {
      "epoch": 1.8149210903873745,
      "grad_norm": 2.663902997970581,
      "learning_rate": 7.088436719622818e-05,
      "loss": 1.3622,
      "step": 2530
    },
    {
      "epoch": 1.8220946915351508,
      "grad_norm": 3.9036219120025635,
      "learning_rate": 7.067939088096436e-05,
      "loss": 1.3495,
      "step": 2540
    },
    {
      "epoch": 1.8292682926829267,
      "grad_norm": 3.1993582248687744,
      "learning_rate": 7.047399444779055e-05,
      "loss": 1.4381,
      "step": 2550
    },
    {
      "epoch": 1.836441893830703,
      "grad_norm": 3.381117820739746,
      "learning_rate": 7.026818206949511e-05,
      "loss": 1.351,
      "step": 2560
    },
    {
      "epoch": 1.8436154949784793,
      "grad_norm": 3.2552223205566406,
      "learning_rate": 7.006195792731654e-05,
      "loss": 1.4485,
      "step": 2570
    },
    {
      "epoch": 1.8507890961262554,
      "grad_norm": 3.2211456298828125,
      "learning_rate": 6.98553262108587e-05,
      "loss": 1.3264,
      "step": 2580
    },
    {
      "epoch": 1.8579626972740315,
      "grad_norm": 2.6628236770629883,
      "learning_rate": 6.964829111800563e-05,
      "loss": 1.4084,
      "step": 2590
    },
    {
      "epoch": 1.8651362984218078,
      "grad_norm": 2.81557035446167,
      "learning_rate": 6.944085685483628e-05,
      "loss": 1.3478,
      "step": 2600
    },
    {
      "epoch": 1.872309899569584,
      "grad_norm": 3.8214659690856934,
      "learning_rate": 6.923302763553903e-05,
      "loss": 1.3871,
      "step": 2610
    },
    {
      "epoch": 1.87948350071736,
      "grad_norm": 3.6332433223724365,
      "learning_rate": 6.902480768232611e-05,
      "loss": 1.3745,
      "step": 2620
    },
    {
      "epoch": 1.8866571018651364,
      "grad_norm": 3.1503844261169434,
      "learning_rate": 6.881620122534784e-05,
      "loss": 1.4052,
      "step": 2630
    },
    {
      "epoch": 1.8938307030129125,
      "grad_norm": 3.807201385498047,
      "learning_rate": 6.860721250260665e-05,
      "loss": 1.3502,
      "step": 2640
    },
    {
      "epoch": 1.9010043041606886,
      "grad_norm": 3.2701632976531982,
      "learning_rate": 6.839784575987095e-05,
      "loss": 1.3655,
      "step": 2650
    },
    {
      "epoch": 1.9081779053084649,
      "grad_norm": 2.6508350372314453,
      "learning_rate": 6.818810525058902e-05,
      "loss": 1.3172,
      "step": 2660
    },
    {
      "epoch": 1.9153515064562412,
      "grad_norm": 2.6825592517852783,
      "learning_rate": 6.79779952358024e-05,
      "loss": 1.3998,
      "step": 2670
    },
    {
      "epoch": 1.922525107604017,
      "grad_norm": 2.888899564743042,
      "learning_rate": 6.776751998405948e-05,
      "loss": 1.4155,
      "step": 2680
    },
    {
      "epoch": 1.9296987087517934,
      "grad_norm": 3.2697620391845703,
      "learning_rate": 6.755668377132869e-05,
      "loss": 1.353,
      "step": 2690
    },
    {
      "epoch": 1.9368723098995697,
      "grad_norm": 3.077801465988159,
      "learning_rate": 6.734549088091168e-05,
      "loss": 1.3086,
      "step": 2700
    },
    {
      "epoch": 1.9440459110473458,
      "grad_norm": 3.0547358989715576,
      "learning_rate": 6.71339456033563e-05,
      "loss": 1.4239,
      "step": 2710
    },
    {
      "epoch": 1.951219512195122,
      "grad_norm": 4.405411243438721,
      "learning_rate": 6.69220522363694e-05,
      "loss": 1.2783,
      "step": 2720
    },
    {
      "epoch": 1.9583931133428982,
      "grad_norm": 3.24776291847229,
      "learning_rate": 6.67098150847295e-05,
      "loss": 1.3304,
      "step": 2730
    },
    {
      "epoch": 1.9655667144906743,
      "grad_norm": 2.7618408203125,
      "learning_rate": 6.649723846019945e-05,
      "loss": 1.3092,
      "step": 2740
    },
    {
      "epoch": 1.9727403156384504,
      "grad_norm": 2.849555730819702,
      "learning_rate": 6.628432668143871e-05,
      "loss": 1.3157,
      "step": 2750
    },
    {
      "epoch": 1.9799139167862267,
      "grad_norm": 3.726445198059082,
      "learning_rate": 6.607108407391567e-05,
      "loss": 1.4633,
      "step": 2760
    },
    {
      "epoch": 1.9870875179340028,
      "grad_norm": 3.1400630474090576,
      "learning_rate": 6.585751496981973e-05,
      "loss": 1.3722,
      "step": 2770
    },
    {
      "epoch": 1.994261119081779,
      "grad_norm": 2.6259658336639404,
      "learning_rate": 6.56436237079734e-05,
      "loss": 1.3222,
      "step": 2780
    },
    {
      "epoch": 2.0014347202295553,
      "grad_norm": 2.4189863204956055,
      "learning_rate": 6.542941463374397e-05,
      "loss": 1.367,
      "step": 2790
    },
    {
      "epoch": 2.0086083213773316,
      "grad_norm": 3.877425193786621,
      "learning_rate": 6.521489209895546e-05,
      "loss": 1.3441,
      "step": 2800
    },
    {
      "epoch": 2.0157819225251075,
      "grad_norm": 3.556180953979492,
      "learning_rate": 6.500006046179995e-05,
      "loss": 1.2791,
      "step": 2810
    },
    {
      "epoch": 2.022955523672884,
      "grad_norm": 3.314121961593628,
      "learning_rate": 6.478492408674928e-05,
      "loss": 1.2999,
      "step": 2820
    },
    {
      "epoch": 2.03012912482066,
      "grad_norm": 3.394306182861328,
      "learning_rate": 6.456948734446624e-05,
      "loss": 1.3372,
      "step": 2830
    },
    {
      "epoch": 2.037302725968436,
      "grad_norm": 3.304502487182617,
      "learning_rate": 6.43537546117158e-05,
      "loss": 1.3054,
      "step": 2840
    },
    {
      "epoch": 2.0444763271162123,
      "grad_norm": 3.567431926727295,
      "learning_rate": 6.413773027127623e-05,
      "loss": 1.3559,
      "step": 2850
    },
    {
      "epoch": 2.0516499282639886,
      "grad_norm": 3.812232732772827,
      "learning_rate": 6.392141871185004e-05,
      "loss": 1.2438,
      "step": 2860
    },
    {
      "epoch": 2.0588235294117645,
      "grad_norm": 3.198052167892456,
      "learning_rate": 6.370482432797476e-05,
      "loss": 1.3099,
      "step": 2870
    },
    {
      "epoch": 2.065997130559541,
      "grad_norm": 3.1621880531311035,
      "learning_rate": 6.348795151993379e-05,
      "loss": 1.3112,
      "step": 2880
    },
    {
      "epoch": 2.073170731707317,
      "grad_norm": 3.543856143951416,
      "learning_rate": 6.327080469366693e-05,
      "loss": 1.3213,
      "step": 2890
    },
    {
      "epoch": 2.0803443328550935,
      "grad_norm": 3.114154100418091,
      "learning_rate": 6.305338826068083e-05,
      "loss": 1.2127,
      "step": 2900
    },
    {
      "epoch": 2.0875179340028693,
      "grad_norm": 3.667628049850464,
      "learning_rate": 6.283570663795943e-05,
      "loss": 1.2633,
      "step": 2910
    },
    {
      "epoch": 2.0946915351506457,
      "grad_norm": 3.7378809452056885,
      "learning_rate": 6.261776424787417e-05,
      "loss": 1.3171,
      "step": 2920
    },
    {
      "epoch": 2.101865136298422,
      "grad_norm": 3.431722402572632,
      "learning_rate": 6.239956551809429e-05,
      "loss": 1.2869,
      "step": 2930
    },
    {
      "epoch": 2.109038737446198,
      "grad_norm": 3.2655227184295654,
      "learning_rate": 6.218111488149666e-05,
      "loss": 1.2808,
      "step": 2940
    },
    {
      "epoch": 2.116212338593974,
      "grad_norm": 3.337392568588257,
      "learning_rate": 6.196241677607586e-05,
      "loss": 1.259,
      "step": 2950
    },
    {
      "epoch": 2.1233859397417505,
      "grad_norm": 2.9470276832580566,
      "learning_rate": 6.174347564485404e-05,
      "loss": 1.2854,
      "step": 2960
    },
    {
      "epoch": 2.1305595408895264,
      "grad_norm": 3.38883900642395,
      "learning_rate": 6.152429593579051e-05,
      "loss": 1.3408,
      "step": 2970
    },
    {
      "epoch": 2.1377331420373027,
      "grad_norm": 2.5959579944610596,
      "learning_rate": 6.130488210169158e-05,
      "loss": 1.2913,
      "step": 2980
    },
    {
      "epoch": 2.144906743185079,
      "grad_norm": 3.1387038230895996,
      "learning_rate": 6.108523860011996e-05,
      "loss": 1.3305,
      "step": 2990
    },
    {
      "epoch": 2.152080344332855,
      "grad_norm": 3.9672582149505615,
      "learning_rate": 6.086536989330417e-05,
      "loss": 1.1916,
      "step": 3000
    },
    {
      "epoch": 2.159253945480631,
      "grad_norm": 2.960664749145508,
      "learning_rate": 6.0645280448048044e-05,
      "loss": 1.2487,
      "step": 3010
    },
    {
      "epoch": 2.1664275466284075,
      "grad_norm": 3.3311333656311035,
      "learning_rate": 6.042497473563984e-05,
      "loss": 1.3334,
      "step": 3020
    },
    {
      "epoch": 2.173601147776184,
      "grad_norm": 3.087369918823242,
      "learning_rate": 6.020445723176143e-05,
      "loss": 1.3048,
      "step": 3030
    },
    {
      "epoch": 2.1807747489239597,
      "grad_norm": 3.5566182136535645,
      "learning_rate": 5.9983732416397454e-05,
      "loss": 1.2784,
      "step": 3040
    },
    {
      "epoch": 2.187948350071736,
      "grad_norm": 3.884516716003418,
      "learning_rate": 5.9762804773744195e-05,
      "loss": 1.2419,
      "step": 3050
    },
    {
      "epoch": 2.1951219512195124,
      "grad_norm": 3.369093418121338,
      "learning_rate": 5.954167879211854e-05,
      "loss": 1.3498,
      "step": 3060
    },
    {
      "epoch": 2.2022955523672882,
      "grad_norm": 2.8262362480163574,
      "learning_rate": 5.932035896386683e-05,
      "loss": 1.2981,
      "step": 3070
    },
    {
      "epoch": 2.2094691535150646,
      "grad_norm": 3.3062775135040283,
      "learning_rate": 5.909884978527348e-05,
      "loss": 1.351,
      "step": 3080
    },
    {
      "epoch": 2.216642754662841,
      "grad_norm": 3.8415563106536865,
      "learning_rate": 5.887715575646976e-05,
      "loss": 1.3259,
      "step": 3090
    },
    {
      "epoch": 2.2238163558106168,
      "grad_norm": 3.467235803604126,
      "learning_rate": 5.8655281381342285e-05,
      "loss": 1.2876,
      "step": 3100
    },
    {
      "epoch": 2.230989956958393,
      "grad_norm": 3.6804542541503906,
      "learning_rate": 5.8433231167441574e-05,
      "loss": 1.2777,
      "step": 3110
    },
    {
      "epoch": 2.2381635581061694,
      "grad_norm": 3.5402207374572754,
      "learning_rate": 5.821100962589041e-05,
      "loss": 1.2661,
      "step": 3120
    },
    {
      "epoch": 2.2453371592539453,
      "grad_norm": 2.9505181312561035,
      "learning_rate": 5.798862127129228e-05,
      "loss": 1.2255,
      "step": 3130
    },
    {
      "epoch": 2.2525107604017216,
      "grad_norm": 3.6349146366119385,
      "learning_rate": 5.776607062163952e-05,
      "loss": 1.2466,
      "step": 3140
    },
    {
      "epoch": 2.259684361549498,
      "grad_norm": 3.274406671524048,
      "learning_rate": 5.754336219822176e-05,
      "loss": 1.2905,
      "step": 3150
    },
    {
      "epoch": 2.266857962697274,
      "grad_norm": 3.559025764465332,
      "learning_rate": 5.7320500525533773e-05,
      "loss": 1.3255,
      "step": 3160
    },
    {
      "epoch": 2.27403156384505,
      "grad_norm": 3.007436513900757,
      "learning_rate": 5.709749013118381e-05,
      "loss": 1.2843,
      "step": 3170
    },
    {
      "epoch": 2.2812051649928264,
      "grad_norm": 3.4741482734680176,
      "learning_rate": 5.687433554580147e-05,
      "loss": 1.3417,
      "step": 3180
    },
    {
      "epoch": 2.2883787661406028,
      "grad_norm": 3.5898056030273438,
      "learning_rate": 5.665104130294574e-05,
      "loss": 1.3323,
      "step": 3190
    },
    {
      "epoch": 2.2955523672883786,
      "grad_norm": 3.084933042526245,
      "learning_rate": 5.6427611939012836e-05,
      "loss": 1.3496,
      "step": 3200
    },
    {
      "epoch": 2.302725968436155,
      "grad_norm": 3.611236572265625,
      "learning_rate": 5.620405199314407e-05,
      "loss": 1.3377,
      "step": 3210
    },
    {
      "epoch": 2.3098995695839313,
      "grad_norm": 3.1445798873901367,
      "learning_rate": 5.598036600713359e-05,
      "loss": 1.3478,
      "step": 3220
    },
    {
      "epoch": 2.317073170731707,
      "grad_norm": 3.4122509956359863,
      "learning_rate": 5.5756558525336243e-05,
      "loss": 1.2409,
      "step": 3230
    },
    {
      "epoch": 2.3242467718794835,
      "grad_norm": 4.17042350769043,
      "learning_rate": 5.553263409457504e-05,
      "loss": 1.2865,
      "step": 3240
    },
    {
      "epoch": 2.33142037302726,
      "grad_norm": 3.1521267890930176,
      "learning_rate": 5.5308597264049e-05,
      "loss": 1.2975,
      "step": 3250
    },
    {
      "epoch": 2.338593974175036,
      "grad_norm": 4.019785404205322,
      "learning_rate": 5.508445258524056e-05,
      "loss": 1.235,
      "step": 3260
    },
    {
      "epoch": 2.345767575322812,
      "grad_norm": 3.553499221801758,
      "learning_rate": 5.486020461182323e-05,
      "loss": 1.307,
      "step": 3270
    },
    {
      "epoch": 2.3529411764705883,
      "grad_norm": 3.8663229942321777,
      "learning_rate": 5.4635857899568986e-05,
      "loss": 1.3048,
      "step": 3280
    },
    {
      "epoch": 2.3601147776183646,
      "grad_norm": 4.172395706176758,
      "learning_rate": 5.441141700625579e-05,
      "loss": 1.34,
      "step": 3290
    },
    {
      "epoch": 2.3672883787661405,
      "grad_norm": 2.9193756580352783,
      "learning_rate": 5.4186886491574964e-05,
      "loss": 1.2351,
      "step": 3300
    },
    {
      "epoch": 2.374461979913917,
      "grad_norm": 3.89379620552063,
      "learning_rate": 5.396227091703856e-05,
      "loss": 1.2766,
      "step": 3310
    },
    {
      "epoch": 2.381635581061693,
      "grad_norm": 3.8077855110168457,
      "learning_rate": 5.373757484588664e-05,
      "loss": 1.2717,
      "step": 3320
    },
    {
      "epoch": 2.388809182209469,
      "grad_norm": 3.036344289779663,
      "learning_rate": 5.351280284299469e-05,
      "loss": 1.2833,
      "step": 3330
    },
    {
      "epoch": 2.3959827833572453,
      "grad_norm": 3.214207172393799,
      "learning_rate": 5.328795947478077e-05,
      "loss": 1.2761,
      "step": 3340
    },
    {
      "epoch": 2.4031563845050217,
      "grad_norm": 3.4590938091278076,
      "learning_rate": 5.306304930911278e-05,
      "loss": 1.2756,
      "step": 3350
    },
    {
      "epoch": 2.4103299856527975,
      "grad_norm": 2.8965559005737305,
      "learning_rate": 5.283807691521566e-05,
      "loss": 1.2737,
      "step": 3360
    },
    {
      "epoch": 2.417503586800574,
      "grad_norm": 3.5921216011047363,
      "learning_rate": 5.261304686357858e-05,
      "loss": 1.3394,
      "step": 3370
    },
    {
      "epoch": 2.42467718794835,
      "grad_norm": 2.753903865814209,
      "learning_rate": 5.2387963725862065e-05,
      "loss": 1.2277,
      "step": 3380
    },
    {
      "epoch": 2.431850789096126,
      "grad_norm": 4.113828659057617,
      "learning_rate": 5.2162832074805155e-05,
      "loss": 1.234,
      "step": 3390
    },
    {
      "epoch": 2.4390243902439024,
      "grad_norm": 3.8019139766693115,
      "learning_rate": 5.193765648413238e-05,
      "loss": 1.318,
      "step": 3400
    },
    {
      "epoch": 2.4461979913916787,
      "grad_norm": 3.320392370223999,
      "learning_rate": 5.1712441528461066e-05,
      "loss": 1.274,
      "step": 3410
    },
    {
      "epoch": 2.4533715925394546,
      "grad_norm": 3.026052474975586,
      "learning_rate": 5.148719178320818e-05,
      "loss": 1.2944,
      "step": 3420
    },
    {
      "epoch": 2.460545193687231,
      "grad_norm": 4.082263469696045,
      "learning_rate": 5.126191182449749e-05,
      "loss": 1.2537,
      "step": 3430
    },
    {
      "epoch": 2.4677187948350072,
      "grad_norm": 4.146859169006348,
      "learning_rate": 5.1036606229066584e-05,
      "loss": 1.265,
      "step": 3440
    },
    {
      "epoch": 2.4748923959827835,
      "grad_norm": 3.419199228286743,
      "learning_rate": 5.0811279574173875e-05,
      "loss": 1.2855,
      "step": 3450
    },
    {
      "epoch": 2.4820659971305594,
      "grad_norm": 3.288977861404419,
      "learning_rate": 5.05859364375056e-05,
      "loss": 1.2838,
      "step": 3460
    },
    {
      "epoch": 2.4892395982783357,
      "grad_norm": 2.4209389686584473,
      "learning_rate": 5.0360581397082894e-05,
      "loss": 1.3241,
      "step": 3470
    },
    {
      "epoch": 2.496413199426112,
      "grad_norm": 3.8145101070404053,
      "learning_rate": 5.013521903116861e-05,
      "loss": 1.2728,
      "step": 3480
    },
    {
      "epoch": 2.503586800573888,
      "grad_norm": 2.848010778427124,
      "learning_rate": 4.9909853918174566e-05,
      "loss": 1.2917,
      "step": 3490
    },
    {
      "epoch": 2.5107604017216643,
      "grad_norm": 3.115325450897217,
      "learning_rate": 4.968449063656829e-05,
      "loss": 1.265,
      "step": 3500
    },
    {
      "epoch": 2.5179340028694406,
      "grad_norm": 3.501776695251465,
      "learning_rate": 4.945913376478011e-05,
      "loss": 1.2014,
      "step": 3510
    },
    {
      "epoch": 2.525107604017217,
      "grad_norm": 3.5916106700897217,
      "learning_rate": 4.923378788111019e-05,
      "loss": 1.2548,
      "step": 3520
    },
    {
      "epoch": 2.5322812051649928,
      "grad_norm": 3.558990001678467,
      "learning_rate": 4.900845756363541e-05,
      "loss": 1.2755,
      "step": 3530
    },
    {
      "epoch": 2.539454806312769,
      "grad_norm": 3.8180360794067383,
      "learning_rate": 4.878314739011645e-05,
      "loss": 1.2305,
      "step": 3540
    },
    {
      "epoch": 2.5466284074605454,
      "grad_norm": 3.659475088119507,
      "learning_rate": 4.8557861937904707e-05,
      "loss": 1.3036,
      "step": 3550
    },
    {
      "epoch": 2.5538020086083213,
      "grad_norm": 3.4732258319854736,
      "learning_rate": 4.8332605783849364e-05,
      "loss": 1.2623,
      "step": 3560
    },
    {
      "epoch": 2.5609756097560976,
      "grad_norm": 3.660759925842285,
      "learning_rate": 4.810738350420442e-05,
      "loss": 1.2396,
      "step": 3570
    },
    {
      "epoch": 2.568149210903874,
      "grad_norm": 3.329643726348877,
      "learning_rate": 4.788219967453565e-05,
      "loss": 1.3243,
      "step": 3580
    },
    {
      "epoch": 2.57532281205165,
      "grad_norm": 3.546639919281006,
      "learning_rate": 4.7657058869627666e-05,
      "loss": 1.2336,
      "step": 3590
    },
    {
      "epoch": 2.582496413199426,
      "grad_norm": 2.957747220993042,
      "learning_rate": 4.743196566339108e-05,
      "loss": 1.1951,
      "step": 3600
    },
    {
      "epoch": 2.5896700143472025,
      "grad_norm": 3.5531489849090576,
      "learning_rate": 4.7206924628769426e-05,
      "loss": 1.2664,
      "step": 3610
    },
    {
      "epoch": 2.5968436154949783,
      "grad_norm": 4.079232692718506,
      "learning_rate": 4.6981940337646375e-05,
      "loss": 1.3581,
      "step": 3620
    },
    {
      "epoch": 2.6040172166427547,
      "grad_norm": 3.9741265773773193,
      "learning_rate": 4.675701736075277e-05,
      "loss": 1.2734,
      "step": 3630
    },
    {
      "epoch": 2.611190817790531,
      "grad_norm": 3.306027412414551,
      "learning_rate": 4.6532160267573824e-05,
      "loss": 1.3532,
      "step": 3640
    },
    {
      "epoch": 2.618364418938307,
      "grad_norm": 3.039512872695923,
      "learning_rate": 4.6307373626256306e-05,
      "loss": 1.282,
      "step": 3650
    },
    {
      "epoch": 2.625538020086083,
      "grad_norm": 3.3926005363464355,
      "learning_rate": 4.608266200351564e-05,
      "loss": 1.2494,
      "step": 3660
    },
    {
      "epoch": 2.6327116212338595,
      "grad_norm": 3.582507610321045,
      "learning_rate": 4.5858029964543264e-05,
      "loss": 1.2295,
      "step": 3670
    },
    {
      "epoch": 2.6398852223816354,
      "grad_norm": 4.5591583251953125,
      "learning_rate": 4.563348207291373e-05,
      "loss": 1.282,
      "step": 3680
    },
    {
      "epoch": 2.6470588235294117,
      "grad_norm": 3.7353901863098145,
      "learning_rate": 4.54090228904921e-05,
      "loss": 1.2944,
      "step": 3690
    },
    {
      "epoch": 2.654232424677188,
      "grad_norm": 3.1297683715820312,
      "learning_rate": 4.518465697734127e-05,
      "loss": 1.2814,
      "step": 3700
    },
    {
      "epoch": 2.661406025824964,
      "grad_norm": 4.015899181365967,
      "learning_rate": 4.496038889162928e-05,
      "loss": 1.3259,
      "step": 3710
    },
    {
      "epoch": 2.66857962697274,
      "grad_norm": 3.841937780380249,
      "learning_rate": 4.473622318953669e-05,
      "loss": 1.2426,
      "step": 3720
    },
    {
      "epoch": 2.6757532281205165,
      "grad_norm": 3.7986607551574707,
      "learning_rate": 4.451216442516413e-05,
      "loss": 1.3014,
      "step": 3730
    },
    {
      "epoch": 2.682926829268293,
      "grad_norm": 4.253111362457275,
      "learning_rate": 4.428821715043963e-05,
      "loss": 1.2547,
      "step": 3740
    },
    {
      "epoch": 2.6901004304160687,
      "grad_norm": 3.4247636795043945,
      "learning_rate": 4.406438591502631e-05,
      "loss": 1.2131,
      "step": 3750
    },
    {
      "epoch": 2.697274031563845,
      "grad_norm": 3.44580340385437,
      "learning_rate": 4.3840675266229774e-05,
      "loss": 1.2741,
      "step": 3760
    },
    {
      "epoch": 2.7044476327116214,
      "grad_norm": 2.982292890548706,
      "learning_rate": 4.361708974890585e-05,
      "loss": 1.2089,
      "step": 3770
    },
    {
      "epoch": 2.7116212338593977,
      "grad_norm": 3.634697437286377,
      "learning_rate": 4.339363390536825e-05,
      "loss": 1.2596,
      "step": 3780
    },
    {
      "epoch": 2.7187948350071736,
      "grad_norm": 3.224252939224243,
      "learning_rate": 4.3170312275296226e-05,
      "loss": 1.2691,
      "step": 3790
    },
    {
      "epoch": 2.72596843615495,
      "grad_norm": 3.310340642929077,
      "learning_rate": 4.2947129395642375e-05,
      "loss": 1.2218,
      "step": 3800
    },
    {
      "epoch": 2.733142037302726,
      "grad_norm": 4.601653575897217,
      "learning_rate": 4.27240898005405e-05,
      "loss": 1.3622,
      "step": 3810
    },
    {
      "epoch": 2.740315638450502,
      "grad_norm": 4.41817569732666,
      "learning_rate": 4.250119802121344e-05,
      "loss": 1.2492,
      "step": 3820
    },
    {
      "epoch": 2.7474892395982784,
      "grad_norm": 3.6491217613220215,
      "learning_rate": 4.227845858588109e-05,
      "loss": 1.1897,
      "step": 3830
    },
    {
      "epoch": 2.7546628407460547,
      "grad_norm": 4.088714599609375,
      "learning_rate": 4.2055876019668294e-05,
      "loss": 1.2012,
      "step": 3840
    },
    {
      "epoch": 2.7618364418938306,
      "grad_norm": 3.0806472301483154,
      "learning_rate": 4.1833454844512996e-05,
      "loss": 1.2449,
      "step": 3850
    },
    {
      "epoch": 2.769010043041607,
      "grad_norm": 3.484145402908325,
      "learning_rate": 4.16111995790744e-05,
      "loss": 1.2369,
      "step": 3860
    },
    {
      "epoch": 2.7761836441893832,
      "grad_norm": 3.648592948913574,
      "learning_rate": 4.138911473864108e-05,
      "loss": 1.3057,
      "step": 3870
    },
    {
      "epoch": 2.783357245337159,
      "grad_norm": 4.067199230194092,
      "learning_rate": 4.116720483503928e-05,
      "loss": 1.2371,
      "step": 3880
    },
    {
      "epoch": 2.7905308464849354,
      "grad_norm": 3.322282552719116,
      "learning_rate": 4.094547437654131e-05,
      "loss": 1.2219,
      "step": 3890
    },
    {
      "epoch": 2.7977044476327118,
      "grad_norm": 3.8566911220550537,
      "learning_rate": 4.072392786777389e-05,
      "loss": 1.2829,
      "step": 3900
    },
    {
      "epoch": 2.8048780487804876,
      "grad_norm": 3.603212356567383,
      "learning_rate": 4.0502569809626694e-05,
      "loss": 1.2267,
      "step": 3910
    },
    {
      "epoch": 2.812051649928264,
      "grad_norm": 3.6677005290985107,
      "learning_rate": 4.0281404699160796e-05,
      "loss": 1.1805,
      "step": 3920
    },
    {
      "epoch": 2.8192252510760403,
      "grad_norm": 3.311893939971924,
      "learning_rate": 4.0060437029517446e-05,
      "loss": 1.2844,
      "step": 3930
    },
    {
      "epoch": 2.826398852223816,
      "grad_norm": 3.057532548904419,
      "learning_rate": 3.983967128982673e-05,
      "loss": 1.2176,
      "step": 3940
    },
    {
      "epoch": 2.8335724533715925,
      "grad_norm": 3.448955774307251,
      "learning_rate": 3.961911196511637e-05,
      "loss": 1.2063,
      "step": 3950
    },
    {
      "epoch": 2.840746054519369,
      "grad_norm": 3.5465822219848633,
      "learning_rate": 3.939876353622056e-05,
      "loss": 1.2714,
      "step": 3960
    },
    {
      "epoch": 2.8479196556671447,
      "grad_norm": 4.243638038635254,
      "learning_rate": 3.9178630479689056e-05,
      "loss": 1.2919,
      "step": 3970
    },
    {
      "epoch": 2.855093256814921,
      "grad_norm": 3.472796678543091,
      "learning_rate": 3.8958717267696086e-05,
      "loss": 1.294,
      "step": 3980
    },
    {
      "epoch": 2.8622668579626973,
      "grad_norm": 2.8352386951446533,
      "learning_rate": 3.8739028367949635e-05,
      "loss": 1.3085,
      "step": 3990
    },
    {
      "epoch": 2.869440459110473,
      "grad_norm": 4.101370334625244,
      "learning_rate": 3.8519568243600545e-05,
      "loss": 1.2594,
      "step": 4000
    },
    {
      "epoch": 2.8766140602582495,
      "grad_norm": 2.9843475818634033,
      "learning_rate": 3.8300341353151924e-05,
      "loss": 1.2942,
      "step": 4010
    },
    {
      "epoch": 2.883787661406026,
      "grad_norm": 4.256008625030518,
      "learning_rate": 3.80813521503686e-05,
      "loss": 1.2803,
      "step": 4020
    },
    {
      "epoch": 2.890961262553802,
      "grad_norm": 3.7964906692504883,
      "learning_rate": 3.786260508418655e-05,
      "loss": 1.1934,
      "step": 4030
    },
    {
      "epoch": 2.8981348637015785,
      "grad_norm": 3.2542455196380615,
      "learning_rate": 3.764410459862252e-05,
      "loss": 1.2735,
      "step": 4040
    },
    {
      "epoch": 2.9053084648493543,
      "grad_norm": 3.629845380783081,
      "learning_rate": 3.742585513268387e-05,
      "loss": 1.1649,
      "step": 4050
    },
    {
      "epoch": 2.9124820659971307,
      "grad_norm": 3.703514814376831,
      "learning_rate": 3.720786112027822e-05,
      "loss": 1.2499,
      "step": 4060
    },
    {
      "epoch": 2.919655667144907,
      "grad_norm": 3.6669225692749023,
      "learning_rate": 3.699012699012352e-05,
      "loss": 1.2411,
      "step": 4070
    },
    {
      "epoch": 2.926829268292683,
      "grad_norm": 3.7416434288024902,
      "learning_rate": 3.6772657165657944e-05,
      "loss": 1.2824,
      "step": 4080
    },
    {
      "epoch": 2.934002869440459,
      "grad_norm": 3.369870185852051,
      "learning_rate": 3.655545606495012e-05,
      "loss": 1.2679,
      "step": 4090
    },
    {
      "epoch": 2.9411764705882355,
      "grad_norm": 3.794203519821167,
      "learning_rate": 3.6338528100609375e-05,
      "loss": 1.2432,
      "step": 4100
    },
    {
      "epoch": 2.9483500717360114,
      "grad_norm": 4.001408100128174,
      "learning_rate": 3.612187767969602e-05,
      "loss": 1.1562,
      "step": 4110
    },
    {
      "epoch": 2.9555236728837877,
      "grad_norm": 4.132880687713623,
      "learning_rate": 3.590550920363184e-05,
      "loss": 1.2598,
      "step": 4120
    },
    {
      "epoch": 2.962697274031564,
      "grad_norm": 3.87060546875,
      "learning_rate": 3.568942706811073e-05,
      "loss": 1.249,
      "step": 4130
    },
    {
      "epoch": 2.96987087517934,
      "grad_norm": 3.6912622451782227,
      "learning_rate": 3.547363566300934e-05,
      "loss": 1.235,
      "step": 4140
    },
    {
      "epoch": 2.977044476327116,
      "grad_norm": 3.7822530269622803,
      "learning_rate": 3.525813937229793e-05,
      "loss": 1.2706,
      "step": 4150
    },
    {
      "epoch": 2.9842180774748925,
      "grad_norm": 3.655256509780884,
      "learning_rate": 3.504294257395121e-05,
      "loss": 1.3361,
      "step": 4160
    },
    {
      "epoch": 2.9913916786226684,
      "grad_norm": 4.37824821472168,
      "learning_rate": 3.482804963985953e-05,
      "loss": 1.1553,
      "step": 4170
    },
    {
      "epoch": 2.9985652797704447,
      "grad_norm": 4.055862903594971,
      "learning_rate": 3.4613464935740016e-05,
      "loss": 1.2664,
      "step": 4180
    },
    {
      "epoch": 3.005738880918221,
      "grad_norm": 3.7572593688964844,
      "learning_rate": 3.439919282104782e-05,
      "loss": 1.2192,
      "step": 4190
    },
    {
      "epoch": 3.012912482065997,
      "grad_norm": 2.818673610687256,
      "learning_rate": 3.418523764888758e-05,
      "loss": 1.1908,
      "step": 4200
    },
    {
      "epoch": 3.0200860832137733,
      "grad_norm": 3.78255033493042,
      "learning_rate": 3.397160376592507e-05,
      "loss": 1.1727,
      "step": 4210
    },
    {
      "epoch": 3.0272596843615496,
      "grad_norm": 3.394413709640503,
      "learning_rate": 3.375829551229875e-05,
      "loss": 1.1884,
      "step": 4220
    },
    {
      "epoch": 3.034433285509326,
      "grad_norm": 4.314614772796631,
      "learning_rate": 3.3545317221531756e-05,
      "loss": 1.3113,
      "step": 4230
    },
    {
      "epoch": 3.0416068866571018,
      "grad_norm": 4.518642425537109,
      "learning_rate": 3.3332673220443665e-05,
      "loss": 1.1848,
      "step": 4240
    },
    {
      "epoch": 3.048780487804878,
      "grad_norm": 4.58601713180542,
      "learning_rate": 3.312036782906276e-05,
      "loss": 1.2056,
      "step": 4250
    },
    {
      "epoch": 3.0559540889526544,
      "grad_norm": 3.874765396118164,
      "learning_rate": 3.290840536053821e-05,
      "loss": 1.2358,
      "step": 4260
    },
    {
      "epoch": 3.0631276901004303,
      "grad_norm": 4.063539505004883,
      "learning_rate": 3.269679012105243e-05,
      "loss": 1.205,
      "step": 4270
    },
    {
      "epoch": 3.0703012912482066,
      "grad_norm": 3.9401004314422607,
      "learning_rate": 3.248552640973357e-05,
      "loss": 1.2457,
      "step": 4280
    },
    {
      "epoch": 3.077474892395983,
      "grad_norm": 3.0074095726013184,
      "learning_rate": 3.2274618518568275e-05,
      "loss": 1.1886,
      "step": 4290
    },
    {
      "epoch": 3.084648493543759,
      "grad_norm": 3.417717218399048,
      "learning_rate": 3.206407073231438e-05,
      "loss": 1.2136,
      "step": 4300
    },
    {
      "epoch": 3.091822094691535,
      "grad_norm": 4.205542087554932,
      "learning_rate": 3.1853887328413947e-05,
      "loss": 1.2328,
      "step": 4310
    },
    {
      "epoch": 3.0989956958393114,
      "grad_norm": 3.787410259246826,
      "learning_rate": 3.164407257690628e-05,
      "loss": 1.1808,
      "step": 4320
    },
    {
      "epoch": 3.1061692969870873,
      "grad_norm": 3.3260421752929688,
      "learning_rate": 3.1434630740341254e-05,
      "loss": 1.1402,
      "step": 4330
    },
    {
      "epoch": 3.1133428981348636,
      "grad_norm": 3.818937063217163,
      "learning_rate": 3.122556607369268e-05,
      "loss": 1.2518,
      "step": 4340
    },
    {
      "epoch": 3.12051649928264,
      "grad_norm": 3.4097204208374023,
      "learning_rate": 3.1016882824271886e-05,
      "loss": 1.1973,
      "step": 4350
    },
    {
      "epoch": 3.1276901004304163,
      "grad_norm": 3.1550230979919434,
      "learning_rate": 3.080858523164136e-05,
      "loss": 1.2314,
      "step": 4360
    },
    {
      "epoch": 3.134863701578192,
      "grad_norm": 3.100231647491455,
      "learning_rate": 3.060067752752874e-05,
      "loss": 1.2829,
      "step": 4370
    },
    {
      "epoch": 3.1420373027259685,
      "grad_norm": 3.744548797607422,
      "learning_rate": 3.039316393574073e-05,
      "loss": 1.2307,
      "step": 4380
    },
    {
      "epoch": 3.149210903873745,
      "grad_norm": 3.804642677307129,
      "learning_rate": 3.0186048672077367e-05,
      "loss": 1.1748,
      "step": 4390
    },
    {
      "epoch": 3.1563845050215207,
      "grad_norm": 3.2979695796966553,
      "learning_rate": 2.99793359442463e-05,
      "loss": 1.1567,
      "step": 4400
    },
    {
      "epoch": 3.163558106169297,
      "grad_norm": 3.2446231842041016,
      "learning_rate": 2.9773029951777364e-05,
      "loss": 1.2027,
      "step": 4410
    },
    {
      "epoch": 3.1707317073170733,
      "grad_norm": 4.503070831298828,
      "learning_rate": 2.95671348859373e-05,
      "loss": 1.2218,
      "step": 4420
    },
    {
      "epoch": 3.177905308464849,
      "grad_norm": 3.4492785930633545,
      "learning_rate": 2.9361654929644472e-05,
      "loss": 1.1826,
      "step": 4430
    },
    {
      "epoch": 3.1850789096126255,
      "grad_norm": 3.4961612224578857,
      "learning_rate": 2.9156594257384058e-05,
      "loss": 1.1889,
      "step": 4440
    },
    {
      "epoch": 3.192252510760402,
      "grad_norm": 3.682335376739502,
      "learning_rate": 2.8951957035123057e-05,
      "loss": 1.1987,
      "step": 4450
    },
    {
      "epoch": 3.1994261119081777,
      "grad_norm": 3.5847089290618896,
      "learning_rate": 2.8747747420225847e-05,
      "loss": 1.1628,
      "step": 4460
    },
    {
      "epoch": 3.206599713055954,
      "grad_norm": 4.206514358520508,
      "learning_rate": 2.8543969561369556e-05,
      "loss": 1.2277,
      "step": 4470
    },
    {
      "epoch": 3.2137733142037304,
      "grad_norm": 3.7147610187530518,
      "learning_rate": 2.8340627598459912e-05,
      "loss": 1.218,
      "step": 4480
    },
    {
      "epoch": 3.2209469153515062,
      "grad_norm": 3.26186203956604,
      "learning_rate": 2.813772566254701e-05,
      "loss": 1.1326,
      "step": 4490
    },
    {
      "epoch": 3.2281205164992826,
      "grad_norm": 3.62960147857666,
      "learning_rate": 2.7935267875741513e-05,
      "loss": 1.1793,
      "step": 4500
    },
    {
      "epoch": 3.235294117647059,
      "grad_norm": 4.091703414916992,
      "learning_rate": 2.7733258351130853e-05,
      "loss": 1.1905,
      "step": 4510
    },
    {
      "epoch": 3.242467718794835,
      "grad_norm": 4.475652694702148,
      "learning_rate": 2.7531701192695624e-05,
      "loss": 1.2425,
      "step": 4520
    },
    {
      "epoch": 3.249641319942611,
      "grad_norm": 4.015239715576172,
      "learning_rate": 2.7330600495226248e-05,
      "loss": 1.1775,
      "step": 4530
    },
    {
      "epoch": 3.2568149210903874,
      "grad_norm": 4.064723968505859,
      "learning_rate": 2.7129960344239824e-05,
      "loss": 1.2229,
      "step": 4540
    },
    {
      "epoch": 3.2639885222381637,
      "grad_norm": 4.092742919921875,
      "learning_rate": 2.6929784815897087e-05,
      "loss": 1.1715,
      "step": 4550
    },
    {
      "epoch": 3.2711621233859396,
      "grad_norm": 4.049345016479492,
      "learning_rate": 2.673007797691959e-05,
      "loss": 1.1793,
      "step": 4560
    },
    {
      "epoch": 3.278335724533716,
      "grad_norm": 3.5590262413024902,
      "learning_rate": 2.6530843884507085e-05,
      "loss": 1.3027,
      "step": 4570
    },
    {
      "epoch": 3.2855093256814922,
      "grad_norm": 3.7845003604888916,
      "learning_rate": 2.633208658625511e-05,
      "loss": 1.145,
      "step": 4580
    },
    {
      "epoch": 3.292682926829268,
      "grad_norm": 3.8717458248138428,
      "learning_rate": 2.6133810120072776e-05,
      "loss": 1.1791,
      "step": 4590
    },
    {
      "epoch": 3.2998565279770444,
      "grad_norm": 3.6261487007141113,
      "learning_rate": 2.5936018514100745e-05,
      "loss": 1.2036,
      "step": 4600
    },
    {
      "epoch": 3.3070301291248207,
      "grad_norm": 3.903003692626953,
      "learning_rate": 2.5738715786629285e-05,
      "loss": 1.1491,
      "step": 4610
    },
    {
      "epoch": 3.314203730272597,
      "grad_norm": 4.166619777679443,
      "learning_rate": 2.5541905946016808e-05,
      "loss": 1.2601,
      "step": 4620
    },
    {
      "epoch": 3.321377331420373,
      "grad_norm": 3.6039326190948486,
      "learning_rate": 2.534559299060826e-05,
      "loss": 1.1757,
      "step": 4630
    },
    {
      "epoch": 3.3285509325681493,
      "grad_norm": 4.885380744934082,
      "learning_rate": 2.514978090865405e-05,
      "loss": 1.2017,
      "step": 4640
    },
    {
      "epoch": 3.3357245337159256,
      "grad_norm": 4.422694206237793,
      "learning_rate": 2.4954473678228906e-05,
      "loss": 1.1454,
      "step": 4650
    },
    {
      "epoch": 3.3428981348637015,
      "grad_norm": 3.2337427139282227,
      "learning_rate": 2.4759675267151123e-05,
      "loss": 1.2077,
      "step": 4660
    },
    {
      "epoch": 3.350071736011478,
      "grad_norm": 3.9897513389587402,
      "learning_rate": 2.4565389632901965e-05,
      "loss": 1.1977,
      "step": 4670
    },
    {
      "epoch": 3.357245337159254,
      "grad_norm": 4.135392665863037,
      "learning_rate": 2.437162072254518e-05,
      "loss": 1.2052,
      "step": 4680
    },
    {
      "epoch": 3.36441893830703,
      "grad_norm": 4.298520088195801,
      "learning_rate": 2.417837247264687e-05,
      "loss": 1.2199,
      "step": 4690
    },
    {
      "epoch": 3.3715925394548063,
      "grad_norm": 3.6611790657043457,
      "learning_rate": 2.3985648809195566e-05,
      "loss": 1.1454,
      "step": 4700
    },
    {
      "epoch": 3.3787661406025826,
      "grad_norm": 3.8052868843078613,
      "learning_rate": 2.379345364752239e-05,
      "loss": 1.2042,
      "step": 4710
    },
    {
      "epoch": 3.3859397417503585,
      "grad_norm": 3.8551933765411377,
      "learning_rate": 2.360179089222155e-05,
      "loss": 1.1955,
      "step": 4720
    },
    {
      "epoch": 3.393113342898135,
      "grad_norm": 4.810494899749756,
      "learning_rate": 2.3410664437070974e-05,
      "loss": 1.2268,
      "step": 4730
    },
    {
      "epoch": 3.400286944045911,
      "grad_norm": 4.265837669372559,
      "learning_rate": 2.322007816495324e-05,
      "loss": 1.2462,
      "step": 4740
    },
    {
      "epoch": 3.407460545193687,
      "grad_norm": 4.237472057342529,
      "learning_rate": 2.304901557690068e-05,
      "loss": 1.1595,
      "step": 4750
    },
    {
      "epoch": 3.4146341463414633,
      "grad_norm": 3.9631285667419434,
      "learning_rate": 2.2859466310519294e-05,
      "loss": 1.2585,
      "step": 4760
    },
    {
      "epoch": 3.4218077474892397,
      "grad_norm": 4.2127604484558105,
      "learning_rate": 2.267046842518943e-05,
      "loss": 1.2127,
      "step": 4770
    },
    {
      "epoch": 3.4289813486370155,
      "grad_norm": 3.7874293327331543,
      "learning_rate": 2.2482025760550125e-05,
      "loss": 1.275,
      "step": 4780
    },
    {
      "epoch": 3.436154949784792,
      "grad_norm": 4.347982406616211,
      "learning_rate": 2.2294142144960696e-05,
      "loss": 1.2824,
      "step": 4790
    },
    {
      "epoch": 3.443328550932568,
      "grad_norm": 3.4955623149871826,
      "learning_rate": 2.2106821395422912e-05,
      "loss": 1.1824,
      "step": 4800
    },
    {
      "epoch": 3.4505021520803445,
      "grad_norm": 4.303693771362305,
      "learning_rate": 2.1920067317503473e-05,
      "loss": 1.1422,
      "step": 4810
    },
    {
      "epoch": 3.4576757532281204,
      "grad_norm": 3.85324764251709,
      "learning_rate": 2.1733883705256748e-05,
      "loss": 1.2362,
      "step": 4820
    },
    {
      "epoch": 3.4648493543758967,
      "grad_norm": 3.4956421852111816,
      "learning_rate": 2.154827434114765e-05,
      "loss": 1.1367,
      "step": 4830
    },
    {
      "epoch": 3.472022955523673,
      "grad_norm": 3.5056331157684326,
      "learning_rate": 2.136324299597474e-05,
      "loss": 1.182,
      "step": 4840
    },
    {
      "epoch": 3.479196556671449,
      "grad_norm": 3.976146697998047,
      "learning_rate": 2.117879342879375e-05,
      "loss": 1.114,
      "step": 4850
    },
    {
      "epoch": 3.486370157819225,
      "grad_norm": 3.959252119064331,
      "learning_rate": 2.0994929386841055e-05,
      "loss": 1.1558,
      "step": 4860
    },
    {
      "epoch": 3.4935437589670015,
      "grad_norm": 4.4583587646484375,
      "learning_rate": 2.081165460545772e-05,
      "loss": 1.2671,
      "step": 4870
    },
    {
      "epoch": 3.500717360114778,
      "grad_norm": 4.145567893981934,
      "learning_rate": 2.062897280801344e-05,
      "loss": 1.1976,
      "step": 4880
    },
    {
      "epoch": 3.5078909612625537,
      "grad_norm": 2.6956064701080322,
      "learning_rate": 2.044688770583103e-05,
      "loss": 1.202,
      "step": 4890
    },
    {
      "epoch": 3.51506456241033,
      "grad_norm": 3.705683708190918,
      "learning_rate": 2.0265402998110987e-05,
      "loss": 1.2189,
      "step": 4900
    },
    {
      "epoch": 3.5222381635581064,
      "grad_norm": 4.747115135192871,
      "learning_rate": 2.008452237185629e-05,
      "loss": 1.1901,
      "step": 4910
    },
    {
      "epoch": 3.5294117647058822,
      "grad_norm": 4.026570796966553,
      "learning_rate": 1.990424950179753e-05,
      "loss": 1.2157,
      "step": 4920
    },
    {
      "epoch": 3.5365853658536586,
      "grad_norm": 3.4651272296905518,
      "learning_rate": 1.972458805031829e-05,
      "loss": 1.219,
      "step": 4930
    },
    {
      "epoch": 3.543758967001435,
      "grad_norm": 3.565861701965332,
      "learning_rate": 1.954554166738069e-05,
      "loss": 1.1214,
      "step": 4940
    },
    {
      "epoch": 3.5509325681492108,
      "grad_norm": 4.14371919631958,
      "learning_rate": 1.936711399045129e-05,
      "loss": 1.1626,
      "step": 4950
    },
    {
      "epoch": 3.558106169296987,
      "grad_norm": 4.0647993087768555,
      "learning_rate": 1.9189308644427106e-05,
      "loss": 1.2082,
      "step": 4960
    },
    {
      "epoch": 3.5652797704447634,
      "grad_norm": 3.1733036041259766,
      "learning_rate": 1.9012129241562016e-05,
      "loss": 1.2212,
      "step": 4970
    },
    {
      "epoch": 3.5724533715925393,
      "grad_norm": 3.9053313732147217,
      "learning_rate": 1.8835579381393416e-05,
      "loss": 1.1878,
      "step": 4980
    },
    {
      "epoch": 3.5796269727403156,
      "grad_norm": 4.7555952072143555,
      "learning_rate": 1.8659662650669036e-05,
      "loss": 1.2173,
      "step": 4990
    },
    {
      "epoch": 3.586800573888092,
      "grad_norm": 4.513724327087402,
      "learning_rate": 1.84843826232741e-05,
      "loss": 1.2452,
      "step": 5000
    },
    {
      "epoch": 3.593974175035868,
      "grad_norm": 3.4467015266418457,
      "learning_rate": 1.830974286015868e-05,
      "loss": 1.1766,
      "step": 5010
    },
    {
      "epoch": 3.601147776183644,
      "grad_norm": 4.12606954574585,
      "learning_rate": 1.813574690926538e-05,
      "loss": 1.1298,
      "step": 5020
    },
    {
      "epoch": 3.6083213773314204,
      "grad_norm": 3.7116858959198,
      "learning_rate": 1.796239830545729e-05,
      "loss": 1.2082,
      "step": 5030
    },
    {
      "epoch": 3.6154949784791963,
      "grad_norm": 3.914360284805298,
      "learning_rate": 1.778970057044615e-05,
      "loss": 1.1506,
      "step": 5040
    },
    {
      "epoch": 3.6226685796269726,
      "grad_norm": 3.9355063438415527,
      "learning_rate": 1.7617657212720722e-05,
      "loss": 1.2001,
      "step": 5050
    },
    {
      "epoch": 3.629842180774749,
      "grad_norm": 3.2213289737701416,
      "learning_rate": 1.7446271727475677e-05,
      "loss": 1.2036,
      "step": 5060
    },
    {
      "epoch": 3.637015781922525,
      "grad_norm": 4.7307047843933105,
      "learning_rate": 1.7275547596540408e-05,
      "loss": 1.1928,
      "step": 5070
    },
    {
      "epoch": 3.644189383070301,
      "grad_norm": 3.616403341293335,
      "learning_rate": 1.7105488288308458e-05,
      "loss": 1.1524,
      "step": 5080
    },
    {
      "epoch": 3.6513629842180775,
      "grad_norm": 3.057546615600586,
      "learning_rate": 1.6936097257666915e-05,
      "loss": 1.1476,
      "step": 5090
    },
    {
      "epoch": 3.658536585365854,
      "grad_norm": 3.5144691467285156,
      "learning_rate": 1.6767377945926333e-05,
      "loss": 1.2037,
      "step": 5100
    },
    {
      "epoch": 3.6657101865136297,
      "grad_norm": 4.208810806274414,
      "learning_rate": 1.6599333780750765e-05,
      "loss": 1.2275,
      "step": 5110
    },
    {
      "epoch": 3.672883787661406,
      "grad_norm": 3.598862886428833,
      "learning_rate": 1.6431968176088125e-05,
      "loss": 1.1799,
      "step": 5120
    },
    {
      "epoch": 3.6800573888091823,
      "grad_norm": 3.3018832206726074,
      "learning_rate": 1.6265284532100823e-05,
      "loss": 1.248,
      "step": 5130
    },
    {
      "epoch": 3.6872309899569586,
      "grad_norm": 3.1273915767669678,
      "learning_rate": 1.609928623509675e-05,
      "loss": 1.1885,
      "step": 5140
    },
    {
      "epoch": 3.6944045911047345,
      "grad_norm": 4.1981730461120605,
      "learning_rate": 1.5933976657460426e-05,
      "loss": 1.1867,
      "step": 5150
    },
    {
      "epoch": 3.701578192252511,
      "grad_norm": 4.524226665496826,
      "learning_rate": 1.5769359157584503e-05,
      "loss": 1.0946,
      "step": 5160
    },
    {
      "epoch": 3.708751793400287,
      "grad_norm": 3.520951271057129,
      "learning_rate": 1.560543707980152e-05,
      "loss": 1.159,
      "step": 5170
    },
    {
      "epoch": 3.715925394548063,
      "grad_norm": 4.577002048492432,
      "learning_rate": 1.5442213754315963e-05,
      "loss": 1.1168,
      "step": 5180
    },
    {
      "epoch": 3.7230989956958394,
      "grad_norm": 3.8930821418762207,
      "learning_rate": 1.5279692497136668e-05,
      "loss": 1.1708,
      "step": 5190
    },
    {
      "epoch": 3.7302725968436157,
      "grad_norm": 4.238011360168457,
      "learning_rate": 1.5117876610009385e-05,
      "loss": 1.1079,
      "step": 5200
    },
    {
      "epoch": 3.7374461979913915,
      "grad_norm": 3.650970458984375,
      "learning_rate": 1.49567693803497e-05,
      "loss": 1.2287,
      "step": 5210
    },
    {
      "epoch": 3.744619799139168,
      "grad_norm": 3.534701108932495,
      "learning_rate": 1.4796374081176312e-05,
      "loss": 1.178,
      "step": 5220
    },
    {
      "epoch": 3.751793400286944,
      "grad_norm": 3.432518243789673,
      "learning_rate": 1.4636693971044452e-05,
      "loss": 1.252,
      "step": 5230
    },
    {
      "epoch": 3.75896700143472,
      "grad_norm": 3.9157893657684326,
      "learning_rate": 1.4477732293979784e-05,
      "loss": 1.2419,
      "step": 5240
    },
    {
      "epoch": 3.7661406025824964,
      "grad_norm": 3.82936429977417,
      "learning_rate": 1.4319492279412388e-05,
      "loss": 1.1429,
      "step": 5250
    },
    {
      "epoch": 3.7733142037302727,
      "grad_norm": 3.8452839851379395,
      "learning_rate": 1.4161977142111254e-05,
      "loss": 1.2482,
      "step": 5260
    },
    {
      "epoch": 3.7804878048780486,
      "grad_norm": 3.7490315437316895,
      "learning_rate": 1.4005190082118903e-05,
      "loss": 1.1457,
      "step": 5270
    },
    {
      "epoch": 3.787661406025825,
      "grad_norm": 4.597872734069824,
      "learning_rate": 1.3849134284686438e-05,
      "loss": 1.2276,
      "step": 5280
    },
    {
      "epoch": 3.7948350071736012,
      "grad_norm": 3.6485514640808105,
      "learning_rate": 1.369381292020871e-05,
      "loss": 1.2336,
      "step": 5290
    },
    {
      "epoch": 3.802008608321377,
      "grad_norm": 4.124776840209961,
      "learning_rate": 1.3539229144160064e-05,
      "loss": 1.171,
      "step": 5300
    },
    {
      "epoch": 3.8091822094691534,
      "grad_norm": 4.130079746246338,
      "learning_rate": 1.338538609703015e-05,
      "loss": 1.0576,
      "step": 5310
    },
    {
      "epoch": 3.8163558106169297,
      "grad_norm": 3.770876169204712,
      "learning_rate": 1.3232286904260144e-05,
      "loss": 1.1856,
      "step": 5320
    },
    {
      "epoch": 3.8235294117647056,
      "grad_norm": 3.602327346801758,
      "learning_rate": 1.3079934676179212e-05,
      "loss": 1.1412,
      "step": 5330
    },
    {
      "epoch": 3.830703012912482,
      "grad_norm": 4.281403541564941,
      "learning_rate": 1.2928332507941342e-05,
      "loss": 1.1728,
      "step": 5340
    },
    {
      "epoch": 3.8378766140602583,
      "grad_norm": 3.847236156463623,
      "learning_rate": 1.2777483479462527e-05,
      "loss": 1.204,
      "step": 5350
    },
    {
      "epoch": 3.8450502152080346,
      "grad_norm": 3.92620849609375,
      "learning_rate": 1.2627390655358107e-05,
      "loss": 1.0812,
      "step": 5360
    },
    {
      "epoch": 3.8522238163558105,
      "grad_norm": 3.531442642211914,
      "learning_rate": 1.2478057084880524e-05,
      "loss": 1.227,
      "step": 5370
    },
    {
      "epoch": 3.859397417503587,
      "grad_norm": 3.4632375240325928,
      "learning_rate": 1.2329485801857443e-05,
      "loss": 1.1573,
      "step": 5380
    },
    {
      "epoch": 3.866571018651363,
      "grad_norm": 4.68064546585083,
      "learning_rate": 1.218167982463e-05,
      "loss": 1.2288,
      "step": 5390
    },
    {
      "epoch": 3.8737446197991394,
      "grad_norm": 4.585046768188477,
      "learning_rate": 1.2034642155991615e-05,
      "loss": 1.2379,
      "step": 5400
    },
    {
      "epoch": 3.8809182209469153,
      "grad_norm": 3.2365214824676514,
      "learning_rate": 1.188837578312687e-05,
      "loss": 1.2156,
      "step": 5410
    },
    {
      "epoch": 3.8880918220946916,
      "grad_norm": 5.103088855743408,
      "learning_rate": 1.1742883677550898e-05,
      "loss": 1.2401,
      "step": 5420
    },
    {
      "epoch": 3.895265423242468,
      "grad_norm": 3.3523221015930176,
      "learning_rate": 1.1598168795048991e-05,
      "loss": 1.2282,
      "step": 5430
    },
    {
      "epoch": 3.902439024390244,
      "grad_norm": 3.5404927730560303,
      "learning_rate": 1.1454234075616572e-05,
      "loss": 1.1799,
      "step": 5440
    },
    {
      "epoch": 3.90961262553802,
      "grad_norm": 4.004697799682617,
      "learning_rate": 1.1311082443399357e-05,
      "loss": 1.234,
      "step": 5450
    },
    {
      "epoch": 3.9167862266857965,
      "grad_norm": 3.977254629135132,
      "learning_rate": 1.1168716806634133e-05,
      "loss": 1.1333,
      "step": 5460
    },
    {
      "epoch": 3.9239598278335723,
      "grad_norm": 3.2319061756134033,
      "learning_rate": 1.1027140057589536e-05,
      "loss": 1.1787,
      "step": 5470
    },
    {
      "epoch": 3.9311334289813487,
      "grad_norm": 4.040940761566162,
      "learning_rate": 1.0886355072507353e-05,
      "loss": 1.2494,
      "step": 5480
    },
    {
      "epoch": 3.938307030129125,
      "grad_norm": 3.8172807693481445,
      "learning_rate": 1.0746364711544043e-05,
      "loss": 1.2288,
      "step": 5490
    },
    {
      "epoch": 3.945480631276901,
      "grad_norm": 4.083395957946777,
      "learning_rate": 1.0607171818712663e-05,
      "loss": 1.2462,
      "step": 5500
    },
    {
      "epoch": 3.952654232424677,
      "grad_norm": 3.8802590370178223,
      "learning_rate": 1.0468779221825103e-05,
      "loss": 1.1035,
      "step": 5510
    },
    {
      "epoch": 3.9598278335724535,
      "grad_norm": 4.332485198974609,
      "learning_rate": 1.0331189732434626e-05,
      "loss": 1.1415,
      "step": 5520
    },
    {
      "epoch": 3.9670014347202294,
      "grad_norm": 4.500051498413086,
      "learning_rate": 1.019440614577869e-05,
      "loss": 1.2152,
      "step": 5530
    },
    {
      "epoch": 3.9741750358680057,
      "grad_norm": 4.184479236602783,
      "learning_rate": 1.0058431240722266e-05,
      "loss": 1.1507,
      "step": 5540
    },
    {
      "epoch": 3.981348637015782,
      "grad_norm": 3.8617615699768066,
      "learning_rate": 9.923267779701323e-06,
      "loss": 1.1579,
      "step": 5550
    },
    {
      "epoch": 3.988522238163558,
      "grad_norm": 3.8623456954956055,
      "learning_rate": 9.78891850866669e-06,
      "loss": 1.2259,
      "step": 5560
    },
    {
      "epoch": 3.995695839311334,
      "grad_norm": 3.6561763286590576,
      "learning_rate": 9.655386157028285e-06,
      "loss": 1.2097,
      "step": 5570
    },
    {
      "epoch": 4.0028694404591105,
      "grad_norm": 3.438598155975342,
      "learning_rate": 9.522673437599705e-06,
      "loss": 1.1916,
      "step": 5580
    },
    {
      "epoch": 4.010043041606886,
      "grad_norm": 4.219484806060791,
      "learning_rate": 9.390783046543073e-06,
      "loss": 1.1362,
      "step": 5590
    },
    {
      "epoch": 4.017216642754663,
      "grad_norm": 4.197929382324219,
      "learning_rate": 9.259717663314283e-06,
      "loss": 1.1676,
      "step": 5600
    },
    {
      "epoch": 4.024390243902439,
      "grad_norm": 3.139819860458374,
      "learning_rate": 9.129479950608494e-06,
      "loss": 1.1488,
      "step": 5610
    },
    {
      "epoch": 4.031563845050215,
      "grad_norm": 4.906191825866699,
      "learning_rate": 9.00007255430616e-06,
      "loss": 1.2078,
      "step": 5620
    },
    {
      "epoch": 4.038737446197992,
      "grad_norm": 4.327427864074707,
      "learning_rate": 8.871498103419185e-06,
      "loss": 1.1287,
      "step": 5630
    },
    {
      "epoch": 4.045911047345768,
      "grad_norm": 3.7278826236724854,
      "learning_rate": 8.743759210037561e-06,
      "loss": 1.1558,
      "step": 5640
    },
    {
      "epoch": 4.053084648493543,
      "grad_norm": 4.654006481170654,
      "learning_rate": 8.616858469276245e-06,
      "loss": 1.1859,
      "step": 5650
    },
    {
      "epoch": 4.06025824964132,
      "grad_norm": 4.236898899078369,
      "learning_rate": 8.490798459222476e-06,
      "loss": 1.22,
      "step": 5660
    },
    {
      "epoch": 4.067431850789096,
      "grad_norm": 4.446300506591797,
      "learning_rate": 8.365581740883417e-06,
      "loss": 1.1494,
      "step": 5670
    },
    {
      "epoch": 4.074605451936872,
      "grad_norm": 4.021160125732422,
      "learning_rate": 8.241210858134108e-06,
      "loss": 1.1913,
      "step": 5680
    },
    {
      "epoch": 4.081779053084649,
      "grad_norm": 3.8721697330474854,
      "learning_rate": 8.117688337665735e-06,
      "loss": 1.1762,
      "step": 5690
    },
    {
      "epoch": 4.088952654232425,
      "grad_norm": 4.5951995849609375,
      "learning_rate": 7.995016688934403e-06,
      "loss": 1.1672,
      "step": 5700
    },
    {
      "epoch": 4.0961262553802005,
      "grad_norm": 5.439704418182373,
      "learning_rate": 7.873198404110065e-06,
      "loss": 1.1935,
      "step": 5710
    },
    {
      "epoch": 4.103299856527977,
      "grad_norm": 4.625734806060791,
      "learning_rate": 7.752235958025921e-06,
      "loss": 1.1241,
      "step": 5720
    },
    {
      "epoch": 4.110473457675753,
      "grad_norm": 4.209372520446777,
      "learning_rate": 7.63213180812814e-06,
      "loss": 1.1386,
      "step": 5730
    },
    {
      "epoch": 4.117647058823529,
      "grad_norm": 3.713423728942871,
      "learning_rate": 7.512888394425943e-06,
      "loss": 1.1578,
      "step": 5740
    },
    {
      "epoch": 4.124820659971306,
      "grad_norm": 2.683363437652588,
      "learning_rate": 7.394508139442036e-06,
      "loss": 1.1372,
      "step": 5750
    },
    {
      "epoch": 4.131994261119082,
      "grad_norm": 4.227528095245361,
      "learning_rate": 7.276993448163377e-06,
      "loss": 1.2034,
      "step": 5760
    },
    {
      "epoch": 4.1391678622668575,
      "grad_norm": 3.32419490814209,
      "learning_rate": 7.160346707992316e-06,
      "loss": 1.1818,
      "step": 5770
    },
    {
      "epoch": 4.146341463414634,
      "grad_norm": 3.138572931289673,
      "learning_rate": 7.044570288698093e-06,
      "loss": 1.1446,
      "step": 5780
    },
    {
      "epoch": 4.15351506456241,
      "grad_norm": 3.6047587394714355,
      "learning_rate": 6.9296665423687235e-06,
      "loss": 1.1792,
      "step": 5790
    },
    {
      "epoch": 4.160688665710187,
      "grad_norm": 3.9020161628723145,
      "learning_rate": 6.8156378033631896e-06,
      "loss": 1.1666,
      "step": 5800
    },
    {
      "epoch": 4.167862266857963,
      "grad_norm": 4.442732810974121,
      "learning_rate": 6.70248638826404e-06,
      "loss": 1.1598,
      "step": 5810
    },
    {
      "epoch": 4.175035868005739,
      "grad_norm": 4.14534854888916,
      "learning_rate": 6.590214595830224e-06,
      "loss": 1.156,
      "step": 5820
    },
    {
      "epoch": 4.182209469153515,
      "grad_norm": 3.876357316970825,
      "learning_rate": 6.478824706950559e-06,
      "loss": 1.1515,
      "step": 5830
    },
    {
      "epoch": 4.189383070301291,
      "grad_norm": 4.1260480880737305,
      "learning_rate": 6.368318984597277e-06,
      "loss": 1.1763,
      "step": 5840
    },
    {
      "epoch": 4.196556671449067,
      "grad_norm": 3.5846104621887207,
      "learning_rate": 6.258699673780083e-06,
      "loss": 1.0858,
      "step": 5850
    },
    {
      "epoch": 4.203730272596844,
      "grad_norm": 4.105388164520264,
      "learning_rate": 6.149969001500522e-06,
      "loss": 1.0947,
      "step": 5860
    },
    {
      "epoch": 4.21090387374462,
      "grad_norm": 3.9972803592681885,
      "learning_rate": 6.042129176706785e-06,
      "loss": 1.2059,
      "step": 5870
    },
    {
      "epoch": 4.218077474892396,
      "grad_norm": 3.5443155765533447,
      "learning_rate": 5.935182390248772e-06,
      "loss": 1.1077,
      "step": 5880
    },
    {
      "epoch": 4.2252510760401725,
      "grad_norm": 4.199293613433838,
      "learning_rate": 5.829130814833656e-06,
      "loss": 1.1558,
      "step": 5890
    },
    {
      "epoch": 4.232424677187948,
      "grad_norm": 4.57097053527832,
      "learning_rate": 5.723976604981651e-06,
      "loss": 1.1879,
      "step": 5900
    },
    {
      "epoch": 4.239598278335724,
      "grad_norm": 4.106747627258301,
      "learning_rate": 5.619721896982327e-06,
      "loss": 1.1889,
      "step": 5910
    },
    {
      "epoch": 4.246771879483501,
      "grad_norm": 4.796617031097412,
      "learning_rate": 5.516368808851186e-06,
      "loss": 1.1337,
      "step": 5920
    },
    {
      "epoch": 4.253945480631277,
      "grad_norm": 4.047421455383301,
      "learning_rate": 5.41391944028658e-06,
      "loss": 1.1201,
      "step": 5930
    },
    {
      "epoch": 4.261119081779053,
      "grad_norm": 4.32187557220459,
      "learning_rate": 5.312375872627107e-06,
      "loss": 1.1638,
      "step": 5940
    },
    {
      "epoch": 4.2682926829268295,
      "grad_norm": 3.584005355834961,
      "learning_rate": 5.211740168809337e-06,
      "loss": 1.1441,
      "step": 5950
    },
    {
      "epoch": 4.275466284074605,
      "grad_norm": 3.550370931625366,
      "learning_rate": 5.112014373325863e-06,
      "loss": 1.1494,
      "step": 5960
    },
    {
      "epoch": 4.282639885222381,
      "grad_norm": 3.4445512294769287,
      "learning_rate": 5.013200512183797e-06,
      "loss": 1.1534,
      "step": 5970
    },
    {
      "epoch": 4.289813486370158,
      "grad_norm": 3.966916084289551,
      "learning_rate": 4.915300592863575e-06,
      "loss": 1.1227,
      "step": 5980
    },
    {
      "epoch": 4.296987087517934,
      "grad_norm": 3.7124502658843994,
      "learning_rate": 4.818316604278195e-06,
      "loss": 1.2076,
      "step": 5990
    },
    {
      "epoch": 4.30416068866571,
      "grad_norm": 3.813258647918701,
      "learning_rate": 4.722250516732823e-06,
      "loss": 1.1367,
      "step": 6000
    },
    {
      "epoch": 4.3113342898134865,
      "grad_norm": 4.39560079574585,
      "learning_rate": 4.627104281884764e-06,
      "loss": 1.2083,
      "step": 6010
    },
    {
      "epoch": 4.318507890961262,
      "grad_norm": 4.537356853485107,
      "learning_rate": 4.532879832703757e-06,
      "loss": 1.2108,
      "step": 6020
    },
    {
      "epoch": 4.325681492109039,
      "grad_norm": 3.8884308338165283,
      "learning_rate": 4.4395790834327985e-06,
      "loss": 1.2273,
      "step": 6030
    },
    {
      "epoch": 4.332855093256815,
      "grad_norm": 3.871563673019409,
      "learning_rate": 4.347203929549148e-06,
      "loss": 1.1298,
      "step": 6040
    },
    {
      "epoch": 4.340028694404591,
      "grad_norm": 3.517484426498413,
      "learning_rate": 4.25575624772594e-06,
      "loss": 1.1623,
      "step": 6050
    },
    {
      "epoch": 4.347202295552368,
      "grad_norm": 6.128349304199219,
      "learning_rate": 4.165237895793928e-06,
      "loss": 1.1241,
      "step": 6060
    },
    {
      "epoch": 4.354375896700144,
      "grad_norm": 5.155741214752197,
      "learning_rate": 4.075650712703849e-06,
      "loss": 1.1941,
      "step": 6070
    },
    {
      "epoch": 4.3615494978479195,
      "grad_norm": 3.504014730453491,
      "learning_rate": 3.986996518489011e-06,
      "loss": 1.1761,
      "step": 6080
    },
    {
      "epoch": 4.368723098995696,
      "grad_norm": 3.768571376800537,
      "learning_rate": 3.899277114228323e-06,
      "loss": 1.1322,
      "step": 6090
    },
    {
      "epoch": 4.375896700143472,
      "grad_norm": 3.3212881088256836,
      "learning_rate": 3.8124942820096987e-06,
      "loss": 1.0485,
      "step": 6100
    },
    {
      "epoch": 4.383070301291248,
      "grad_norm": 4.536830425262451,
      "learning_rate": 3.726649784893876e-06,
      "loss": 1.1428,
      "step": 6110
    },
    {
      "epoch": 4.390243902439025,
      "grad_norm": 3.4675984382629395,
      "learning_rate": 3.6417453668785907e-06,
      "loss": 1.1927,
      "step": 6120
    },
    {
      "epoch": 4.397417503586801,
      "grad_norm": 3.3721094131469727,
      "learning_rate": 3.557782752863137e-06,
      "loss": 1.174,
      "step": 6130
    },
    {
      "epoch": 4.4045911047345765,
      "grad_norm": 3.7881767749786377,
      "learning_rate": 3.4747636486133107e-06,
      "loss": 1.1477,
      "step": 6140
    },
    {
      "epoch": 4.411764705882353,
      "grad_norm": 4.125890731811523,
      "learning_rate": 3.39268974072679e-06,
      "loss": 1.1572,
      "step": 6150
    },
    {
      "epoch": 4.418938307030129,
      "grad_norm": 3.47713303565979,
      "learning_rate": 3.3115626965988468e-06,
      "loss": 1.1656,
      "step": 6160
    },
    {
      "epoch": 4.426111908177905,
      "grad_norm": 4.148692607879639,
      "learning_rate": 3.2313841643884957e-06,
      "loss": 1.2636,
      "step": 6170
    },
    {
      "epoch": 4.433285509325682,
      "grad_norm": 4.7784857749938965,
      "learning_rate": 3.152155772984966e-06,
      "loss": 1.1443,
      "step": 6180
    },
    {
      "epoch": 4.440459110473458,
      "grad_norm": 4.314377784729004,
      "learning_rate": 3.0738791319746606e-06,
      "loss": 1.1526,
      "step": 6190
    },
    {
      "epoch": 4.4476327116212335,
      "grad_norm": 4.142667770385742,
      "learning_rate": 2.9965558316084164e-06,
      "loss": 1.2285,
      "step": 6200
    },
    {
      "epoch": 4.45480631276901,
      "grad_norm": 3.934419870376587,
      "learning_rate": 2.9201874427692342e-06,
      "loss": 1.1635,
      "step": 6210
    },
    {
      "epoch": 4.461979913916786,
      "grad_norm": 3.6532692909240723,
      "learning_rate": 2.844775516940318e-06,
      "loss": 1.0577,
      "step": 6220
    },
    {
      "epoch": 4.469153515064562,
      "grad_norm": 4.75184440612793,
      "learning_rate": 2.7703215861735966e-06,
      "loss": 1.0984,
      "step": 6230
    },
    {
      "epoch": 4.476327116212339,
      "grad_norm": 4.3739495277404785,
      "learning_rate": 2.6968271630586017e-06,
      "loss": 1.181,
      "step": 6240
    },
    {
      "epoch": 4.483500717360115,
      "grad_norm": 4.103611946105957,
      "learning_rate": 2.6242937406916856e-06,
      "loss": 1.1073,
      "step": 6250
    },
    {
      "epoch": 4.490674318507891,
      "grad_norm": 3.864940643310547,
      "learning_rate": 2.552722792645734e-06,
      "loss": 1.0906,
      "step": 6260
    },
    {
      "epoch": 4.497847919655667,
      "grad_norm": 3.2048678398132324,
      "learning_rate": 2.48211577294023e-06,
      "loss": 1.1709,
      "step": 6270
    },
    {
      "epoch": 4.505021520803443,
      "grad_norm": 4.373214244842529,
      "learning_rate": 2.4124741160116983e-06,
      "loss": 1.1838,
      "step": 6280
    },
    {
      "epoch": 4.512195121951219,
      "grad_norm": 4.656866073608398,
      "learning_rate": 2.343799236684574e-06,
      "loss": 1.1467,
      "step": 6290
    },
    {
      "epoch": 4.519368723098996,
      "grad_norm": 4.800868511199951,
      "learning_rate": 2.2760925301424374e-06,
      "loss": 1.1488,
      "step": 6300
    },
    {
      "epoch": 4.526542324246772,
      "grad_norm": 3.9957621097564697,
      "learning_rate": 2.209355371899685e-06,
      "loss": 1.1535,
      "step": 6310
    },
    {
      "epoch": 4.533715925394548,
      "grad_norm": 4.2643914222717285,
      "learning_rate": 2.1435891177736036e-06,
      "loss": 1.2229,
      "step": 6320
    },
    {
      "epoch": 4.540889526542324,
      "grad_norm": 3.103882312774658,
      "learning_rate": 2.078795103856801e-06,
      "loss": 1.2001,
      "step": 6330
    },
    {
      "epoch": 4.5480631276901,
      "grad_norm": 4.655590534210205,
      "learning_rate": 2.014974646490059e-06,
      "loss": 1.0692,
      "step": 6340
    },
    {
      "epoch": 4.555236728837876,
      "grad_norm": 3.978806257247925,
      "learning_rate": 1.952129042235612e-06,
      "loss": 1.2326,
      "step": 6350
    },
    {
      "epoch": 4.562410329985653,
      "grad_norm": 4.086568355560303,
      "learning_rate": 1.8902595678507829e-06,
      "loss": 1.1353,
      "step": 6360
    },
    {
      "epoch": 4.569583931133429,
      "grad_norm": 3.9552853107452393,
      "learning_rate": 1.8293674802620786e-06,
      "loss": 1.0889,
      "step": 6370
    },
    {
      "epoch": 4.5767575322812055,
      "grad_norm": 3.6346330642700195,
      "learning_rate": 1.7694540165396068e-06,
      "loss": 1.114,
      "step": 6380
    },
    {
      "epoch": 4.583931133428981,
      "grad_norm": 3.5891366004943848,
      "learning_rate": 1.7105203938719894e-06,
      "loss": 1.1567,
      "step": 6390
    },
    {
      "epoch": 4.591104734576757,
      "grad_norm": 4.022536754608154,
      "learning_rate": 1.6525678095416165e-06,
      "loss": 1.2194,
      "step": 6400
    },
    {
      "epoch": 4.598278335724534,
      "grad_norm": 3.7485926151275635,
      "learning_rate": 1.5955974409002982e-06,
      "loss": 1.1637,
      "step": 6410
    },
    {
      "epoch": 4.60545193687231,
      "grad_norm": 3.6213912963867188,
      "learning_rate": 1.5396104453453907e-06,
      "loss": 1.1455,
      "step": 6420
    },
    {
      "epoch": 4.612625538020086,
      "grad_norm": 5.187356472015381,
      "learning_rate": 1.484607960296258e-06,
      "loss": 1.1308,
      "step": 6430
    },
    {
      "epoch": 4.619799139167863,
      "grad_norm": 3.8496525287628174,
      "learning_rate": 1.4305911031711584e-06,
      "loss": 1.1728,
      "step": 6440
    },
    {
      "epoch": 4.626972740315638,
      "grad_norm": 3.9992589950561523,
      "learning_rate": 1.3828195510442566e-06,
      "loss": 1.1796,
      "step": 6450
    },
    {
      "epoch": 4.634146341463414,
      "grad_norm": 3.9095611572265625,
      "learning_rate": 1.3306783937289401e-06,
      "loss": 1.1903,
      "step": 6460
    },
    {
      "epoch": 4.641319942611191,
      "grad_norm": 3.149012804031372,
      "learning_rate": 1.2795259915365343e-06,
      "loss": 1.1377,
      "step": 6470
    },
    {
      "epoch": 4.648493543758967,
      "grad_norm": 4.515384674072266,
      "learning_rate": 1.2293633836678697e-06,
      "loss": 1.1905,
      "step": 6480
    },
    {
      "epoch": 4.655667144906743,
      "grad_norm": 3.5992014408111572,
      "learning_rate": 1.1801915892153459e-06,
      "loss": 1.1641,
      "step": 6490
    },
    {
      "epoch": 4.66284074605452,
      "grad_norm": 4.231520652770996,
      "learning_rate": 1.1320116071422026e-06,
      "loss": 1.1588,
      "step": 6500
    },
    {
      "epoch": 4.6700143472022955,
      "grad_norm": 4.539124488830566,
      "learning_rate": 1.084824416262259e-06,
      "loss": 1.1192,
      "step": 6510
    },
    {
      "epoch": 4.677187948350072,
      "grad_norm": 4.305497169494629,
      "learning_rate": 1.0386309752200008e-06,
      "loss": 1.1433,
      "step": 6520
    },
    {
      "epoch": 4.684361549497848,
      "grad_norm": 3.6606502532958984,
      "learning_rate": 9.934322224711024e-07,
      "loss": 1.1922,
      "step": 6530
    },
    {
      "epoch": 4.691535150645624,
      "grad_norm": 4.960061550140381,
      "learning_rate": 9.49229076263386e-07,
      "loss": 1.1596,
      "step": 6540
    },
    {
      "epoch": 4.698708751793401,
      "grad_norm": 3.9539074897766113,
      "learning_rate": 9.060224346181479e-07,
      "loss": 1.1584,
      "step": 6550
    },
    {
      "epoch": 4.705882352941177,
      "grad_norm": 3.419041156768799,
      "learning_rate": 8.638131753119172e-07,
      "loss": 1.2047,
      "step": 6560
    },
    {
      "epoch": 4.7130559540889525,
      "grad_norm": 3.6398210525512695,
      "learning_rate": 8.226021558586316e-07,
      "loss": 1.142,
      "step": 6570
    },
    {
      "epoch": 4.720229555236729,
      "grad_norm": 4.206430435180664,
      "learning_rate": 7.823902134921957e-07,
      "loss": 1.1509,
      "step": 6580
    },
    {
      "epoch": 4.727403156384505,
      "grad_norm": 3.6498618125915527,
      "learning_rate": 7.431781651495052e-07,
      "loss": 1.1468,
      "step": 6590
    },
    {
      "epoch": 4.734576757532281,
      "grad_norm": 4.296510219573975,
      "learning_rate": 7.049668074538107e-07,
      "loss": 1.1314,
      "step": 6600
    },
    {
      "epoch": 4.741750358680058,
      "grad_norm": 4.179267883300781,
      "learning_rate": 6.677569166985754e-07,
      "loss": 1.1375,
      "step": 6610
    },
    {
      "epoch": 4.748923959827834,
      "grad_norm": 3.603591203689575,
      "learning_rate": 6.315492488316588e-07,
      "loss": 1.0923,
      "step": 6620
    },
    {
      "epoch": 4.7560975609756095,
      "grad_norm": 4.93718147277832,
      "learning_rate": 5.96344539440008e-07,
      "loss": 1.192,
      "step": 6630
    },
    {
      "epoch": 4.763271162123386,
      "grad_norm": 3.327321767807007,
      "learning_rate": 5.621435037346634e-07,
      "loss": 1.2087,
      "step": 6640
    },
    {
      "epoch": 4.770444763271162,
      "grad_norm": 4.446008682250977,
      "learning_rate": 5.289468365362815e-07,
      "loss": 1.2241,
      "step": 6650
    },
    {
      "epoch": 4.777618364418938,
      "grad_norm": 3.8823020458221436,
      "learning_rate": 4.967552122609631e-07,
      "loss": 1.1369,
      "step": 6660
    },
    {
      "epoch": 4.784791965566715,
      "grad_norm": 4.147777080535889,
      "learning_rate": 4.6556928490659737e-07,
      "loss": 1.1205,
      "step": 6670
    },
    {
      "epoch": 4.791965566714491,
      "grad_norm": 4.411613941192627,
      "learning_rate": 4.353896880395614e-07,
      "loss": 1.1283,
      "step": 6680
    },
    {
      "epoch": 4.799139167862267,
      "grad_norm": 4.086779594421387,
      "learning_rate": 4.0621703478183617e-07,
      "loss": 1.192,
      "step": 6690
    },
    {
      "epoch": 4.806312769010043,
      "grad_norm": 3.3838930130004883,
      "learning_rate": 3.780519177985498e-07,
      "loss": 1.1579,
      "step": 6700
    },
    {
      "epoch": 4.813486370157819,
      "grad_norm": 3.84722638130188,
      "learning_rate": 3.508949092859759e-07,
      "loss": 1.1376,
      "step": 6710
    },
    {
      "epoch": 4.820659971305595,
      "grad_norm": 4.336162090301514,
      "learning_rate": 3.247465609598599e-07,
      "loss": 1.1641,
      "step": 6720
    },
    {
      "epoch": 4.827833572453372,
      "grad_norm": 4.328902244567871,
      "learning_rate": 2.996074040442387e-07,
      "loss": 1.1128,
      "step": 6730
    },
    {
      "epoch": 4.835007173601148,
      "grad_norm": 3.7426934242248535,
      "learning_rate": 2.75477949260633e-07,
      "loss": 1.1823,
      "step": 6740
    },
    {
      "epoch": 4.842180774748924,
      "grad_norm": 4.942738056182861,
      "learning_rate": 2.523586868176886e-07,
      "loss": 1.1568,
      "step": 6750
    },
    {
      "epoch": 4.8493543758967,
      "grad_norm": 4.486761093139648,
      "learning_rate": 2.3025008640119584e-07,
      "loss": 1.1169,
      "step": 6760
    },
    {
      "epoch": 4.856527977044476,
      "grad_norm": 4.624928951263428,
      "learning_rate": 2.0915259716458024e-07,
      "loss": 1.1498,
      "step": 6770
    },
    {
      "epoch": 4.863701578192252,
      "grad_norm": 4.13065767288208,
      "learning_rate": 1.8906664771973782e-07,
      "loss": 1.1128,
      "step": 6780
    },
    {
      "epoch": 4.870875179340029,
      "grad_norm": 3.368609666824341,
      "learning_rate": 1.6999264612835299e-07,
      "loss": 1.1394,
      "step": 6790
    },
    {
      "epoch": 4.878048780487805,
      "grad_norm": 2.9444918632507324,
      "learning_rate": 1.5193097989360527e-07,
      "loss": 1.1926,
      "step": 6800
    },
    {
      "epoch": 4.885222381635581,
      "grad_norm": 3.3272347450256348,
      "learning_rate": 1.348820159522979e-07,
      "loss": 1.1247,
      "step": 6810
    },
    {
      "epoch": 4.892395982783357,
      "grad_norm": 4.270864486694336,
      "learning_rate": 1.1884610066738578e-07,
      "loss": 1.1796,
      "step": 6820
    },
    {
      "epoch": 4.899569583931133,
      "grad_norm": 2.6554269790649414,
      "learning_rate": 1.0382355982095915e-07,
      "loss": 1.0764,
      "step": 6830
    },
    {
      "epoch": 4.906743185078909,
      "grad_norm": 4.106497287750244,
      "learning_rate": 8.981469860763203e-08,
      "loss": 1.1755,
      "step": 6840
    },
    {
      "epoch": 4.913916786226686,
      "grad_norm": 3.3695130348205566,
      "learning_rate": 7.681980162830282e-08,
      "loss": 1.1265,
      "step": 6850
    },
    {
      "epoch": 4.921090387374462,
      "grad_norm": 3.82474946975708,
      "learning_rate": 6.483913288441446e-08,
      "loss": 1.1251,
      "step": 6860
    },
    {
      "epoch": 4.928263988522238,
      "grad_norm": 3.527655839920044,
      "learning_rate": 5.387293577257535e-08,
      "loss": 1.1474,
      "step": 6870
    },
    {
      "epoch": 4.9354375896700144,
      "grad_norm": 3.6977837085723877,
      "learning_rate": 4.392143307960783e-08,
      "loss": 1.196,
      "step": 6880
    },
    {
      "epoch": 4.94261119081779,
      "grad_norm": 4.262316703796387,
      "learning_rate": 3.4984826978029514e-08,
      "loss": 1.1149,
      "step": 6890
    },
    {
      "epoch": 4.949784791965567,
      "grad_norm": 4.185179233551025,
      "learning_rate": 2.7063299021939935e-08,
      "loss": 1.1799,
      "step": 6900
    },
    {
      "epoch": 4.956958393113343,
      "grad_norm": 4.03181266784668,
      "learning_rate": 2.015701014334015e-08,
      "loss": 1.1455,
      "step": 6910
    },
    {
      "epoch": 4.964131994261119,
      "grad_norm": 3.8499367237091064,
      "learning_rate": 1.4266100648868685e-08,
      "loss": 1.1647,
      "step": 6920
    },
    {
      "epoch": 4.971305595408896,
      "grad_norm": 3.587264060974121,
      "learning_rate": 9.390690216926068e-09,
      "loss": 1.1793,
      "step": 6930
    },
    {
      "epoch": 4.9784791965566715,
      "grad_norm": 4.409994125366211,
      "learning_rate": 5.5308778952711805e-09,
      "loss": 1.1913,
      "step": 6940
    },
    {
      "epoch": 4.985652797704447,
      "grad_norm": 2.883042097091675,
      "learning_rate": 2.6867420989895588e-09,
      "loss": 1.0942,
      "step": 6950
    },
    {
      "epoch": 4.992826398852224,
      "grad_norm": 4.393721103668213,
      "learning_rate": 8.583406089168744e-10,
      "loss": 1.1013,
      "step": 6960
    },
    {
      "epoch": 5.0,
      "grad_norm": 5.107578754425049,
      "learning_rate": 4.571057044544525e-11,
      "loss": 1.1561,
      "step": 6970
    },
    {
      "epoch": 5.0,
      "step": 6970,
      "total_flos": 2.3256068955144192e+17,
      "train_loss": 1.3242331258535727,
      "train_runtime": 1951.7153,
      "train_samples_per_second": 14.282,
      "train_steps_per_second": 3.571
    }
  ],
  "logging_steps": 10,
  "max_steps": 6970,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500.0,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": false,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.3256068955144192e+17,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
