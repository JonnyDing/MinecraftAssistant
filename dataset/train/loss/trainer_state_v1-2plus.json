{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 6970,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.007173601147776184,
      "grad_norm": 0.883919358253479,
      "learning_rate": 9.99994921055553e-06,
      "loss": 2.5958,
      "step": 10
    },
    {
      "epoch": 0.014347202295552367,
      "grad_norm": 0.9566828608512878,
      "learning_rate": 9.999796843253935e-06,
      "loss": 2.626,
      "step": 20
    },
    {
      "epoch": 0.021520803443328552,
      "grad_norm": 1.3534319400787354,
      "learning_rate": 9.999542901190683e-06,
      "loss": 2.5161,
      "step": 30
    },
    {
      "epoch": 0.028694404591104734,
      "grad_norm": 1.4687187671661377,
      "learning_rate": 9.999187389524804e-06,
      "loss": 2.5452,
      "step": 40
    },
    {
      "epoch": 0.035868005738880916,
      "grad_norm": 1.431752324104309,
      "learning_rate": 9.998730315478793e-06,
      "loss": 2.4191,
      "step": 50
    },
    {
      "epoch": 0.043041606886657105,
      "grad_norm": 1.8270663022994995,
      "learning_rate": 9.998171688338463e-06,
      "loss": 2.4909,
      "step": 60
    },
    {
      "epoch": 0.05021520803443329,
      "grad_norm": 1.4207212924957275,
      "learning_rate": 9.99751151945276e-06,
      "loss": 2.5032,
      "step": 70
    },
    {
      "epoch": 0.05738880918220947,
      "grad_norm": 1.3922983407974243,
      "learning_rate": 9.99674982223353e-06,
      "loss": 2.4248,
      "step": 80
    },
    {
      "epoch": 0.06456241032998565,
      "grad_norm": 1.770218849182129,
      "learning_rate": 9.995886612155243e-06,
      "loss": 2.2867,
      "step": 90
    },
    {
      "epoch": 0.07173601147776183,
      "grad_norm": 1.5866750478744507,
      "learning_rate": 9.994921906754683e-06,
      "loss": 2.4073,
      "step": 100
    },
    {
      "epoch": 0.07890961262553801,
      "grad_norm": 1.4005277156829834,
      "learning_rate": 9.99385572563059e-06,
      "loss": 2.2412,
      "step": 110
    },
    {
      "epoch": 0.08608321377331421,
      "grad_norm": 1.8948150873184204,
      "learning_rate": 9.992688090443263e-06,
      "loss": 2.2243,
      "step": 120
    },
    {
      "epoch": 0.09325681492109039,
      "grad_norm": 1.3965283632278442,
      "learning_rate": 9.991419024914121e-06,
      "loss": 2.2221,
      "step": 130
    },
    {
      "epoch": 0.10043041606886657,
      "grad_norm": 2.1755878925323486,
      "learning_rate": 9.990048554825214e-06,
      "loss": 2.1927,
      "step": 140
    },
    {
      "epoch": 0.10760401721664276,
      "grad_norm": 1.5753406286239624,
      "learning_rate": 9.988576708018712e-06,
      "loss": 2.1834,
      "step": 150
    },
    {
      "epoch": 0.11477761836441894,
      "grad_norm": 2.0646164417266846,
      "learning_rate": 9.987003514396323e-06,
      "loss": 2.1626,
      "step": 160
    },
    {
      "epoch": 0.12195121951219512,
      "grad_norm": 2.0096778869628906,
      "learning_rate": 9.985329005918702e-06,
      "loss": 2.0897,
      "step": 170
    },
    {
      "epoch": 0.1291248206599713,
      "grad_norm": 1.8192379474639893,
      "learning_rate": 9.983553216604792e-06,
      "loss": 2.0241,
      "step": 180
    },
    {
      "epoch": 0.13629842180774748,
      "grad_norm": 2.0981667041778564,
      "learning_rate": 9.981676182531132e-06,
      "loss": 2.0304,
      "step": 190
    },
    {
      "epoch": 0.14347202295552366,
      "grad_norm": 1.9760040044784546,
      "learning_rate": 9.979697941831129e-06,
      "loss": 1.9215,
      "step": 200
    },
    {
      "epoch": 0.15064562410329985,
      "grad_norm": 1.8339335918426514,
      "learning_rate": 9.977618534694283e-06,
      "loss": 1.9648,
      "step": 210
    },
    {
      "epoch": 0.15781922525107603,
      "grad_norm": 2.0320370197296143,
      "learning_rate": 9.975438003365365e-06,
      "loss": 1.9913,
      "step": 220
    },
    {
      "epoch": 0.1649928263988522,
      "grad_norm": 1.494991660118103,
      "learning_rate": 9.973156392143569e-06,
      "loss": 1.9515,
      "step": 230
    },
    {
      "epoch": 0.17216642754662842,
      "grad_norm": 1.5304771661758423,
      "learning_rate": 9.970773747381597e-06,
      "loss": 1.9571,
      "step": 240
    },
    {
      "epoch": 0.1793400286944046,
      "grad_norm": 1.65315580368042,
      "learning_rate": 9.968290117484732e-06,
      "loss": 1.9396,
      "step": 250
    },
    {
      "epoch": 0.18651362984218078,
      "grad_norm": 1.727305293083191,
      "learning_rate": 9.96570555290985e-06,
      "loss": 1.9103,
      "step": 260
    },
    {
      "epoch": 0.19368723098995697,
      "grad_norm": 2.2755842208862305,
      "learning_rate": 9.963020106164386e-06,
      "loss": 1.9308,
      "step": 270
    },
    {
      "epoch": 0.20086083213773315,
      "grad_norm": 2.323488235473633,
      "learning_rate": 9.960233831805284e-06,
      "loss": 1.9151,
      "step": 280
    },
    {
      "epoch": 0.20803443328550933,
      "grad_norm": 2.4849750995635986,
      "learning_rate": 9.95734678643787e-06,
      "loss": 1.9522,
      "step": 290
    },
    {
      "epoch": 0.2152080344332855,
      "grad_norm": 2.0995020866394043,
      "learning_rate": 9.95435902871472e-06,
      "loss": 1.8415,
      "step": 300
    },
    {
      "epoch": 0.2223816355810617,
      "grad_norm": 2.1106300354003906,
      "learning_rate": 9.951270619334454e-06,
      "loss": 1.8259,
      "step": 310
    },
    {
      "epoch": 0.22955523672883787,
      "grad_norm": 2.4355764389038086,
      "learning_rate": 9.94808162104051e-06,
      "loss": 1.8944,
      "step": 320
    },
    {
      "epoch": 0.23672883787661406,
      "grad_norm": 1.9372035264968872,
      "learning_rate": 9.944792098619871e-06,
      "loss": 1.9294,
      "step": 330
    },
    {
      "epoch": 0.24390243902439024,
      "grad_norm": 2.9297828674316406,
      "learning_rate": 9.941402118901743e-06,
      "loss": 1.8761,
      "step": 340
    },
    {
      "epoch": 0.25107604017216645,
      "grad_norm": 2.4037182331085205,
      "learning_rate": 9.9379117507562e-06,
      "loss": 1.8563,
      "step": 350
    },
    {
      "epoch": 0.2582496413199426,
      "grad_norm": 1.9286789894104004,
      "learning_rate": 9.934321065092786e-06,
      "loss": 1.9231,
      "step": 360
    },
    {
      "epoch": 0.2654232424677188,
      "grad_norm": 2.064483404159546,
      "learning_rate": 9.930630134859071e-06,
      "loss": 1.8318,
      "step": 370
    },
    {
      "epoch": 0.27259684361549497,
      "grad_norm": 2.1706879138946533,
      "learning_rate": 9.926839035039178e-06,
      "loss": 1.7919,
      "step": 380
    },
    {
      "epoch": 0.2797704447632712,
      "grad_norm": 1.7651305198669434,
      "learning_rate": 9.922947842652243e-06,
      "loss": 1.8534,
      "step": 390
    },
    {
      "epoch": 0.28694404591104733,
      "grad_norm": 2.572965383529663,
      "learning_rate": 9.91895663675087e-06,
      "loss": 1.8338,
      "step": 400
    },
    {
      "epoch": 0.29411764705882354,
      "grad_norm": 2.0386295318603516,
      "learning_rate": 9.915279106859131e-06,
      "loss": 1.8046,
      "step": 410
    },
    {
      "epoch": 0.3012912482065997,
      "grad_norm": 2.2829933166503906,
      "learning_rate": 9.911098100345988e-06,
      "loss": 1.8423,
      "step": 420
    },
    {
      "epoch": 0.3084648493543759,
      "grad_norm": 2.22885799407959,
      "learning_rate": 9.906817321055138e-06,
      "loss": 1.7861,
      "step": 430
    },
    {
      "epoch": 0.31563845050215206,
      "grad_norm": 2.344467878341675,
      "learning_rate": 9.902436855953941e-06,
      "loss": 1.782,
      "step": 440
    },
    {
      "epoch": 0.32281205164992827,
      "grad_norm": 2.4786338806152344,
      "learning_rate": 9.89795679403495e-06,
      "loss": 1.8358,
      "step": 450
    },
    {
      "epoch": 0.3299856527977044,
      "grad_norm": 2.3942737579345703,
      "learning_rate": 9.893377226314113e-06,
      "loss": 1.7957,
      "step": 460
    },
    {
      "epoch": 0.33715925394548063,
      "grad_norm": 2.2589478492736816,
      "learning_rate": 9.888698245828906e-06,
      "loss": 1.8151,
      "step": 470
    },
    {
      "epoch": 0.34433285509325684,
      "grad_norm": 2.381671905517578,
      "learning_rate": 9.883919947636457e-06,
      "loss": 1.8891,
      "step": 480
    },
    {
      "epoch": 0.351506456241033,
      "grad_norm": 2.56766939163208,
      "learning_rate": 9.879042428811613e-06,
      "loss": 1.8744,
      "step": 490
    },
    {
      "epoch": 0.3586800573888092,
      "grad_norm": 2.2738685607910156,
      "learning_rate": 9.874065788444963e-06,
      "loss": 1.7708,
      "step": 500
    },
    {
      "epoch": 0.36585365853658536,
      "grad_norm": 2.1692593097686768,
      "learning_rate": 9.86899012764082e-06,
      "loss": 1.7835,
      "step": 510
    },
    {
      "epoch": 0.37302725968436157,
      "grad_norm": 2.2210967540740967,
      "learning_rate": 9.86381554951519e-06,
      "loss": 1.7379,
      "step": 520
    },
    {
      "epoch": 0.3802008608321377,
      "grad_norm": 2.3920814990997314,
      "learning_rate": 9.858542159193646e-06,
      "loss": 1.7616,
      "step": 530
    },
    {
      "epoch": 0.38737446197991393,
      "grad_norm": 2.584855794906616,
      "learning_rate": 9.853170063809217e-06,
      "loss": 1.8169,
      "step": 540
    },
    {
      "epoch": 0.3945480631276901,
      "grad_norm": 2.6481025218963623,
      "learning_rate": 9.847699372500197e-06,
      "loss": 1.6874,
      "step": 550
    },
    {
      "epoch": 0.4017216642754663,
      "grad_norm": 2.4585766792297363,
      "learning_rate": 9.842130196407937e-06,
      "loss": 1.7984,
      "step": 560
    },
    {
      "epoch": 0.40889526542324245,
      "grad_norm": 2.9714927673339844,
      "learning_rate": 9.836462648674581e-06,
      "loss": 1.7708,
      "step": 570
    },
    {
      "epoch": 0.41606886657101866,
      "grad_norm": 2.300363302230835,
      "learning_rate": 9.830696844440767e-06,
      "loss": 1.7861,
      "step": 580
    },
    {
      "epoch": 0.4232424677187948,
      "grad_norm": 2.3597586154937744,
      "learning_rate": 9.824832900843296e-06,
      "loss": 1.8437,
      "step": 590
    },
    {
      "epoch": 0.430416068866571,
      "grad_norm": 2.1707215309143066,
      "learning_rate": 9.818870937012739e-06,
      "loss": 1.7803,
      "step": 600
    },
    {
      "epoch": 0.4375896700143472,
      "grad_norm": 2.350713014602661,
      "learning_rate": 9.812811074071031e-06,
      "loss": 1.7691,
      "step": 610
    },
    {
      "epoch": 0.4447632711621234,
      "grad_norm": 2.727660894393921,
      "learning_rate": 9.806653435129003e-06,
      "loss": 1.7649,
      "step": 620
    },
    {
      "epoch": 0.4519368723098996,
      "grad_norm": 3.1337509155273438,
      "learning_rate": 9.800398145283874e-06,
      "loss": 1.8257,
      "step": 630
    },
    {
      "epoch": 0.45911047345767575,
      "grad_norm": 2.6467082500457764,
      "learning_rate": 9.794045331616727e-06,
      "loss": 1.7394,
      "step": 640
    },
    {
      "epoch": 0.46628407460545196,
      "grad_norm": 2.691652774810791,
      "learning_rate": 9.787595123189911e-06,
      "loss": 1.741,
      "step": 650
    },
    {
      "epoch": 0.4734576757532281,
      "grad_norm": 2.6426572799682617,
      "learning_rate": 9.781047651044428e-06,
      "loss": 1.7916,
      "step": 660
    },
    {
      "epoch": 0.4806312769010043,
      "grad_norm": 4.745182037353516,
      "learning_rate": 9.774403048197266e-06,
      "loss": 1.7573,
      "step": 670
    },
    {
      "epoch": 0.4878048780487805,
      "grad_norm": 2.6733222007751465,
      "learning_rate": 9.767661449638702e-06,
      "loss": 1.7052,
      "step": 680
    },
    {
      "epoch": 0.4949784791965567,
      "grad_norm": 2.8778741359710693,
      "learning_rate": 9.76082299232955e-06,
      "loss": 1.8133,
      "step": 690
    },
    {
      "epoch": 0.5021520803443329,
      "grad_norm": 2.809638500213623,
      "learning_rate": 9.753887815198392e-06,
      "loss": 1.7654,
      "step": 700
    },
    {
      "epoch": 0.509325681492109,
      "grad_norm": 2.7456748485565186,
      "learning_rate": 9.746856059138749e-06,
      "loss": 1.7536,
      "step": 710
    },
    {
      "epoch": 0.5164992826398852,
      "grad_norm": 2.667546510696411,
      "learning_rate": 9.739727867006208e-06,
      "loss": 1.8147,
      "step": 720
    },
    {
      "epoch": 0.5236728837876614,
      "grad_norm": 2.4356415271759033,
      "learning_rate": 9.73250338361554e-06,
      "loss": 1.7561,
      "step": 730
    },
    {
      "epoch": 0.5308464849354376,
      "grad_norm": 2.6215975284576416,
      "learning_rate": 9.725182755737744e-06,
      "loss": 1.8046,
      "step": 740
    },
    {
      "epoch": 0.5380200860832137,
      "grad_norm": 2.669119119644165,
      "learning_rate": 9.717766132097069e-06,
      "loss": 1.72,
      "step": 750
    },
    {
      "epoch": 0.5451936872309899,
      "grad_norm": 2.859308958053589,
      "learning_rate": 9.710253663367992e-06,
      "loss": 1.7198,
      "step": 760
    },
    {
      "epoch": 0.5523672883787661,
      "grad_norm": 2.95778226852417,
      "learning_rate": 9.70264550217216e-06,
      "loss": 1.6991,
      "step": 770
    },
    {
      "epoch": 0.5595408895265424,
      "grad_norm": 3.350583553314209,
      "learning_rate": 9.694941803075285e-06,
      "loss": 1.8174,
      "step": 780
    },
    {
      "epoch": 0.5667144906743186,
      "grad_norm": 2.644071578979492,
      "learning_rate": 9.687142722584003e-06,
      "loss": 1.7492,
      "step": 790
    },
    {
      "epoch": 0.5738880918220947,
      "grad_norm": 2.3962292671203613,
      "learning_rate": 9.679248419142704e-06,
      "loss": 1.7348,
      "step": 800
    },
    {
      "epoch": 0.5810616929698709,
      "grad_norm": 2.9848179817199707,
      "learning_rate": 9.6712590531303e-06,
      "loss": 1.7114,
      "step": 810
    },
    {
      "epoch": 0.5882352941176471,
      "grad_norm": 2.723771572113037,
      "learning_rate": 9.663174786856977e-06,
      "loss": 1.7105,
      "step": 820
    },
    {
      "epoch": 0.5954088952654233,
      "grad_norm": 2.769979476928711,
      "learning_rate": 9.654995784560892e-06,
      "loss": 1.7747,
      "step": 830
    },
    {
      "epoch": 0.6025824964131994,
      "grad_norm": 2.5553808212280273,
      "learning_rate": 9.646722212404837e-06,
      "loss": 1.7052,
      "step": 840
    },
    {
      "epoch": 0.6097560975609756,
      "grad_norm": 2.626511812210083,
      "learning_rate": 9.638354238472868e-06,
      "loss": 1.6592,
      "step": 850
    },
    {
      "epoch": 0.6169296987087518,
      "grad_norm": 2.7082104682922363,
      "learning_rate": 9.629892032766882e-06,
      "loss": 1.7901,
      "step": 860
    },
    {
      "epoch": 0.624103299856528,
      "grad_norm": 3.334251642227173,
      "learning_rate": 9.62133576720317e-06,
      "loss": 1.7483,
      "step": 870
    },
    {
      "epoch": 0.6312769010043041,
      "grad_norm": 2.9557251930236816,
      "learning_rate": 9.612685615608922e-06,
      "loss": 1.7339,
      "step": 880
    },
    {
      "epoch": 0.6384505021520803,
      "grad_norm": 2.979186534881592,
      "learning_rate": 9.603941753718695e-06,
      "loss": 1.734,
      "step": 890
    },
    {
      "epoch": 0.6456241032998565,
      "grad_norm": 2.948289155960083,
      "learning_rate": 9.595104359170847e-06,
      "loss": 1.7656,
      "step": 900
    },
    {
      "epoch": 0.6527977044476327,
      "grad_norm": 3.3162829875946045,
      "learning_rate": 9.586173611503918e-06,
      "loss": 1.6637,
      "step": 910
    },
    {
      "epoch": 0.6599713055954088,
      "grad_norm": 3.1352052688598633,
      "learning_rate": 9.577149692152994e-06,
      "loss": 1.8036,
      "step": 920
    },
    {
      "epoch": 0.667144906743185,
      "grad_norm": 2.7304587364196777,
      "learning_rate": 9.568032784446018e-06,
      "loss": 1.6778,
      "step": 930
    },
    {
      "epoch": 0.6743185078909613,
      "grad_norm": 3.0460550785064697,
      "learning_rate": 9.558823073600057e-06,
      "loss": 1.7054,
      "step": 940
    },
    {
      "epoch": 0.6814921090387375,
      "grad_norm": 2.9644906520843506,
      "learning_rate": 9.549520746717553e-06,
      "loss": 1.7384,
      "step": 950
    },
    {
      "epoch": 0.6886657101865137,
      "grad_norm": 2.751709222793579,
      "learning_rate": 9.540125992782512e-06,
      "loss": 1.6609,
      "step": 960
    },
    {
      "epoch": 0.6958393113342898,
      "grad_norm": 2.3526322841644287,
      "learning_rate": 9.530639002656665e-06,
      "loss": 1.7084,
      "step": 970
    },
    {
      "epoch": 0.703012912482066,
      "grad_norm": 2.8505704402923584,
      "learning_rate": 9.5210599690756e-06,
      "loss": 1.7627,
      "step": 980
    },
    {
      "epoch": 0.7101865136298422,
      "grad_norm": 3.285496950149536,
      "learning_rate": 9.511389086644828e-06,
      "loss": 1.7098,
      "step": 990
    },
    {
      "epoch": 0.7173601147776184,
      "grad_norm": 2.9181559085845947,
      "learning_rate": 9.501626551835851e-06,
      "loss": 1.6353,
      "step": 1000
    },
    {
      "epoch": 0.7245337159253945,
      "grad_norm": 2.6509344577789307,
      "learning_rate": 9.491772562982159e-06,
      "loss": 1.7025,
      "step": 1010
    },
    {
      "epoch": 0.7317073170731707,
      "grad_norm": 2.5296335220336914,
      "learning_rate": 9.481827320275197e-06,
      "loss": 1.6832,
      "step": 1020
    },
    {
      "epoch": 0.7388809182209469,
      "grad_norm": 2.748121976852417,
      "learning_rate": 9.471791025760307e-06,
      "loss": 1.7277,
      "step": 1030
    },
    {
      "epoch": 0.7460545193687231,
      "grad_norm": 3.56577467918396,
      "learning_rate": 9.461663883332615e-06,
      "loss": 1.6876,
      "step": 1040
    },
    {
      "epoch": 0.7532281205164992,
      "grad_norm": 2.894422769546509,
      "learning_rate": 9.451446098732901e-06,
      "loss": 1.6776,
      "step": 1050
    },
    {
      "epoch": 0.7604017216642754,
      "grad_norm": 2.822057008743286,
      "learning_rate": 9.441137879543405e-06,
      "loss": 1.7149,
      "step": 1060
    },
    {
      "epoch": 0.7675753228120517,
      "grad_norm": 2.7124316692352295,
      "learning_rate": 9.430739435183615e-06,
      "loss": 1.6945,
      "step": 1070
    },
    {
      "epoch": 0.7747489239598279,
      "grad_norm": 2.962575912475586,
      "learning_rate": 9.420250976906017e-06,
      "loss": 1.713,
      "step": 1080
    },
    {
      "epoch": 0.7819225251076041,
      "grad_norm": 2.7682173252105713,
      "learning_rate": 9.409672717791802e-06,
      "loss": 1.8023,
      "step": 1090
    },
    {
      "epoch": 0.7890961262553802,
      "grad_norm": 3.0816893577575684,
      "learning_rate": 9.399004872746528e-06,
      "loss": 1.6935,
      "step": 1100
    },
    {
      "epoch": 0.7962697274031564,
      "grad_norm": 3.2089383602142334,
      "learning_rate": 9.388247658495766e-06,
      "loss": 1.6915,
      "step": 1110
    },
    {
      "epoch": 0.8034433285509326,
      "grad_norm": 2.8749632835388184,
      "learning_rate": 9.37740129358069e-06,
      "loss": 1.6839,
      "step": 1120
    },
    {
      "epoch": 0.8106169296987088,
      "grad_norm": 3.0901451110839844,
      "learning_rate": 9.366465998353639e-06,
      "loss": 1.6392,
      "step": 1130
    },
    {
      "epoch": 0.8177905308464849,
      "grad_norm": 3.271280527114868,
      "learning_rate": 9.355441994973639e-06,
      "loss": 1.6704,
      "step": 1140
    },
    {
      "epoch": 0.8249641319942611,
      "grad_norm": 2.967210054397583,
      "learning_rate": 9.344329507401898e-06,
      "loss": 1.596,
      "step": 1150
    },
    {
      "epoch": 0.8321377331420373,
      "grad_norm": 3.4978082180023193,
      "learning_rate": 9.33312876139724e-06,
      "loss": 1.7361,
      "step": 1160
    },
    {
      "epoch": 0.8393113342898135,
      "grad_norm": 2.9873270988464355,
      "learning_rate": 9.321839984511534e-06,
      "loss": 1.6786,
      "step": 1170
    },
    {
      "epoch": 0.8464849354375896,
      "grad_norm": 2.5278329849243164,
      "learning_rate": 9.310463406085061e-06,
      "loss": 1.6985,
      "step": 1180
    },
    {
      "epoch": 0.8536585365853658,
      "grad_norm": 3.353712320327759,
      "learning_rate": 9.298999257241862e-06,
      "loss": 1.6962,
      "step": 1190
    },
    {
      "epoch": 0.860832137733142,
      "grad_norm": 3.2279422283172607,
      "learning_rate": 9.287447770885038e-06,
      "loss": 1.6509,
      "step": 1200
    },
    {
      "epoch": 0.8680057388809183,
      "grad_norm": 3.2630465030670166,
      "learning_rate": 9.275809181692016e-06,
      "loss": 1.7244,
      "step": 1210
    },
    {
      "epoch": 0.8751793400286944,
      "grad_norm": 3.270476818084717,
      "learning_rate": 9.264083726109789e-06,
      "loss": 1.6997,
      "step": 1220
    },
    {
      "epoch": 0.8823529411764706,
      "grad_norm": 2.633563280105591,
      "learning_rate": 9.25227164235011e-06,
      "loss": 1.694,
      "step": 1230
    },
    {
      "epoch": 0.8895265423242468,
      "grad_norm": 3.3755722045898438,
      "learning_rate": 9.240373170384643e-06,
      "loss": 1.6914,
      "step": 1240
    },
    {
      "epoch": 0.896700143472023,
      "grad_norm": 3.3245837688446045,
      "learning_rate": 9.228388551940104e-06,
      "loss": 1.7158,
      "step": 1250
    },
    {
      "epoch": 0.9038737446197992,
      "grad_norm": 2.925102949142456,
      "learning_rate": 9.216318030493338e-06,
      "loss": 1.7887,
      "step": 1260
    },
    {
      "epoch": 0.9110473457675753,
      "grad_norm": 3.2025563716888428,
      "learning_rate": 9.204161851266376e-06,
      "loss": 1.7174,
      "step": 1270
    },
    {
      "epoch": 0.9182209469153515,
      "grad_norm": 3.179119110107422,
      "learning_rate": 9.191920261221451e-06,
      "loss": 1.8188,
      "step": 1280
    },
    {
      "epoch": 0.9253945480631277,
      "grad_norm": 3.7679569721221924,
      "learning_rate": 9.17959350905599e-06,
      "loss": 1.6999,
      "step": 1290
    },
    {
      "epoch": 0.9325681492109039,
      "grad_norm": 3.2679715156555176,
      "learning_rate": 9.16718184519755e-06,
      "loss": 1.6749,
      "step": 1300
    },
    {
      "epoch": 0.93974175035868,
      "grad_norm": 3.3203465938568115,
      "learning_rate": 9.154685521798736e-06,
      "loss": 1.6801,
      "step": 1310
    },
    {
      "epoch": 0.9469153515064562,
      "grad_norm": 3.1652066707611084,
      "learning_rate": 9.142104792732078e-06,
      "loss": 1.7029,
      "step": 1320
    },
    {
      "epoch": 0.9540889526542324,
      "grad_norm": 3.0842859745025635,
      "learning_rate": 9.129439913584869e-06,
      "loss": 1.6109,
      "step": 1330
    },
    {
      "epoch": 0.9612625538020086,
      "grad_norm": 3.1463284492492676,
      "learning_rate": 9.11669114165398e-06,
      "loss": 1.7158,
      "step": 1340
    },
    {
      "epoch": 0.9684361549497847,
      "grad_norm": 3.3286545276641846,
      "learning_rate": 9.103858735940635e-06,
      "loss": 1.6259,
      "step": 1350
    },
    {
      "epoch": 0.975609756097561,
      "grad_norm": 2.822491407394409,
      "learning_rate": 9.090942957145131e-06,
      "loss": 1.642,
      "step": 1360
    },
    {
      "epoch": 0.9827833572453372,
      "grad_norm": 3.614206552505493,
      "learning_rate": 9.07794406766156e-06,
      "loss": 1.6462,
      "step": 1370
    },
    {
      "epoch": 0.9899569583931134,
      "grad_norm": 2.968879461288452,
      "learning_rate": 9.064862331572472e-06,
      "loss": 1.728,
      "step": 1380
    },
    {
      "epoch": 0.9971305595408895,
      "grad_norm": 3.642909526824951,
      "learning_rate": 9.051698014643514e-06,
      "loss": 1.7161,
      "step": 1390
    },
    {
      "epoch": 1.0043041606886658,
      "grad_norm": 3.433839797973633,
      "learning_rate": 9.038451384318019e-06,
      "loss": 1.6951,
      "step": 1400
    },
    {
      "epoch": 1.011477761836442,
      "grad_norm": 2.9540271759033203,
      "learning_rate": 9.02512270971159e-06,
      "loss": 1.6391,
      "step": 1410
    },
    {
      "epoch": 1.018651362984218,
      "grad_norm": 3.5939834117889404,
      "learning_rate": 9.011712261606615e-06,
      "loss": 1.5985,
      "step": 1420
    },
    {
      "epoch": 1.0258249641319943,
      "grad_norm": 3.4749510288238525,
      "learning_rate": 8.998220312446779e-06,
      "loss": 1.7045,
      "step": 1430
    },
    {
      "epoch": 1.0329985652797704,
      "grad_norm": 3.2292141914367676,
      "learning_rate": 8.984647136331524e-06,
      "loss": 1.6494,
      "step": 1440
    },
    {
      "epoch": 1.0401721664275467,
      "grad_norm": 4.48723030090332,
      "learning_rate": 8.970993009010476e-06,
      "loss": 1.6665,
      "step": 1450
    },
    {
      "epoch": 1.0473457675753228,
      "grad_norm": 3.515862464904785,
      "learning_rate": 8.957258207877855e-06,
      "loss": 1.6159,
      "step": 1460
    },
    {
      "epoch": 1.054519368723099,
      "grad_norm": 3.5689125061035156,
      "learning_rate": 8.94344301196683e-06,
      "loss": 1.6943,
      "step": 1470
    },
    {
      "epoch": 1.0616929698708752,
      "grad_norm": 3.6505086421966553,
      "learning_rate": 8.929547701943849e-06,
      "loss": 1.6901,
      "step": 1480
    },
    {
      "epoch": 1.0688665710186513,
      "grad_norm": 3.109992265701294,
      "learning_rate": 8.915572560102942e-06,
      "loss": 1.6558,
      "step": 1490
    },
    {
      "epoch": 1.0760401721664274,
      "grad_norm": 3.528444766998291,
      "learning_rate": 8.901517870359987e-06,
      "loss": 1.6125,
      "step": 1500
    },
    {
      "epoch": 1.0832137733142038,
      "grad_norm": 3.21643328666687,
      "learning_rate": 8.887383918246936e-06,
      "loss": 1.6188,
      "step": 1510
    },
    {
      "epoch": 1.0903873744619799,
      "grad_norm": 4.036917686462402,
      "learning_rate": 8.873170990906021e-06,
      "loss": 1.719,
      "step": 1520
    },
    {
      "epoch": 1.0975609756097562,
      "grad_norm": 3.333281993865967,
      "learning_rate": 8.858879377083915e-06,
      "loss": 1.612,
      "step": 1530
    },
    {
      "epoch": 1.1047345767575323,
      "grad_norm": 3.405703067779541,
      "learning_rate": 8.844509367125868e-06,
      "loss": 1.6225,
      "step": 1540
    },
    {
      "epoch": 1.1119081779053084,
      "grad_norm": 3.9551010131835938,
      "learning_rate": 8.830061252969812e-06,
      "loss": 1.5942,
      "step": 1550
    },
    {
      "epoch": 1.1190817790530847,
      "grad_norm": 3.117313861846924,
      "learning_rate": 8.81553532814042e-06,
      "loss": 1.652,
      "step": 1560
    },
    {
      "epoch": 1.1262553802008608,
      "grad_norm": 3.1222569942474365,
      "learning_rate": 8.800931887743154e-06,
      "loss": 1.6576,
      "step": 1570
    },
    {
      "epoch": 1.133428981348637,
      "grad_norm": 3.2730636596679688,
      "learning_rate": 8.786251228458264e-06,
      "loss": 1.6277,
      "step": 1580
    },
    {
      "epoch": 1.1406025824964132,
      "grad_norm": 3.2030446529388428,
      "learning_rate": 8.771493648534765e-06,
      "loss": 1.633,
      "step": 1590
    },
    {
      "epoch": 1.1477761836441893,
      "grad_norm": 3.210270404815674,
      "learning_rate": 8.756659447784367e-06,
      "loss": 1.6102,
      "step": 1600
    },
    {
      "epoch": 1.1549497847919656,
      "grad_norm": 2.3783247470855713,
      "learning_rate": 8.741748927575399e-06,
      "loss": 1.7147,
      "step": 1610
    },
    {
      "epoch": 1.1621233859397417,
      "grad_norm": 3.368567705154419,
      "learning_rate": 8.726762390826674e-06,
      "loss": 1.6428,
      "step": 1620
    },
    {
      "epoch": 1.169296987087518,
      "grad_norm": 3.7670886516571045,
      "learning_rate": 8.711700142001345e-06,
      "loss": 1.6695,
      "step": 1630
    },
    {
      "epoch": 1.1764705882352942,
      "grad_norm": 3.1495141983032227,
      "learning_rate": 8.69656248710071e-06,
      "loss": 1.6736,
      "step": 1640
    },
    {
      "epoch": 1.1836441893830703,
      "grad_norm": 3.33394455909729,
      "learning_rate": 8.681349733658002e-06,
      "loss": 1.6201,
      "step": 1650
    },
    {
      "epoch": 1.1908177905308466,
      "grad_norm": 2.952537775039673,
      "learning_rate": 8.66606219073214e-06,
      "loss": 1.6234,
      "step": 1660
    },
    {
      "epoch": 1.1979913916786227,
      "grad_norm": 3.510659694671631,
      "learning_rate": 8.650700168901453e-06,
      "loss": 1.7119,
      "step": 1670
    },
    {
      "epoch": 1.2051649928263988,
      "grad_norm": 3.897096872329712,
      "learning_rate": 8.635263980257356e-06,
      "loss": 1.68,
      "step": 1680
    },
    {
      "epoch": 1.212338593974175,
      "grad_norm": 2.8592050075531006,
      "learning_rate": 8.619753938398034e-06,
      "loss": 1.6651,
      "step": 1690
    },
    {
      "epoch": 1.2195121951219512,
      "grad_norm": 3.3112127780914307,
      "learning_rate": 8.604170358422045e-06,
      "loss": 1.6135,
      "step": 1700
    },
    {
      "epoch": 1.2266857962697273,
      "grad_norm": 3.5553975105285645,
      "learning_rate": 8.58851355692194e-06,
      "loss": 1.7021,
      "step": 1710
    },
    {
      "epoch": 1.2338593974175036,
      "grad_norm": 3.439347267150879,
      "learning_rate": 8.572783851977821e-06,
      "loss": 1.6413,
      "step": 1720
    },
    {
      "epoch": 1.2410329985652797,
      "grad_norm": 5.110249042510986,
      "learning_rate": 8.556981563150874e-06,
      "loss": 1.6292,
      "step": 1730
    },
    {
      "epoch": 1.248206599713056,
      "grad_norm": 4.343703269958496,
      "learning_rate": 8.541107011476891e-06,
      "loss": 1.6322,
      "step": 1740
    },
    {
      "epoch": 1.2553802008608321,
      "grad_norm": 3.332969903945923,
      "learning_rate": 8.525160519459735e-06,
      "loss": 1.5929,
      "step": 1750
    },
    {
      "epoch": 1.2625538020086085,
      "grad_norm": 3.9976441860198975,
      "learning_rate": 8.509142411064792e-06,
      "loss": 1.7105,
      "step": 1760
    },
    {
      "epoch": 1.2697274031563845,
      "grad_norm": 4.224552154541016,
      "learning_rate": 8.493053011712397e-06,
      "loss": 1.6895,
      "step": 1770
    },
    {
      "epoch": 1.2769010043041606,
      "grad_norm": 3.7799384593963623,
      "learning_rate": 8.47689264827121e-06,
      "loss": 1.6742,
      "step": 1780
    },
    {
      "epoch": 1.284074605451937,
      "grad_norm": 3.69478178024292,
      "learning_rate": 8.460661649051583e-06,
      "loss": 1.6185,
      "step": 1790
    },
    {
      "epoch": 1.291248206599713,
      "grad_norm": 2.942312717437744,
      "learning_rate": 8.444360343798892e-06,
      "loss": 1.6865,
      "step": 1800
    },
    {
      "epoch": 1.2984218077474892,
      "grad_norm": 4.33381986618042,
      "learning_rate": 8.427989063686828e-06,
      "loss": 1.5841,
      "step": 1810
    },
    {
      "epoch": 1.3055954088952655,
      "grad_norm": 3.7636489868164062,
      "learning_rate": 8.411548141310683e-06,
      "loss": 1.621,
      "step": 1820
    },
    {
      "epoch": 1.3127690100430416,
      "grad_norm": 3.5813307762145996,
      "learning_rate": 8.395037910680581e-06,
      "loss": 1.614,
      "step": 1830
    },
    {
      "epoch": 1.3199426111908177,
      "grad_norm": 3.8901021480560303,
      "learning_rate": 8.3784587072147e-06,
      "loss": 1.6187,
      "step": 1840
    },
    {
      "epoch": 1.327116212338594,
      "grad_norm": 3.680753469467163,
      "learning_rate": 8.361810867732455e-06,
      "loss": 1.6531,
      "step": 1850
    },
    {
      "epoch": 1.33428981348637,
      "grad_norm": 3.7308337688446045,
      "learning_rate": 8.345094730447649e-06,
      "loss": 1.6614,
      "step": 1860
    },
    {
      "epoch": 1.3414634146341464,
      "grad_norm": 3.5293796062469482,
      "learning_rate": 8.328310634961617e-06,
      "loss": 1.5698,
      "step": 1870
    },
    {
      "epoch": 1.3486370157819225,
      "grad_norm": 3.8343796730041504,
      "learning_rate": 8.311458922256309e-06,
      "loss": 1.7043,
      "step": 1880
    },
    {
      "epoch": 1.3558106169296988,
      "grad_norm": 3.468930959701538,
      "learning_rate": 8.29453993468738e-06,
      "loss": 1.6453,
      "step": 1890
    },
    {
      "epoch": 1.362984218077475,
      "grad_norm": 3.7263400554656982,
      "learning_rate": 8.277554015977221e-06,
      "loss": 1.7164,
      "step": 1900
    },
    {
      "epoch": 1.370157819225251,
      "grad_norm": 3.9708962440490723,
      "learning_rate": 8.26050151120798e-06,
      "loss": 1.6617,
      "step": 1910
    },
    {
      "epoch": 1.3773314203730274,
      "grad_norm": 3.470851421356201,
      "learning_rate": 8.243382766814555e-06,
      "loss": 1.7053,
      "step": 1920
    },
    {
      "epoch": 1.3845050215208035,
      "grad_norm": 3.4313783645629883,
      "learning_rate": 8.226198130577556e-06,
      "loss": 1.6349,
      "step": 1930
    },
    {
      "epoch": 1.3916786226685796,
      "grad_norm": 3.7953200340270996,
      "learning_rate": 8.208947951616231e-06,
      "loss": 1.6596,
      "step": 1940
    },
    {
      "epoch": 1.3988522238163559,
      "grad_norm": 3.9742777347564697,
      "learning_rate": 8.191632580381384e-06,
      "loss": 1.6417,
      "step": 1950
    },
    {
      "epoch": 1.406025824964132,
      "grad_norm": 3.7300024032592773,
      "learning_rate": 8.174252368648249e-06,
      "loss": 1.6353,
      "step": 1960
    },
    {
      "epoch": 1.413199426111908,
      "grad_norm": 4.186769008636475,
      "learning_rate": 8.156807669509346e-06,
      "loss": 1.6478,
      "step": 1970
    },
    {
      "epoch": 1.4203730272596844,
      "grad_norm": 4.020175933837891,
      "learning_rate": 8.139298837367305e-06,
      "loss": 1.5216,
      "step": 1980
    },
    {
      "epoch": 1.4275466284074605,
      "grad_norm": 4.515802383422852,
      "learning_rate": 8.12172622792767e-06,
      "loss": 1.7143,
      "step": 1990
    },
    {
      "epoch": 1.4347202295552366,
      "grad_norm": 3.7551658153533936,
      "learning_rate": 8.10409019819167e-06,
      "loss": 1.6109,
      "step": 2000
    },
    {
      "epoch": 1.441893830703013,
      "grad_norm": 3.6938462257385254,
      "learning_rate": 8.086391106448965e-06,
      "loss": 1.7564,
      "step": 2010
    },
    {
      "epoch": 1.4490674318507892,
      "grad_norm": 3.426755905151367,
      "learning_rate": 8.068629312270372e-06,
      "loss": 1.5833,
      "step": 2020
    },
    {
      "epoch": 1.4562410329985653,
      "grad_norm": 3.6759064197540283,
      "learning_rate": 8.050805176500554e-06,
      "loss": 1.629,
      "step": 2030
    },
    {
      "epoch": 1.4634146341463414,
      "grad_norm": 4.7514166831970215,
      "learning_rate": 8.03291906125069e-06,
      "loss": 1.77,
      "step": 2040
    },
    {
      "epoch": 1.4705882352941178,
      "grad_norm": 3.8634610176086426,
      "learning_rate": 8.014971329891126e-06,
      "loss": 1.7222,
      "step": 2050
    },
    {
      "epoch": 1.4777618364418939,
      "grad_norm": 3.7637624740600586,
      "learning_rate": 7.996962347043982e-06,
      "loss": 1.6729,
      "step": 2060
    },
    {
      "epoch": 1.48493543758967,
      "grad_norm": 3.7477571964263916,
      "learning_rate": 7.978892478575752e-06,
      "loss": 1.604,
      "step": 2070
    },
    {
      "epoch": 1.4921090387374463,
      "grad_norm": 3.856468915939331,
      "learning_rate": 7.96076209158987e-06,
      "loss": 1.5666,
      "step": 2080
    },
    {
      "epoch": 1.4992826398852224,
      "grad_norm": 4.173006534576416,
      "learning_rate": 7.94257155441925e-06,
      "loss": 1.6614,
      "step": 2090
    },
    {
      "epoch": 1.5064562410329985,
      "grad_norm": 4.3249006271362305,
      "learning_rate": 7.9243212366188e-06,
      "loss": 1.6084,
      "step": 2100
    },
    {
      "epoch": 1.5136298421807748,
      "grad_norm": 3.1659603118896484,
      "learning_rate": 7.906011508957922e-06,
      "loss": 1.6086,
      "step": 2110
    },
    {
      "epoch": 1.5208034433285509,
      "grad_norm": 4.178988933563232,
      "learning_rate": 7.887642743412978e-06,
      "loss": 1.5947,
      "step": 2120
    },
    {
      "epoch": 1.527977044476327,
      "grad_norm": 4.197861194610596,
      "learning_rate": 7.86921531315972e-06,
      "loss": 1.6529,
      "step": 2130
    },
    {
      "epoch": 1.5351506456241033,
      "grad_norm": 4.113617897033691,
      "learning_rate": 7.850729592565734e-06,
      "loss": 1.6331,
      "step": 2140
    },
    {
      "epoch": 1.5423242467718796,
      "grad_norm": 3.7542648315429688,
      "learning_rate": 7.832185957182806e-06,
      "loss": 1.5992,
      "step": 2150
    },
    {
      "epoch": 1.5494978479196555,
      "grad_norm": 3.924823522567749,
      "learning_rate": 7.813584783739314e-06,
      "loss": 1.5767,
      "step": 2160
    },
    {
      "epoch": 1.5566714490674318,
      "grad_norm": 3.800712823867798,
      "learning_rate": 7.794926450132565e-06,
      "loss": 1.5843,
      "step": 2170
    },
    {
      "epoch": 1.5638450502152081,
      "grad_norm": 4.2379608154296875,
      "learning_rate": 7.776211335421117e-06,
      "loss": 1.6463,
      "step": 2180
    },
    {
      "epoch": 1.5710186513629842,
      "grad_norm": 4.136100769042969,
      "learning_rate": 7.757439819817084e-06,
      "loss": 1.6369,
      "step": 2190
    },
    {
      "epoch": 1.5781922525107603,
      "grad_norm": 3.602625608444214,
      "learning_rate": 7.738612284678404e-06,
      "loss": 1.6272,
      "step": 2200
    },
    {
      "epoch": 1.5853658536585367,
      "grad_norm": 4.472548007965088,
      "learning_rate": 7.719729112501098e-06,
      "loss": 1.624,
      "step": 2210
    },
    {
      "epoch": 1.5925394548063128,
      "grad_norm": 4.205074310302734,
      "learning_rate": 7.700790686911494e-06,
      "loss": 1.6549,
      "step": 2220
    },
    {
      "epoch": 1.5997130559540889,
      "grad_norm": 4.04252290725708,
      "learning_rate": 7.68179739265844e-06,
      "loss": 1.6236,
      "step": 2230
    },
    {
      "epoch": 1.6068866571018652,
      "grad_norm": 5.073679447174072,
      "learning_rate": 7.662749615605483e-06,
      "loss": 1.659,
      "step": 2240
    },
    {
      "epoch": 1.6140602582496413,
      "grad_norm": 5.263606071472168,
      "learning_rate": 7.643647742723025e-06,
      "loss": 1.6563,
      "step": 2250
    },
    {
      "epoch": 1.6212338593974174,
      "grad_norm": 4.70211935043335,
      "learning_rate": 7.624492162080472e-06,
      "loss": 1.6198,
      "step": 2260
    },
    {
      "epoch": 1.6284074605451937,
      "grad_norm": 3.6150543689727783,
      "learning_rate": 7.605283262838345e-06,
      "loss": 1.65,
      "step": 2270
    },
    {
      "epoch": 1.63558106169297,
      "grad_norm": 4.174437522888184,
      "learning_rate": 7.586021435240374e-06,
      "loss": 1.6715,
      "step": 2280
    },
    {
      "epoch": 1.642754662840746,
      "grad_norm": 4.112207412719727,
      "learning_rate": 7.5667070706055645e-06,
      "loss": 1.5987,
      "step": 2290
    },
    {
      "epoch": 1.6499282639885222,
      "grad_norm": 3.862330913543701,
      "learning_rate": 7.5473405613202596e-06,
      "loss": 1.6249,
      "step": 2300
    },
    {
      "epoch": 1.6571018651362985,
      "grad_norm": 4.445520401000977,
      "learning_rate": 7.527922300830156e-06,
      "loss": 1.603,
      "step": 2310
    },
    {
      "epoch": 1.6642754662840746,
      "grad_norm": 4.286288261413574,
      "learning_rate": 7.508452683632321e-06,
      "loss": 1.6862,
      "step": 2320
    },
    {
      "epoch": 1.6714490674318507,
      "grad_norm": 4.542356967926025,
      "learning_rate": 7.488932105267171e-06,
      "loss": 1.6543,
      "step": 2330
    },
    {
      "epoch": 1.678622668579627,
      "grad_norm": 3.4357030391693115,
      "learning_rate": 7.469360962310438e-06,
      "loss": 1.6064,
      "step": 2340
    },
    {
      "epoch": 1.6857962697274032,
      "grad_norm": 4.394213676452637,
      "learning_rate": 7.449739652365112e-06,
      "loss": 1.6194,
      "step": 2350
    },
    {
      "epoch": 1.6929698708751793,
      "grad_norm": 4.647204399108887,
      "learning_rate": 7.430068574053368e-06,
      "loss": 1.6815,
      "step": 2360
    },
    {
      "epoch": 1.7001434720229556,
      "grad_norm": 4.471418380737305,
      "learning_rate": 7.410348127008462e-06,
      "loss": 1.6465,
      "step": 2370
    },
    {
      "epoch": 1.7073170731707317,
      "grad_norm": 4.058635234832764,
      "learning_rate": 7.390578711866612e-06,
      "loss": 1.5747,
      "step": 2380
    },
    {
      "epoch": 1.7144906743185078,
      "grad_norm": 4.454770088195801,
      "learning_rate": 7.370760730258862e-06,
      "loss": 1.5735,
      "step": 2390
    },
    {
      "epoch": 1.721664275466284,
      "grad_norm": 3.3811800479888916,
      "learning_rate": 7.350894584802928e-06,
      "loss": 1.6444,
      "step": 2400
    },
    {
      "epoch": 1.7288378766140604,
      "grad_norm": 4.44746732711792,
      "learning_rate": 7.330980679095e-06,
      "loss": 1.5608,
      "step": 2410
    },
    {
      "epoch": 1.7360114777618363,
      "grad_norm": 4.57877254486084,
      "learning_rate": 7.311019417701567e-06,
      "loss": 1.7052,
      "step": 2420
    },
    {
      "epoch": 1.7431850789096126,
      "grad_norm": 4.749820232391357,
      "learning_rate": 7.291011206151176e-06,
      "loss": 1.5642,
      "step": 2430
    },
    {
      "epoch": 1.750358680057389,
      "grad_norm": 3.452590227127075,
      "learning_rate": 7.270956450926209e-06,
      "loss": 1.6353,
      "step": 2440
    },
    {
      "epoch": 1.757532281205165,
      "grad_norm": 4.356596946716309,
      "learning_rate": 7.250855559454615e-06,
      "loss": 1.6023,
      "step": 2450
    },
    {
      "epoch": 1.7647058823529411,
      "grad_norm": 3.9627525806427,
      "learning_rate": 7.2307089401016405e-06,
      "loss": 1.6218,
      "step": 2460
    },
    {
      "epoch": 1.7718794835007174,
      "grad_norm": 4.846285820007324,
      "learning_rate": 7.210517002161525e-06,
      "loss": 1.6259,
      "step": 2470
    },
    {
      "epoch": 1.7790530846484935,
      "grad_norm": 3.8124663829803467,
      "learning_rate": 7.190280155849196e-06,
      "loss": 1.6349,
      "step": 2480
    },
    {
      "epoch": 1.7862266857962696,
      "grad_norm": 4.130495548248291,
      "learning_rate": 7.169998812291923e-06,
      "loss": 1.6714,
      "step": 2490
    },
    {
      "epoch": 1.793400286944046,
      "grad_norm": 4.240413188934326,
      "learning_rate": 7.149673383520978e-06,
      "loss": 1.5979,
      "step": 2500
    },
    {
      "epoch": 1.800573888091822,
      "grad_norm": 4.097777843475342,
      "learning_rate": 7.129304282463253e-06,
      "loss": 1.5749,
      "step": 2510
    },
    {
      "epoch": 1.8077474892395982,
      "grad_norm": 4.352802276611328,
      "learning_rate": 7.10889192293288e-06,
      "loss": 1.6732,
      "step": 2520
    },
    {
      "epoch": 1.8149210903873745,
      "grad_norm": 3.664320230484009,
      "learning_rate": 7.088436719622819e-06,
      "loss": 1.5981,
      "step": 2530
    },
    {
      "epoch": 1.8220946915351508,
      "grad_norm": 5.20065975189209,
      "learning_rate": 7.0679390880964364e-06,
      "loss": 1.5965,
      "step": 2540
    },
    {
      "epoch": 1.8292682926829267,
      "grad_norm": 4.357119560241699,
      "learning_rate": 7.047399444779055e-06,
      "loss": 1.6777,
      "step": 2550
    },
    {
      "epoch": 1.836441893830703,
      "grad_norm": 4.286840438842773,
      "learning_rate": 7.026818206949512e-06,
      "loss": 1.6588,
      "step": 2560
    },
    {
      "epoch": 1.8436154949784793,
      "grad_norm": 4.276779651641846,
      "learning_rate": 7.0061957927316534e-06,
      "loss": 1.6694,
      "step": 2570
    },
    {
      "epoch": 1.8507890961262554,
      "grad_norm": 4.360471725463867,
      "learning_rate": 6.985532621085871e-06,
      "loss": 1.5754,
      "step": 2580
    },
    {
      "epoch": 1.8579626972740315,
      "grad_norm": 3.8147873878479004,
      "learning_rate": 6.964829111800564e-06,
      "loss": 1.6714,
      "step": 2590
    },
    {
      "epoch": 1.8651362984218078,
      "grad_norm": 3.973052501678467,
      "learning_rate": 6.944085685483628e-06,
      "loss": 1.5993,
      "step": 2600
    },
    {
      "epoch": 1.872309899569584,
      "grad_norm": 4.807898998260498,
      "learning_rate": 6.923302763553903e-06,
      "loss": 1.6285,
      "step": 2610
    },
    {
      "epoch": 1.87948350071736,
      "grad_norm": 4.763532638549805,
      "learning_rate": 6.9024807682326114e-06,
      "loss": 1.6311,
      "step": 2620
    },
    {
      "epoch": 1.8866571018651364,
      "grad_norm": 4.089593887329102,
      "learning_rate": 6.881620122534785e-06,
      "loss": 1.6572,
      "step": 2630
    },
    {
      "epoch": 1.8938307030129125,
      "grad_norm": 4.709824562072754,
      "learning_rate": 6.860721250260664e-06,
      "loss": 1.5825,
      "step": 2640
    },
    {
      "epoch": 1.9010043041606886,
      "grad_norm": 4.138241767883301,
      "learning_rate": 6.841879932388079e-06,
      "loss": 1.5935,
      "step": 2650
    },
    {
      "epoch": 1.9081779053084649,
      "grad_norm": 3.7684028148651123,
      "learning_rate": 6.820909599963124e-06,
      "loss": 1.572,
      "step": 2660
    },
    {
      "epoch": 1.9153515064562412,
      "grad_norm": 3.5025687217712402,
      "learning_rate": 6.799902274343357e-06,
      "loss": 1.667,
      "step": 2670
    },
    {
      "epoch": 1.922525107604017,
      "grad_norm": 3.805840492248535,
      "learning_rate": 6.778858382308946e-06,
      "loss": 1.6538,
      "step": 2680
    },
    {
      "epoch": 1.9296987087517934,
      "grad_norm": 4.1812944412231445,
      "learning_rate": 6.75777835138292e-06,
      "loss": 1.5986,
      "step": 2690
    },
    {
      "epoch": 1.9368723098995697,
      "grad_norm": 4.109395503997803,
      "learning_rate": 6.736662609822504e-06,
      "loss": 1.541,
      "step": 2700
    },
    {
      "epoch": 1.9440459110473458,
      "grad_norm": 4.363220691680908,
      "learning_rate": 6.715511586610412e-06,
      "loss": 1.6826,
      "step": 2710
    },
    {
      "epoch": 1.951219512195122,
      "grad_norm": 5.00598669052124,
      "learning_rate": 6.694325711446133e-06,
      "loss": 1.5182,
      "step": 2720
    },
    {
      "epoch": 1.9583931133428982,
      "grad_norm": 4.200891971588135,
      "learning_rate": 6.6731054147371965e-06,
      "loss": 1.5479,
      "step": 2730
    },
    {
      "epoch": 1.9655667144906743,
      "grad_norm": 3.854321241378784,
      "learning_rate": 6.651851127590437e-06,
      "loss": 1.5367,
      "step": 2740
    },
    {
      "epoch": 1.9727403156384504,
      "grad_norm": 3.943309783935547,
      "learning_rate": 6.630563281803228e-06,
      "loss": 1.5489,
      "step": 2750
    },
    {
      "epoch": 1.9799139167862267,
      "grad_norm": 5.058989524841309,
      "learning_rate": 6.609242309854714e-06,
      "loss": 1.6992,
      "step": 2760
    },
    {
      "epoch": 1.9870875179340028,
      "grad_norm": 4.2707600593566895,
      "learning_rate": 6.587888644897025e-06,
      "loss": 1.6216,
      "step": 2770
    },
    {
      "epoch": 1.994261119081779,
      "grad_norm": 3.425509452819824,
      "learning_rate": 6.566502720746472e-06,
      "loss": 1.5665,
      "step": 2780
    },
    {
      "epoch": 2.0014347202295553,
      "grad_norm": 3.6666088104248047,
      "learning_rate": 6.545084971874738e-06,
      "loss": 1.6564,
      "step": 2790
    },
    {
      "epoch": 2.0086083213773316,
      "grad_norm": 5.185142517089844,
      "learning_rate": 6.523635833400049e-06,
      "loss": 1.6612,
      "step": 2800
    },
    {
      "epoch": 2.0157819225251075,
      "grad_norm": 4.80631160736084,
      "learning_rate": 6.5021557410783375e-06,
      "loss": 1.5825,
      "step": 2810
    },
    {
      "epoch": 2.022955523672884,
      "grad_norm": 4.881715774536133,
      "learning_rate": 6.480645131294386e-06,
      "loss": 1.5956,
      "step": 2820
    },
    {
      "epoch": 2.03012912482066,
      "grad_norm": 4.405465126037598,
      "learning_rate": 6.459104441052962e-06,
      "loss": 1.6215,
      "step": 2830
    },
    {
      "epoch": 2.037302725968436,
      "grad_norm": 4.767330646514893,
      "learning_rate": 6.437534107969942e-06,
      "loss": 1.5903,
      "step": 2840
    },
    {
      "epoch": 2.0444763271162123,
      "grad_norm": 4.691718101501465,
      "learning_rate": 6.41593457026342e-06,
      "loss": 1.6493,
      "step": 2850
    },
    {
      "epoch": 2.0516499282639886,
      "grad_norm": 5.495260238647461,
      "learning_rate": 6.394306266744805e-06,
      "loss": 1.5773,
      "step": 2860
    },
    {
      "epoch": 2.0588235294117645,
      "grad_norm": 4.0904693603515625,
      "learning_rate": 6.372649636809905e-06,
      "loss": 1.6058,
      "step": 2870
    },
    {
      "epoch": 2.065997130559541,
      "grad_norm": 4.370164394378662,
      "learning_rate": 6.350965120429999e-06,
      "loss": 1.658,
      "step": 2880
    },
    {
      "epoch": 2.073170731707317,
      "grad_norm": 4.115325450897217,
      "learning_rate": 6.329253158142907e-06,
      "loss": 1.6377,
      "step": 2890
    },
    {
      "epoch": 2.0803443328550935,
      "grad_norm": 4.1937174797058105,
      "learning_rate": 6.3075141910440284e-06,
      "loss": 1.5463,
      "step": 2900
    },
    {
      "epoch": 2.0875179340028693,
      "grad_norm": 4.775271892547607,
      "learning_rate": 6.285748660777388e-06,
      "loss": 1.5699,
      "step": 2910
    },
    {
      "epoch": 2.0946915351506457,
      "grad_norm": 4.732351303100586,
      "learning_rate": 6.263957009526662e-06,
      "loss": 1.6241,
      "step": 2920
    },
    {
      "epoch": 2.101865136298422,
      "grad_norm": 4.935391902923584,
      "learning_rate": 6.242139680006196e-06,
      "loss": 1.6099,
      "step": 2930
    },
    {
      "epoch": 2.109038737446198,
      "grad_norm": 4.295470714569092,
      "learning_rate": 6.220297115452008e-06,
      "loss": 1.5634,
      "step": 2940
    },
    {
      "epoch": 2.116212338593974,
      "grad_norm": 4.149561405181885,
      "learning_rate": 6.198429759612785e-06,
      "loss": 1.5818,
      "step": 2950
    },
    {
      "epoch": 2.1233859397417505,
      "grad_norm": 3.926194906234741,
      "learning_rate": 6.176538056740871e-06,
      "loss": 1.5793,
      "step": 2960
    },
    {
      "epoch": 2.1305595408895264,
      "grad_norm": 4.342965602874756,
      "learning_rate": 6.154622451583233e-06,
      "loss": 1.6258,
      "step": 2970
    },
    {
      "epoch": 2.1377331420373027,
      "grad_norm": 3.2309916019439697,
      "learning_rate": 6.132683389372442e-06,
      "loss": 1.6309,
      "step": 2980
    },
    {
      "epoch": 2.144906743185079,
      "grad_norm": 4.24520206451416,
      "learning_rate": 6.110721315817603e-06,
      "loss": 1.6711,
      "step": 2990
    },
    {
      "epoch": 2.152080344332855,
      "grad_norm": 4.638644218444824,
      "learning_rate": 6.088736677095328e-06,
      "loss": 1.4989,
      "step": 3000
    },
    {
      "epoch": 2.159253945480631,
      "grad_norm": 3.7195332050323486,
      "learning_rate": 6.06672991984065e-06,
      "loss": 1.5388,
      "step": 3010
    },
    {
      "epoch": 2.1664275466284075,
      "grad_norm": 4.042150497436523,
      "learning_rate": 6.044701491137958e-06,
      "loss": 1.6389,
      "step": 3020
    },
    {
      "epoch": 2.173601147776184,
      "grad_norm": 3.9503417015075684,
      "learning_rate": 6.022651838511918e-06,
      "loss": 1.6343,
      "step": 3030
    },
    {
      "epoch": 2.1807747489239597,
      "grad_norm": 4.219050407409668,
      "learning_rate": 6.000581409918371e-06,
      "loss": 1.5678,
      "step": 3040
    },
    {
      "epoch": 2.187948350071736,
      "grad_norm": 4.497432231903076,
      "learning_rate": 5.9784906537352385e-06,
      "loss": 1.5441,
      "step": 3050
    },
    {
      "epoch": 2.1951219512195124,
      "grad_norm": 4.588307857513428,
      "learning_rate": 5.9563800187534184e-06,
      "loss": 1.6133,
      "step": 3060
    },
    {
      "epoch": 2.2022955523672882,
      "grad_norm": 3.617439031600952,
      "learning_rate": 5.934249954167652e-06,
      "loss": 1.6044,
      "step": 3070
    },
    {
      "epoch": 2.2094691535150646,
      "grad_norm": 4.48012113571167,
      "learning_rate": 5.912100909567418e-06,
      "loss": 1.6596,
      "step": 3080
    },
    {
      "epoch": 2.216642754662841,
      "grad_norm": 5.040704727172852,
      "learning_rate": 5.889933334927786e-06,
      "loss": 1.6516,
      "step": 3090
    },
    {
      "epoch": 2.2238163558106168,
      "grad_norm": 4.71259880065918,
      "learning_rate": 5.867747680600272e-06,
      "loss": 1.6068,
      "step": 3100
    },
    {
      "epoch": 2.230989956958393,
      "grad_norm": 4.257416248321533,
      "learning_rate": 5.845544397303703e-06,
      "loss": 1.6074,
      "step": 3110
    },
    {
      "epoch": 2.2381635581061694,
      "grad_norm": 4.409589767456055,
      "learning_rate": 5.823323936115048e-06,
      "loss": 1.5894,
      "step": 3120
    },
    {
      "epoch": 2.2453371592539453,
      "grad_norm": 3.637420177459717,
      "learning_rate": 5.801086748460255e-06,
      "loss": 1.5526,
      "step": 3130
    },
    {
      "epoch": 2.2525107604017216,
      "grad_norm": 4.419875144958496,
      "learning_rate": 5.778833286105094e-06,
      "loss": 1.6043,
      "step": 3140
    },
    {
      "epoch": 2.259684361549498,
      "grad_norm": 4.492452144622803,
      "learning_rate": 5.756564001145956e-06,
      "loss": 1.585,
      "step": 3150
    },
    {
      "epoch": 2.266857962697274,
      "grad_norm": 4.65122652053833,
      "learning_rate": 5.7342793460006875e-06,
      "loss": 1.628,
      "step": 3160
    },
    {
      "epoch": 2.27403156384505,
      "grad_norm": 4.299788475036621,
      "learning_rate": 5.711979773399391e-06,
      "loss": 1.5854,
      "step": 3170
    },
    {
      "epoch": 2.2812051649928264,
      "grad_norm": 4.478477954864502,
      "learning_rate": 5.689665736375227e-06,
      "loss": 1.6201,
      "step": 3180
    },
    {
      "epoch": 2.2883787661406028,
      "grad_norm": 4.644171714782715,
      "learning_rate": 5.667337688255215e-06,
      "loss": 1.6344,
      "step": 3190
    },
    {
      "epoch": 2.2955523672883786,
      "grad_norm": 3.873291015625,
      "learning_rate": 5.644996082651018e-06,
      "loss": 1.6742,
      "step": 3200
    },
    {
      "epoch": 2.302725968436155,
      "grad_norm": 4.3887786865234375,
      "learning_rate": 5.622641373449729e-06,
      "loss": 1.5973,
      "step": 3210
    },
    {
      "epoch": 2.3098995695839313,
      "grad_norm": 4.302896499633789,
      "learning_rate": 5.600274014804656e-06,
      "loss": 1.675,
      "step": 3220
    },
    {
      "epoch": 2.317073170731707,
      "grad_norm": 4.341148376464844,
      "learning_rate": 5.577894461126086e-06,
      "loss": 1.5722,
      "step": 3230
    },
    {
      "epoch": 2.3242467718794835,
      "grad_norm": 5.464095115661621,
      "learning_rate": 5.555503167072057e-06,
      "loss": 1.5878,
      "step": 3240
    },
    {
      "epoch": 2.33142037302726,
      "grad_norm": 4.341732978820801,
      "learning_rate": 5.533100587539126e-06,
      "loss": 1.6178,
      "step": 3250
    },
    {
      "epoch": 2.338593974175036,
      "grad_norm": 4.744466304779053,
      "learning_rate": 5.510687177653118e-06,
      "loss": 1.5645,
      "step": 3260
    },
    {
      "epoch": 2.345767575322812,
      "grad_norm": 4.801852226257324,
      "learning_rate": 5.488263392759889e-06,
      "loss": 1.5635,
      "step": 3270
    },
    {
      "epoch": 2.3529411764705883,
      "grad_norm": 4.732244968414307,
      "learning_rate": 5.4658296884160714e-06,
      "loss": 1.6205,
      "step": 3280
    },
    {
      "epoch": 2.3601147776183646,
      "grad_norm": 5.230564117431641,
      "learning_rate": 5.443386520379814e-06,
      "loss": 1.6839,
      "step": 3290
    },
    {
      "epoch": 2.3672883787661405,
      "grad_norm": 4.0107574462890625,
      "learning_rate": 5.420934344601536e-06,
      "loss": 1.5603,
      "step": 3300
    },
    {
      "epoch": 2.374461979913917,
      "grad_norm": 5.033327102661133,
      "learning_rate": 5.398473617214648e-06,
      "loss": 1.6106,
      "step": 3310
    },
    {
      "epoch": 2.381635581061693,
      "grad_norm": 5.151188850402832,
      "learning_rate": 5.376004794526297e-06,
      "loss": 1.5827,
      "step": 3320
    },
    {
      "epoch": 2.388809182209469,
      "grad_norm": 4.109166145324707,
      "learning_rate": 5.353528333008094e-06,
      "loss": 1.6224,
      "step": 3330
    },
    {
      "epoch": 2.3959827833572453,
      "grad_norm": 4.072045803070068,
      "learning_rate": 5.331044689286836e-06,
      "loss": 1.6035,
      "step": 3340
    },
    {
      "epoch": 2.4031563845050217,
      "grad_norm": 4.721465110778809,
      "learning_rate": 5.308554320135232e-06,
      "loss": 1.5577,
      "step": 3350
    },
    {
      "epoch": 2.4103299856527975,
      "grad_norm": 3.9917335510253906,
      "learning_rate": 5.286057682462623e-06,
      "loss": 1.5706,
      "step": 3360
    },
    {
      "epoch": 2.417503586800574,
      "grad_norm": 4.760295867919922,
      "learning_rate": 5.263555233305701e-06,
      "loss": 1.6357,
      "step": 3370
    },
    {
      "epoch": 2.42467718794835,
      "grad_norm": 4.122778415679932,
      "learning_rate": 5.2410474298192285e-06,
      "loss": 1.534,
      "step": 3380
    },
    {
      "epoch": 2.431850789096126,
      "grad_norm": 5.549314022064209,
      "learning_rate": 5.218534729266731e-06,
      "loss": 1.5435,
      "step": 3390
    },
    {
      "epoch": 2.4390243902439024,
      "grad_norm": 5.136187553405762,
      "learning_rate": 5.196017589011236e-06,
      "loss": 1.6116,
      "step": 3400
    },
    {
      "epoch": 2.4461979913916787,
      "grad_norm": 4.078512668609619,
      "learning_rate": 5.173496466505959e-06,
      "loss": 1.5657,
      "step": 3410
    },
    {
      "epoch": 2.4533715925394546,
      "grad_norm": 3.9479634761810303,
      "learning_rate": 5.150971819285022e-06,
      "loss": 1.5952,
      "step": 3420
    },
    {
      "epoch": 2.460545193687231,
      "grad_norm": 4.584644317626953,
      "learning_rate": 5.128444104954154e-06,
      "loss": 1.5471,
      "step": 3430
    },
    {
      "epoch": 2.4677187948350072,
      "grad_norm": 5.6115570068359375,
      "learning_rate": 5.1059137811813875e-06,
      "loss": 1.5866,
      "step": 3440
    },
    {
      "epoch": 2.4748923959827835,
      "grad_norm": 4.367930889129639,
      "learning_rate": 5.08338130568778e-06,
      "loss": 1.585,
      "step": 3450
    },
    {
      "epoch": 2.4820659971305594,
      "grad_norm": 4.2019944190979,
      "learning_rate": 5.060847136238094e-06,
      "loss": 1.5933,
      "step": 3460
    },
    {
      "epoch": 2.4892395982783357,
      "grad_norm": 3.4313952922821045,
      "learning_rate": 5.0383117306315085e-06,
      "loss": 1.6081,
      "step": 3470
    },
    {
      "epoch": 2.496413199426112,
      "grad_norm": 4.8354268074035645,
      "learning_rate": 5.015775546692318e-06,
      "loss": 1.5926,
      "step": 3480
    },
    {
      "epoch": 2.503586800573888,
      "grad_norm": 3.5297632217407227,
      "learning_rate": 4.993239042260624e-06,
      "loss": 1.5761,
      "step": 3490
    },
    {
      "epoch": 2.5107604017216643,
      "grad_norm": 3.661533832550049,
      "learning_rate": 4.970702675183048e-06,
      "loss": 1.5688,
      "step": 3500
    },
    {
      "epoch": 2.5179340028694406,
      "grad_norm": 5.099844932556152,
      "learning_rate": 4.948166903303412e-06,
      "loss": 1.5362,
      "step": 3510
    },
    {
      "epoch": 2.525107604017217,
      "grad_norm": 4.637667655944824,
      "learning_rate": 4.9256321844534495e-06,
      "loss": 1.5822,
      "step": 3520
    },
    {
      "epoch": 2.5322812051649928,
      "grad_norm": 4.487159729003906,
      "learning_rate": 4.9030989764435024e-06,
      "loss": 1.574,
      "step": 3530
    },
    {
      "epoch": 2.539454806312769,
      "grad_norm": 4.577577590942383,
      "learning_rate": 4.88056773705322e-06,
      "loss": 1.5211,
      "step": 3540
    },
    {
      "epoch": 2.5466284074605454,
      "grad_norm": 4.300015449523926,
      "learning_rate": 4.858038924022249e-06,
      "loss": 1.6057,
      "step": 3550
    },
    {
      "epoch": 2.5538020086083213,
      "grad_norm": 4.561922550201416,
      "learning_rate": 4.835512995040954e-06,
      "loss": 1.5954,
      "step": 3560
    },
    {
      "epoch": 2.5609756097560976,
      "grad_norm": 4.917219638824463,
      "learning_rate": 4.8129904077410996e-06,
      "loss": 1.5329,
      "step": 3570
    },
    {
      "epoch": 2.568149210903874,
      "grad_norm": 4.578744411468506,
      "learning_rate": 4.790471619686568e-06,
      "loss": 1.6117,
      "step": 3580
    },
    {
      "epoch": 2.57532281205165,
      "grad_norm": 4.512048244476318,
      "learning_rate": 4.7679570883640515e-06,
      "loss": 1.5721,
      "step": 3590
    },
    {
      "epoch": 2.582496413199426,
      "grad_norm": 3.943474054336548,
      "learning_rate": 4.745447271173763e-06,
      "loss": 1.4721,
      "step": 3600
    },
    {
      "epoch": 2.5896700143472025,
      "grad_norm": 4.9133992195129395,
      "learning_rate": 4.7229426254201504e-06,
      "loss": 1.5951,
      "step": 3610
    },
    {
      "epoch": 2.5968436154949783,
      "grad_norm": 5.566059589385986,
      "learning_rate": 4.700443608302596e-06,
      "loss": 1.6436,
      "step": 3620
    },
    {
      "epoch": 2.6040172166427547,
      "grad_norm": 5.11680793762207,
      "learning_rate": 4.677950676906128e-06,
      "loss": 1.5783,
      "step": 3630
    },
    {
      "epoch": 2.611190817790531,
      "grad_norm": 4.824029445648193,
      "learning_rate": 4.655464288192147e-06,
      "loss": 1.6622,
      "step": 3640
    },
    {
      "epoch": 2.618364418938307,
      "grad_norm": 4.1654372215271,
      "learning_rate": 4.632984898989126e-06,
      "loss": 1.5663,
      "step": 3650
    },
    {
      "epoch": 2.625538020086083,
      "grad_norm": 4.733531475067139,
      "learning_rate": 4.610512965983345e-06,
      "loss": 1.5764,
      "step": 3660
    },
    {
      "epoch": 2.6327116212338595,
      "grad_norm": 4.352785587310791,
      "learning_rate": 4.588048945709597e-06,
      "loss": 1.5627,
      "step": 3670
    },
    {
      "epoch": 2.6398852223816354,
      "grad_norm": 5.636826038360596,
      "learning_rate": 4.565593294541927e-06,
      "loss": 1.6139,
      "step": 3680
    },
    {
      "epoch": 2.6470588235294117,
      "grad_norm": 5.10280704498291,
      "learning_rate": 4.543146468684357e-06,
      "loss": 1.6185,
      "step": 3690
    },
    {
      "epoch": 2.654232424677188,
      "grad_norm": 4.450313091278076,
      "learning_rate": 4.520708924161612e-06,
      "loss": 1.5829,
      "step": 3700
    },
    {
      "epoch": 2.661406025824964,
      "grad_norm": 5.116763591766357,
      "learning_rate": 4.498281116809859e-06,
      "loss": 1.6407,
      "step": 3710
    },
    {
      "epoch": 2.66857962697274,
      "grad_norm": 4.992338180541992,
      "learning_rate": 4.475863502267449e-06,
      "loss": 1.5339,
      "step": 3720
    },
    {
      "epoch": 2.6757532281205165,
      "grad_norm": 4.610321044921875,
      "learning_rate": 4.4534565359656576e-06,
      "loss": 1.6009,
      "step": 3730
    },
    {
      "epoch": 2.682926829268293,
      "grad_norm": 5.518273830413818,
      "learning_rate": 4.4310606731194365e-06,
      "loss": 1.5428,
      "step": 3740
    },
    {
      "epoch": 2.6901004304160687,
      "grad_norm": 5.1075029373168945,
      "learning_rate": 4.408676368718154e-06,
      "loss": 1.5231,
      "step": 3750
    },
    {
      "epoch": 2.697274031563845,
      "grad_norm": 4.705279350280762,
      "learning_rate": 4.386304077516366e-06,
      "loss": 1.6012,
      "step": 3760
    },
    {
      "epoch": 2.7044476327116214,
      "grad_norm": 4.0844035148620605,
      "learning_rate": 4.3639442540245695e-06,
      "loss": 1.5108,
      "step": 3770
    },
    {
      "epoch": 2.7116212338593977,
      "grad_norm": 4.610934734344482,
      "learning_rate": 4.341597352499971e-06,
      "loss": 1.5689,
      "step": 3780
    },
    {
      "epoch": 2.7187948350071736,
      "grad_norm": 4.25030517578125,
      "learning_rate": 4.319263826937253e-06,
      "loss": 1.5698,
      "step": 3790
    },
    {
      "epoch": 2.72596843615495,
      "grad_norm": 4.2889862060546875,
      "learning_rate": 4.296944131059362e-06,
      "loss": 1.5317,
      "step": 3800
    },
    {
      "epoch": 2.733142037302726,
      "grad_norm": 5.41288948059082,
      "learning_rate": 4.274638718308276e-06,
      "loss": 1.6699,
      "step": 3810
    },
    {
      "epoch": 2.740315638450502,
      "grad_norm": 5.53465461730957,
      "learning_rate": 4.252348041835807e-06,
      "loss": 1.6027,
      "step": 3820
    },
    {
      "epoch": 2.7474892395982784,
      "grad_norm": 5.389010429382324,
      "learning_rate": 4.230072554494384e-06,
      "loss": 1.4978,
      "step": 3830
    },
    {
      "epoch": 2.7546628407460547,
      "grad_norm": 5.000797748565674,
      "learning_rate": 4.210037976628163e-06,
      "loss": 1.5293,
      "step": 3840
    },
    {
      "epoch": 2.7618364418938306,
      "grad_norm": 4.385794162750244,
      "learning_rate": 4.187792595131588e-06,
      "loss": 1.5812,
      "step": 3850
    },
    {
      "epoch": 2.769010043041607,
      "grad_norm": 4.570942401885986,
      "learning_rate": 4.165563714260166e-06,
      "loss": 1.5471,
      "step": 3860
    },
    {
      "epoch": 2.7761836441893832,
      "grad_norm": 4.750805854797363,
      "learning_rate": 4.143351785610906e-06,
      "loss": 1.6115,
      "step": 3870
    },
    {
      "epoch": 2.783357245337159,
      "grad_norm": 4.273472785949707,
      "learning_rate": 4.1211572604364125e-06,
      "loss": 1.5743,
      "step": 3880
    },
    {
      "epoch": 2.7905308464849354,
      "grad_norm": 4.441281795501709,
      "learning_rate": 4.098980589635729e-06,
      "loss": 1.5347,
      "step": 3890
    },
    {
      "epoch": 2.7977044476327118,
      "grad_norm": 4.9329514503479,
      "learning_rate": 4.07682222374517e-06,
      "loss": 1.6277,
      "step": 3900
    },
    {
      "epoch": 2.8048780487804876,
      "grad_norm": 4.651209831237793,
      "learning_rate": 4.054682612929172e-06,
      "loss": 1.5164,
      "step": 3910
    },
    {
      "epoch": 2.812051649928264,
      "grad_norm": 4.946371078491211,
      "learning_rate": 4.032562206971151e-06,
      "loss": 1.4847,
      "step": 3920
    },
    {
      "epoch": 2.8192252510760403,
      "grad_norm": 4.758925437927246,
      "learning_rate": 4.0104614552643586e-06,
      "loss": 1.6173,
      "step": 3930
    },
    {
      "epoch": 2.826398852223816,
      "grad_norm": 4.458917140960693,
      "learning_rate": 3.988380806802753e-06,
      "loss": 1.5177,
      "step": 3940
    },
    {
      "epoch": 2.8335724533715925,
      "grad_norm": 4.800405502319336,
      "learning_rate": 3.966320710171885e-06,
      "loss": 1.5391,
      "step": 3950
    },
    {
      "epoch": 2.840746054519369,
      "grad_norm": 4.7320356369018555,
      "learning_rate": 3.944281613539774e-06,
      "loss": 1.6099,
      "step": 3960
    },
    {
      "epoch": 2.8479196556671447,
      "grad_norm": 5.528425693511963,
      "learning_rate": 3.922263964647811e-06,
      "loss": 1.6085,
      "step": 3970
    },
    {
      "epoch": 2.855093256814921,
      "grad_norm": 4.551238059997559,
      "learning_rate": 3.9002682108016585e-06,
      "loss": 1.6138,
      "step": 3980
    },
    {
      "epoch": 2.8622668579626973,
      "grad_norm": 3.786184310913086,
      "learning_rate": 3.878294798862161e-06,
      "loss": 1.6127,
      "step": 3990
    },
    {
      "epoch": 2.869440459110473,
      "grad_norm": 6.750377655029297,
      "learning_rate": 3.856344175236276e-06,
      "loss": 1.5743,
      "step": 4000
    },
    {
      "epoch": 2.8766140602582495,
      "grad_norm": 3.8408913612365723,
      "learning_rate": 3.834416785867994e-06,
      "loss": 1.5725,
      "step": 4010
    },
    {
      "epoch": 2.883787661406026,
      "grad_norm": 6.89605188369751,
      "learning_rate": 3.8125130762292862e-06,
      "loss": 1.6073,
      "step": 4020
    },
    {
      "epoch": 2.890961262553802,
      "grad_norm": 4.984270095825195,
      "learning_rate": 3.790633491311049e-06,
      "loss": 1.5268,
      "step": 4030
    },
    {
      "epoch": 2.8981348637015785,
      "grad_norm": 4.5666985511779785,
      "learning_rate": 3.768778475614066e-06,
      "loss": 1.5814,
      "step": 4040
    },
    {
      "epoch": 2.9053084648493543,
      "grad_norm": 4.317162036895752,
      "learning_rate": 3.746948473139984e-06,
      "loss": 1.4571,
      "step": 4050
    },
    {
      "epoch": 2.9124820659971307,
      "grad_norm": 5.121085166931152,
      "learning_rate": 3.7251439273822803e-06,
      "loss": 1.5846,
      "step": 4060
    },
    {
      "epoch": 2.919655667144907,
      "grad_norm": 5.571870803833008,
      "learning_rate": 3.7033652813172593e-06,
      "loss": 1.5734,
      "step": 4070
    },
    {
      "epoch": 2.926829268292683,
      "grad_norm": 4.869257926940918,
      "learning_rate": 3.6816129773950583e-06,
      "loss": 1.5727,
      "step": 4080
    },
    {
      "epoch": 2.934002869440459,
      "grad_norm": 4.5508713722229,
      "learning_rate": 3.6598874575306475e-06,
      "loss": 1.5901,
      "step": 4090
    },
    {
      "epoch": 2.9411764705882355,
      "grad_norm": 4.918824672698975,
      "learning_rate": 3.6381891630948646e-06,
      "loss": 1.5659,
      "step": 4100
    },
    {
      "epoch": 2.9483500717360114,
      "grad_norm": 5.343818664550781,
      "learning_rate": 3.616518534905434e-06,
      "loss": 1.5045,
      "step": 4110
    },
    {
      "epoch": 2.9555236728837877,
      "grad_norm": 5.23090934753418,
      "learning_rate": 3.5948760132180223e-06,
      "loss": 1.5714,
      "step": 4120
    },
    {
      "epoch": 2.962697274031564,
      "grad_norm": 4.951622009277344,
      "learning_rate": 3.5732620377172933e-06,
      "loss": 1.5915,
      "step": 4130
    },
    {
      "epoch": 2.96987087517934,
      "grad_norm": 4.885720252990723,
      "learning_rate": 3.5516770475079717e-06,
      "loss": 1.5826,
      "step": 4140
    },
    {
      "epoch": 2.977044476327116,
      "grad_norm": 5.204196929931641,
      "learning_rate": 3.530121481105917e-06,
      "loss": 1.6032,
      "step": 4150
    },
    {
      "epoch": 2.9842180774748925,
      "grad_norm": 5.193727493286133,
      "learning_rate": 3.5085957764292307e-06,
      "loss": 1.6692,
      "step": 4160
    },
    {
      "epoch": 2.9913916786226684,
      "grad_norm": 5.5978684425354,
      "learning_rate": 3.487100370789343e-06,
      "loss": 1.5283,
      "step": 4170
    },
    {
      "epoch": 2.9985652797704447,
      "grad_norm": 5.1900811195373535,
      "learning_rate": 3.465635700882142e-06,
      "loss": 1.5728,
      "step": 4180
    },
    {
      "epoch": 3.005738880918221,
      "grad_norm": 5.007804870605469,
      "learning_rate": 3.4442022027790888e-06,
      "loss": 1.6221,
      "step": 4190
    },
    {
      "epoch": 3.012912482065997,
      "grad_norm": 3.714120388031006,
      "learning_rate": 3.422800311918367e-06,
      "loss": 1.5336,
      "step": 4200
    },
    {
      "epoch": 3.0200860832137733,
      "grad_norm": 4.984997749328613,
      "learning_rate": 3.4014304630960402e-06,
      "loss": 1.5444,
      "step": 4210
    },
    {
      "epoch": 3.0272596843615496,
      "grad_norm": 4.416684627532959,
      "learning_rate": 3.380093090457206e-06,
      "loss": 1.5889,
      "step": 4220
    },
    {
      "epoch": 3.034433285509326,
      "grad_norm": 4.978219509124756,
      "learning_rate": 3.3587886274871827e-06,
      "loss": 1.675,
      "step": 4230
    },
    {
      "epoch": 3.0416068866571018,
      "grad_norm": 5.3789896965026855,
      "learning_rate": 3.3375175070027104e-06,
      "loss": 1.5787,
      "step": 4240
    },
    {
      "epoch": 3.048780487804878,
      "grad_norm": 5.701040267944336,
      "learning_rate": 3.316280161143143e-06,
      "loss": 1.571,
      "step": 4250
    },
    {
      "epoch": 3.0559540889526544,
      "grad_norm": 5.261663913726807,
      "learning_rate": 3.2950770213616835e-06,
      "loss": 1.6035,
      "step": 4260
    },
    {
      "epoch": 3.0631276901004303,
      "grad_norm": 5.057870388031006,
      "learning_rate": 3.273908518416604e-06,
      "loss": 1.5341,
      "step": 4270
    },
    {
      "epoch": 3.0703012912482066,
      "grad_norm": 4.982771396636963,
      "learning_rate": 3.252775082362508e-06,
      "loss": 1.605,
      "step": 4280
    },
    {
      "epoch": 3.077474892395983,
      "grad_norm": 3.839228868484497,
      "learning_rate": 3.231677142541587e-06,
      "loss": 1.5312,
      "step": 4290
    },
    {
      "epoch": 3.084648493543759,
      "grad_norm": 4.821488380432129,
      "learning_rate": 3.2106151275748996e-06,
      "loss": 1.5648,
      "step": 4300
    },
    {
      "epoch": 3.091822094691535,
      "grad_norm": 5.671468734741211,
      "learning_rate": 3.1895894653536576e-06,
      "loss": 1.5848,
      "step": 4310
    },
    {
      "epoch": 3.0989956958393114,
      "grad_norm": 5.601500988006592,
      "learning_rate": 3.168600583030545e-06,
      "loss": 1.5634,
      "step": 4320
    },
    {
      "epoch": 3.1061692969870873,
      "grad_norm": 4.747995853424072,
      "learning_rate": 3.1476489070110307e-06,
      "loss": 1.4905,
      "step": 4330
    },
    {
      "epoch": 3.1133428981348636,
      "grad_norm": 5.269165992736816,
      "learning_rate": 3.1267348629447104e-06,
      "loss": 1.6027,
      "step": 4340
    },
    {
      "epoch": 3.12051649928264,
      "grad_norm": 4.694492340087891,
      "learning_rate": 3.105858875716655e-06,
      "loss": 1.5644,
      "step": 4350
    },
    {
      "epoch": 3.1276901004304163,
      "grad_norm": 4.4797468185424805,
      "learning_rate": 3.08502136943878e-06,
      "loss": 1.6084,
      "step": 4360
    },
    {
      "epoch": 3.134863701578192,
      "grad_norm": 4.172919273376465,
      "learning_rate": 3.0642227674412363e-06,
      "loss": 1.6832,
      "step": 4370
    },
    {
      "epoch": 3.1420373027259685,
      "grad_norm": 5.2582621574401855,
      "learning_rate": 3.0434634922637994e-06,
      "loss": 1.5974,
      "step": 4380
    },
    {
      "epoch": 3.149210903873745,
      "grad_norm": 4.4488630294799805,
      "learning_rate": 3.0227439656472878e-06,
      "loss": 1.5189,
      "step": 4390
    },
    {
      "epoch": 3.1563845050215207,
      "grad_norm": 4.228070259094238,
      "learning_rate": 3.0020646085250045e-06,
      "loss": 1.5204,
      "step": 4400
    },
    {
      "epoch": 3.163558106169297,
      "grad_norm": 4.215377330780029,
      "learning_rate": 2.98142584101417e-06,
      "loss": 1.5675,
      "step": 4410
    },
    {
      "epoch": 3.1707317073170733,
      "grad_norm": 5.365569114685059,
      "learning_rate": 2.9608280824074026e-06,
      "loss": 1.5922,
      "step": 4420
    },
    {
      "epoch": 3.177905308464849,
      "grad_norm": 4.682549953460693,
      "learning_rate": 2.9402717511641865e-06,
      "loss": 1.5348,
      "step": 4430
    },
    {
      "epoch": 3.1850789096126255,
      "grad_norm": 4.602301120758057,
      "learning_rate": 2.9197572649023785e-06,
      "loss": 1.5204,
      "step": 4440
    },
    {
      "epoch": 3.192252510760402,
      "grad_norm": 4.920101165771484,
      "learning_rate": 2.8992850403897257e-06,
      "loss": 1.5737,
      "step": 4450
    },
    {
      "epoch": 3.1994261119081777,
      "grad_norm": 4.8239970207214355,
      "learning_rate": 2.878855493535392e-06,
      "loss": 1.5251,
      "step": 4460
    },
    {
      "epoch": 3.206599713055954,
      "grad_norm": 5.514883041381836,
      "learning_rate": 2.8584690393815086e-06,
      "loss": 1.6063,
      "step": 4470
    },
    {
      "epoch": 3.2137733142037304,
      "grad_norm": 4.612925052642822,
      "learning_rate": 2.8381260920947497e-06,
      "loss": 1.5766,
      "step": 4480
    },
    {
      "epoch": 3.2209469153515062,
      "grad_norm": 4.359739303588867,
      "learning_rate": 2.8178270649579137e-06,
      "loss": 1.4875,
      "step": 4490
    },
    {
      "epoch": 3.2281205164992826,
      "grad_norm": 4.653151035308838,
      "learning_rate": 2.7975723703615243e-06,
      "loss": 1.5269,
      "step": 4500
    },
    {
      "epoch": 3.235294117647059,
      "grad_norm": 5.771918773651123,
      "learning_rate": 2.777362419795453e-06,
      "loss": 1.5689,
      "step": 4510
    },
    {
      "epoch": 3.242467718794835,
      "grad_norm": 5.164924144744873,
      "learning_rate": 2.757197623840567e-06,
      "loss": 1.5798,
      "step": 4520
    },
    {
      "epoch": 3.249641319942611,
      "grad_norm": 4.7157883644104,
      "learning_rate": 2.7370783921603814e-06,
      "loss": 1.5816,
      "step": 4530
    },
    {
      "epoch": 3.2568149210903874,
      "grad_norm": 5.150050640106201,
      "learning_rate": 2.7170051334927363e-06,
      "loss": 1.5646,
      "step": 4540
    },
    {
      "epoch": 3.2639885222381637,
      "grad_norm": 5.433572292327881,
      "learning_rate": 2.696978255641494e-06,
      "loss": 1.555,
      "step": 4550
    },
    {
      "epoch": 3.2711621233859396,
      "grad_norm": 5.139019966125488,
      "learning_rate": 2.676998165468251e-06,
      "loss": 1.5771,
      "step": 4560
    },
    {
      "epoch": 3.278335724533716,
      "grad_norm": 4.747610092163086,
      "learning_rate": 2.6570652688840827e-06,
      "loss": 1.654,
      "step": 4570
    },
    {
      "epoch": 3.2855093256814922,
      "grad_norm": 4.854026794433594,
      "learning_rate": 2.6371799708412883e-06,
      "loss": 1.4964,
      "step": 4580
    },
    {
      "epoch": 3.292682926829268,
      "grad_norm": 5.114801406860352,
      "learning_rate": 2.61734267532516e-06,
      "loss": 1.5618,
      "step": 4590
    },
    {
      "epoch": 3.2998565279770444,
      "grad_norm": 5.334105491638184,
      "learning_rate": 2.59755378534579e-06,
      "loss": 1.5508,
      "step": 4600
    },
    {
      "epoch": 3.3070301291248207,
      "grad_norm": 4.611128330230713,
      "learning_rate": 2.5778137029298656e-06,
      "loss": 1.4908,
      "step": 4610
    },
    {
      "epoch": 3.314203730272597,
      "grad_norm": 5.44845724105835,
      "learning_rate": 2.558122829112518e-06,
      "loss": 1.6345,
      "step": 4620
    },
    {
      "epoch": 3.321377331420373,
      "grad_norm": 4.544666290283203,
      "learning_rate": 2.5384815639291617e-06,
      "loss": 1.4958,
      "step": 4630
    },
    {
      "epoch": 3.3285509325681493,
      "grad_norm": 5.854852199554443,
      "learning_rate": 2.5188903064073766e-06,
      "loss": 1.5684,
      "step": 4640
    },
    {
      "epoch": 3.3357245337159256,
      "grad_norm": 5.334578990936279,
      "learning_rate": 2.499349454558797e-06,
      "loss": 1.521,
      "step": 4650
    },
    {
      "epoch": 3.3428981348637015,
      "grad_norm": 4.305833339691162,
      "learning_rate": 2.479859405371031e-06,
      "loss": 1.5645,
      "step": 4660
    },
    {
      "epoch": 3.350071736011478,
      "grad_norm": 5.3272294998168945,
      "learning_rate": 2.4604205547995796e-06,
      "loss": 1.5456,
      "step": 4670
    },
    {
      "epoch": 3.357245337159254,
      "grad_norm": 5.217097759246826,
      "learning_rate": 2.4410332977598152e-06,
      "loss": 1.553,
      "step": 4680
    },
    {
      "epoch": 3.36441893830703,
      "grad_norm": 5.688138484954834,
      "learning_rate": 2.421698028118943e-06,
      "loss": 1.6216,
      "step": 4690
    },
    {
      "epoch": 3.3715925394548063,
      "grad_norm": 4.944797039031982,
      "learning_rate": 2.402415138688007e-06,
      "loss": 1.5457,
      "step": 4700
    },
    {
      "epoch": 3.3787661406025826,
      "grad_norm": 4.4637837409973145,
      "learning_rate": 2.3831850212139026e-06,
      "loss": 1.5872,
      "step": 4710
    },
    {
      "epoch": 3.3859397417503585,
      "grad_norm": 4.911711692810059,
      "learning_rate": 2.3640080663714205e-06,
      "loss": 1.5921,
      "step": 4720
    },
    {
      "epoch": 3.393113342898135,
      "grad_norm": 5.935577869415283,
      "learning_rate": 2.3448846637553164e-06,
      "loss": 1.6214,
      "step": 4730
    },
    {
      "epoch": 3.400286944045911,
      "grad_norm": 4.930183410644531,
      "learning_rate": 2.325815201872388e-06,
      "loss": 1.599,
      "step": 4740
    },
    {
      "epoch": 3.407460545193687,
      "grad_norm": 4.976247787475586,
      "learning_rate": 2.306800068133588e-06,
      "loss": 1.5315,
      "step": 4750
    },
    {
      "epoch": 3.4146341463414633,
      "grad_norm": 5.872716426849365,
      "learning_rate": 2.287839648846146e-06,
      "loss": 1.6472,
      "step": 4760
    },
    {
      "epoch": 3.4218077474892397,
      "grad_norm": 4.896582126617432,
      "learning_rate": 2.2689343292057243e-06,
      "loss": 1.5724,
      "step": 4770
    },
    {
      "epoch": 3.4289813486370155,
      "grad_norm": 5.314901828765869,
      "learning_rate": 2.2500844932885983e-06,
      "loss": 1.6797,
      "step": 4780
    },
    {
      "epoch": 3.436154949784792,
      "grad_norm": 5.886935710906982,
      "learning_rate": 2.231290524043848e-06,
      "loss": 1.6353,
      "step": 4790
    },
    {
      "epoch": 3.443328550932568,
      "grad_norm": 4.129147052764893,
      "learning_rate": 2.2125528032855727e-06,
      "loss": 1.5804,
      "step": 4800
    },
    {
      "epoch": 3.4505021520803445,
      "grad_norm": 5.450525283813477,
      "learning_rate": 2.1938717116851445e-06,
      "loss": 1.5471,
      "step": 4810
    },
    {
      "epoch": 3.4576757532281204,
      "grad_norm": 5.247200012207031,
      "learning_rate": 2.175247628763472e-06,
      "loss": 1.5988,
      "step": 4820
    },
    {
      "epoch": 3.4648493543758967,
      "grad_norm": 4.37732458114624,
      "learning_rate": 2.156680932883283e-06,
      "loss": 1.5079,
      "step": 4830
    },
    {
      "epoch": 3.472022955523673,
      "grad_norm": 4.104616641998291,
      "learning_rate": 2.1381720012414435e-06,
      "loss": 1.5632,
      "step": 4840
    },
    {
      "epoch": 3.479196556671449,
      "grad_norm": 5.266754627227783,
      "learning_rate": 2.119721209861298e-06,
      "loss": 1.5,
      "step": 4850
    },
    {
      "epoch": 3.486370157819225,
      "grad_norm": 5.077564716339111,
      "learning_rate": 2.101328933585024e-06,
      "loss": 1.5267,
      "step": 4860
    },
    {
      "epoch": 3.4935437589670015,
      "grad_norm": 5.669919967651367,
      "learning_rate": 2.082995546066023e-06,
      "loss": 1.6237,
      "step": 4870
    },
    {
      "epoch": 3.500717360114778,
      "grad_norm": 5.102479457855225,
      "learning_rate": 2.064721419761315e-06,
      "loss": 1.5789,
      "step": 4880
    },
    {
      "epoch": 3.5078909612625537,
      "grad_norm": 3.2241008281707764,
      "learning_rate": 2.046506925923993e-06,
      "loss": 1.544,
      "step": 4890
    },
    {
      "epoch": 3.51506456241033,
      "grad_norm": 4.6451311111450195,
      "learning_rate": 2.0283524345956653e-06,
      "loss": 1.5873,
      "step": 4900
    },
    {
      "epoch": 3.5222381635581064,
      "grad_norm": 6.883460998535156,
      "learning_rate": 2.010258314598946e-06,
      "loss": 1.5733,
      "step": 4910
    },
    {
      "epoch": 3.5294117647058822,
      "grad_norm": 4.806614398956299,
      "learning_rate": 1.992224933529955e-06,
      "loss": 1.5909,
      "step": 4920
    },
    {
      "epoch": 3.5365853658536586,
      "grad_norm": 4.608686447143555,
      "learning_rate": 1.9742526577508523e-06,
      "loss": 1.5572,
      "step": 4930
    },
    {
      "epoch": 3.543758967001435,
      "grad_norm": 5.7697930335998535,
      "learning_rate": 1.956341852382401e-06,
      "loss": 1.4873,
      "step": 4940
    },
    {
      "epoch": 3.5509325681492108,
      "grad_norm": 5.565706729888916,
      "learning_rate": 1.9384928812965455e-06,
      "loss": 1.5612,
      "step": 4950
    },
    {
      "epoch": 3.558106169296987,
      "grad_norm": 5.111209869384766,
      "learning_rate": 1.9207061071090118e-06,
      "loss": 1.5712,
      "step": 4960
    },
    {
      "epoch": 3.5652797704447634,
      "grad_norm": 4.476255893707275,
      "learning_rate": 1.902981891171954e-06,
      "loss": 1.6026,
      "step": 4970
    },
    {
      "epoch": 3.5724533715925393,
      "grad_norm": 4.857025146484375,
      "learning_rate": 1.8853205935666058e-06,
      "loss": 1.5449,
      "step": 4980
    },
    {
      "epoch": 3.5796269727403156,
      "grad_norm": 5.673941135406494,
      "learning_rate": 1.8677225730959637e-06,
      "loss": 1.5438,
      "step": 4990
    },
    {
      "epoch": 3.586800573888092,
      "grad_norm": 6.18610143661499,
      "learning_rate": 1.8501881872774985e-06,
      "loss": 1.621,
      "step": 5000
    },
    {
      "epoch": 3.593974175035868,
      "grad_norm": 4.331984043121338,
      "learning_rate": 1.8327177923358992e-06,
      "loss": 1.5359,
      "step": 5010
    },
    {
      "epoch": 3.601147776183644,
      "grad_norm": 5.412600994110107,
      "learning_rate": 1.8153117431958261e-06,
      "loss": 1.4801,
      "step": 5020
    },
    {
      "epoch": 3.6083213773314204,
      "grad_norm": 4.536812782287598,
      "learning_rate": 1.7979703934747101e-06,
      "loss": 1.5801,
      "step": 5030
    },
    {
      "epoch": 3.6154949784791963,
      "grad_norm": 5.22519588470459,
      "learning_rate": 1.7806940954755514e-06,
      "loss": 1.4924,
      "step": 5040
    },
    {
      "epoch": 3.6226685796269726,
      "grad_norm": 5.099027633666992,
      "learning_rate": 1.7634832001797846e-06,
      "loss": 1.5815,
      "step": 5050
    },
    {
      "epoch": 3.629842180774749,
      "grad_norm": 3.9346470832824707,
      "learning_rate": 1.7463380572401338e-06,
      "loss": 1.535,
      "step": 5060
    },
    {
      "epoch": 3.637015781922525,
      "grad_norm": 5.890203475952148,
      "learning_rate": 1.7292590149735156e-06,
      "loss": 1.5535,
      "step": 5070
    },
    {
      "epoch": 3.644189383070301,
      "grad_norm": 5.094865322113037,
      "learning_rate": 1.7122464203539534e-06,
      "loss": 1.5333,
      "step": 5080
    },
    {
      "epoch": 3.6513629842180775,
      "grad_norm": 4.342983722686768,
      "learning_rate": 1.6953006190055427e-06,
      "loss": 1.5012,
      "step": 5090
    },
    {
      "epoch": 3.658536585365854,
      "grad_norm": 4.648706436157227,
      "learning_rate": 1.678421955195415e-06,
      "loss": 1.5979,
      "step": 5100
    },
    {
      "epoch": 3.6657101865136297,
      "grad_norm": 5.34298038482666,
      "learning_rate": 1.6616107718267572e-06,
      "loss": 1.6022,
      "step": 5110
    },
    {
      "epoch": 3.672883787661406,
      "grad_norm": 4.793816089630127,
      "learning_rate": 1.6448674104318308e-06,
      "loss": 1.5693,
      "step": 5120
    },
    {
      "epoch": 3.6800573888091823,
      "grad_norm": 4.6575236320495605,
      "learning_rate": 1.6281922111650477e-06,
      "loss": 1.5924,
      "step": 5130
    },
    {
      "epoch": 3.6872309899569586,
      "grad_norm": 4.251752853393555,
      "learning_rate": 1.6115855127960523e-06,
      "loss": 1.51,
      "step": 5140
    },
    {
      "epoch": 3.6944045911047345,
      "grad_norm": 5.318329334259033,
      "learning_rate": 1.5950476527028364e-06,
      "loss": 1.5596,
      "step": 5150
    },
    {
      "epoch": 3.701578192252511,
      "grad_norm": 5.296874046325684,
      "learning_rate": 1.5785789668648898e-06,
      "loss": 1.4518,
      "step": 5160
    },
    {
      "epoch": 3.708751793400287,
      "grad_norm": 5.162954807281494,
      "learning_rate": 1.5621797898563756e-06,
      "loss": 1.5602,
      "step": 5170
    },
    {
      "epoch": 3.715925394548063,
      "grad_norm": 6.11090612411499,
      "learning_rate": 1.5458504548393304e-06,
      "loss": 1.5193,
      "step": 5180
    },
    {
      "epoch": 3.7230989956958394,
      "grad_norm": 5.217702865600586,
      "learning_rate": 1.5295912935568985e-06,
      "loss": 1.5279,
      "step": 5190
    },
    {
      "epoch": 3.7302725968436157,
      "grad_norm": 5.033320903778076,
      "learning_rate": 1.5134026363265818e-06,
      "loss": 1.4719,
      "step": 5200
    },
    {
      "epoch": 3.7374461979913915,
      "grad_norm": 4.631922245025635,
      "learning_rate": 1.4972848120335453e-06,
      "loss": 1.5892,
      "step": 5210
    },
    {
      "epoch": 3.744619799139168,
      "grad_norm": 4.9406514167785645,
      "learning_rate": 1.4812381481239275e-06,
      "loss": 1.5816,
      "step": 5220
    },
    {
      "epoch": 3.751793400286944,
      "grad_norm": 4.462995529174805,
      "learning_rate": 1.4652629705981864e-06,
      "loss": 1.6148,
      "step": 5230
    },
    {
      "epoch": 3.75896700143472,
      "grad_norm": 4.862168312072754,
      "learning_rate": 1.4493596040044773e-06,
      "loss": 1.6271,
      "step": 5240
    },
    {
      "epoch": 3.7661406025824964,
      "grad_norm": 4.860846042633057,
      "learning_rate": 1.4335283714320635e-06,
      "loss": 1.5348,
      "step": 5250
    },
    {
      "epoch": 3.7733142037302727,
      "grad_norm": 4.886119842529297,
      "learning_rate": 1.417769594504746e-06,
      "loss": 1.5995,
      "step": 5260
    },
    {
      "epoch": 3.7804878048780486,
      "grad_norm": 5.1256103515625,
      "learning_rate": 1.4020835933743381e-06,
      "loss": 1.4807,
      "step": 5270
    },
    {
      "epoch": 3.787661406025825,
      "grad_norm": 5.317882537841797,
      "learning_rate": 1.3864706867141497e-06,
      "loss": 1.5778,
      "step": 5280
    },
    {
      "epoch": 3.7948350071736012,
      "grad_norm": 4.979245185852051,
      "learning_rate": 1.370931191712525e-06,
      "loss": 1.6162,
      "step": 5290
    },
    {
      "epoch": 3.802008608321377,
      "grad_norm": 5.403132438659668,
      "learning_rate": 1.3554654240663933e-06,
      "loss": 1.5577,
      "step": 5300
    },
    {
      "epoch": 3.8091822094691534,
      "grad_norm": 5.104295253753662,
      "learning_rate": 1.3400736979748514e-06,
      "loss": 1.4388,
      "step": 5310
    },
    {
      "epoch": 3.8163558106169297,
      "grad_norm": 4.928079605102539,
      "learning_rate": 1.3247563261327845e-06,
      "loss": 1.5412,
      "step": 5320
    },
    {
      "epoch": 3.8235294117647056,
      "grad_norm": 5.386422157287598,
      "learning_rate": 1.3095136197245173e-06,
      "loss": 1.4926,
      "step": 5330
    },
    {
      "epoch": 3.830703012912482,
      "grad_norm": 5.404440402984619,
      "learning_rate": 1.2943458884174865e-06,
      "loss": 1.584,
      "step": 5340
    },
    {
      "epoch": 3.8378766140602583,
      "grad_norm": 4.788195610046387,
      "learning_rate": 1.2792534403559515e-06,
      "loss": 1.6143,
      "step": 5350
    },
    {
      "epoch": 3.8450502152080346,
      "grad_norm": 5.342644691467285,
      "learning_rate": 1.264236582154732e-06,
      "loss": 1.4316,
      "step": 5360
    },
    {
      "epoch": 3.8522238163558105,
      "grad_norm": 5.3714165687561035,
      "learning_rate": 1.2492956188929818e-06,
      "loss": 1.6032,
      "step": 5370
    },
    {
      "epoch": 3.859397417503587,
      "grad_norm": 4.433401584625244,
      "learning_rate": 1.2344308541079909e-06,
      "loss": 1.5382,
      "step": 5380
    },
    {
      "epoch": 3.866571018651363,
      "grad_norm": 5.287528038024902,
      "learning_rate": 1.2196425897890196e-06,
      "loss": 1.5867,
      "step": 5390
    },
    {
      "epoch": 3.8737446197991394,
      "grad_norm": 5.831390857696533,
      "learning_rate": 1.2049311263711566e-06,
      "loss": 1.6121,
      "step": 5400
    },
    {
      "epoch": 3.8809182209469153,
      "grad_norm": 4.301455497741699,
      "learning_rate": 1.1902967627292267e-06,
      "loss": 1.5966,
      "step": 5410
    },
    {
      "epoch": 3.8880918220946916,
      "grad_norm": 5.953823566436768,
      "learning_rate": 1.1757397961717065e-06,
      "loss": 1.5696,
      "step": 5420
    },
    {
      "epoch": 3.895265423242468,
      "grad_norm": 4.747607707977295,
      "learning_rate": 1.1612605224346968e-06,
      "loss": 1.5963,
      "step": 5430
    },
    {
      "epoch": 3.902439024390244,
      "grad_norm": 4.652257442474365,
      "learning_rate": 1.1468592356759029e-06,
      "loss": 1.5514,
      "step": 5440
    },
    {
      "epoch": 3.90961262553802,
      "grad_norm": 5.274447441101074,
      "learning_rate": 1.132536228468668e-06,
      "loss": 1.599,
      "step": 5450
    },
    {
      "epoch": 3.9167862266857965,
      "grad_norm": 5.58336067199707,
      "learning_rate": 1.1182917917960245e-06,
      "loss": 1.488,
      "step": 5460
    },
    {
      "epoch": 3.9239598278335723,
      "grad_norm": 4.495450019836426,
      "learning_rate": 1.1041262150447818e-06,
      "loss": 1.5935,
      "step": 5470
    },
    {
      "epoch": 3.9311334289813487,
      "grad_norm": 4.961677551269531,
      "learning_rate": 1.0900397859996475e-06,
      "loss": 1.5829,
      "step": 5480
    },
    {
      "epoch": 3.938307030129125,
      "grad_norm": 4.9641642570495605,
      "learning_rate": 1.0760327908373858e-06,
      "loss": 1.555,
      "step": 5490
    },
    {
      "epoch": 3.945480631276901,
      "grad_norm": 5.4197587966918945,
      "learning_rate": 1.0621055141209974e-06,
      "loss": 1.6215,
      "step": 5500
    },
    {
      "epoch": 3.952654232424677,
      "grad_norm": 5.023562431335449,
      "learning_rate": 1.0482582387939434e-06,
      "loss": 1.4962,
      "step": 5510
    },
    {
      "epoch": 3.9598278335724535,
      "grad_norm": 6.0153679847717285,
      "learning_rate": 1.0344912461743905e-06,
      "loss": 1.5833,
      "step": 5520
    },
    {
      "epoch": 3.9670014347202294,
      "grad_norm": 5.2649736404418945,
      "learning_rate": 1.0208048159494998e-06,
      "loss": 1.571,
      "step": 5530
    },
    {
      "epoch": 3.9741750358680057,
      "grad_norm": 5.9556989669799805,
      "learning_rate": 1.0071992261697488e-06,
      "loss": 1.551,
      "step": 5540
    },
    {
      "epoch": 3.981348637015782,
      "grad_norm": 5.271484851837158,
      "learning_rate": 9.936747532432756e-07,
      "loss": 1.511,
      "step": 5550
    },
    {
      "epoch": 3.988522238163558,
      "grad_norm": 5.392214298248291,
      "learning_rate": 9.802316719302685e-07,
      "loss": 1.5962,
      "step": 5560
    },
    {
      "epoch": 3.995695839311334,
      "grad_norm": 4.591812610626221,
      "learning_rate": 9.668702553373783e-07,
      "loss": 1.552,
      "step": 5570
    },
    {
      "epoch": 4.0028694404591105,
      "grad_norm": 4.769450664520264,
      "learning_rate": 9.535907749121748e-07,
      "loss": 1.6148,
      "step": 5580
    },
    {
      "epoch": 4.010043041606886,
      "grad_norm": 5.467865943908691,
      "learning_rate": 9.403935004376324e-07,
      "loss": 1.532,
      "step": 5590
    },
    {
      "epoch": 4.017216642754663,
      "grad_norm": 5.384133815765381,
      "learning_rate": 9.272787000266481e-07,
      "loss": 1.534,
      "step": 5600
    },
    {
      "epoch": 4.024390243902439,
      "grad_norm": 4.185093879699707,
      "learning_rate": 9.142466401165905e-07,
      "loss": 1.5453,
      "step": 5610
    },
    {
      "epoch": 4.031563845050215,
      "grad_norm": 6.475484371185303,
      "learning_rate": 9.01297585463895e-07,
      "loss": 1.6128,
      "step": 5620
    },
    {
      "epoch": 4.038737446197992,
      "grad_norm": 5.212262153625488,
      "learning_rate": 8.884317991386765e-07,
      "loss": 1.5849,
      "step": 5630
    },
    {
      "epoch": 4.045911047345768,
      "grad_norm": 4.634660243988037,
      "learning_rate": 8.756495425193928e-07,
      "loss": 1.5719,
      "step": 5640
    },
    {
      "epoch": 4.053084648493543,
      "grad_norm": 5.472566604614258,
      "learning_rate": 8.629510752875269e-07,
      "loss": 1.5987,
      "step": 5650
    },
    {
      "epoch": 4.06025824964132,
      "grad_norm": 5.4323554039001465,
      "learning_rate": 8.503366554223191e-07,
      "loss": 1.6253,
      "step": 5660
    },
    {
      "epoch": 4.067431850789096,
      "grad_norm": 5.372097015380859,
      "learning_rate": 8.378065391955214e-07,
      "loss": 1.5273,
      "step": 5670
    },
    {
      "epoch": 4.074605451936872,
      "grad_norm": 4.8421311378479,
      "learning_rate": 8.253609811661889e-07,
      "loss": 1.5996,
      "step": 5680
    },
    {
      "epoch": 4.081779053084649,
      "grad_norm": 4.9463276863098145,
      "learning_rate": 8.130002341755122e-07,
      "loss": 1.5711,
      "step": 5690
    },
    {
      "epoch": 4.088952654232425,
      "grad_norm": 5.841972351074219,
      "learning_rate": 8.007245493416816e-07,
      "loss": 1.5504,
      "step": 5700
    },
    {
      "epoch": 4.0961262553802005,
      "grad_norm": 6.477831840515137,
      "learning_rate": 7.885341760547821e-07,
      "loss": 1.6076,
      "step": 5710
    },
    {
      "epoch": 4.103299856527977,
      "grad_norm": 5.354301929473877,
      "learning_rate": 7.764293619717306e-07,
      "loss": 1.5167,
      "step": 5720
    },
    {
      "epoch": 4.110473457675753,
      "grad_norm": 5.492114067077637,
      "learning_rate": 7.644103530112379e-07,
      "loss": 1.5084,
      "step": 5730
    },
    {
      "epoch": 4.117647058823529,
      "grad_norm": 4.487227916717529,
      "learning_rate": 7.524773933488194e-07,
      "loss": 1.5207,
      "step": 5740
    },
    {
      "epoch": 4.124820659971306,
      "grad_norm": 3.658602476119995,
      "learning_rate": 7.406307254118317e-07,
      "loss": 1.5472,
      "step": 5750
    },
    {
      "epoch": 4.131994261119082,
      "grad_norm": 5.526279926300049,
      "learning_rate": 7.288705898745501e-07,
      "loss": 1.5802,
      "step": 5760
    },
    {
      "epoch": 4.1391678622668575,
      "grad_norm": 4.549692153930664,
      "learning_rate": 7.171972256532728e-07,
      "loss": 1.5494,
      "step": 5770
    },
    {
      "epoch": 4.146341463414634,
      "grad_norm": 3.975756883621216,
      "learning_rate": 7.056108699014758e-07,
      "loss": 1.527,
      "step": 5780
    },
    {
      "epoch": 4.15351506456241,
      "grad_norm": 4.814899921417236,
      "learning_rate": 6.941117580049844e-07,
      "loss": 1.607,
      "step": 5790
    },
    {
      "epoch": 4.160688665710187,
      "grad_norm": 4.791717052459717,
      "learning_rate": 6.827001235772046e-07,
      "loss": 1.5518,
      "step": 5800
    },
    {
      "epoch": 4.167862266857963,
      "grad_norm": 5.410518169403076,
      "learning_rate": 6.713761984543621e-07,
      "loss": 1.5545,
      "step": 5810
    },
    {
      "epoch": 4.175035868005739,
      "grad_norm": 5.340766429901123,
      "learning_rate": 6.601402126908052e-07,
      "loss": 1.5153,
      "step": 5820
    },
    {
      "epoch": 4.182209469153515,
      "grad_norm": 4.721994876861572,
      "learning_rate": 6.489923945543252e-07,
      "loss": 1.5222,
      "step": 5830
    },
    {
      "epoch": 4.189383070301291,
      "grad_norm": 5.4740309715271,
      "learning_rate": 6.379329705215165e-07,
      "loss": 1.5838,
      "step": 5840
    },
    {
      "epoch": 4.196556671449067,
      "grad_norm": 4.800774574279785,
      "learning_rate": 6.269621652731794e-07,
      "loss": 1.5092,
      "step": 5850
    },
    {
      "epoch": 4.203730272596844,
      "grad_norm": 4.922849178314209,
      "learning_rate": 6.160802016897566e-07,
      "loss": 1.4868,
      "step": 5860
    },
    {
      "epoch": 4.21090387374462,
      "grad_norm": 5.378692626953125,
      "learning_rate": 6.052873008468019e-07,
      "loss": 1.6381,
      "step": 5870
    },
    {
      "epoch": 4.218077474892396,
      "grad_norm": 4.966732501983643,
      "learning_rate": 5.945836820104917e-07,
      "loss": 1.5419,
      "step": 5880
    },
    {
      "epoch": 4.2252510760401725,
      "grad_norm": 5.334605693817139,
      "learning_rate": 5.83969562633167e-07,
      "loss": 1.588,
      "step": 5890
    },
    {
      "epoch": 4.232424677187948,
      "grad_norm": 5.5960798263549805,
      "learning_rate": 5.734451583489175e-07,
      "loss": 1.6004,
      "step": 5900
    },
    {
      "epoch": 4.239598278335724,
      "grad_norm": 5.061458110809326,
      "learning_rate": 5.630106829692033e-07,
      "loss": 1.5754,
      "step": 5910
    },
    {
      "epoch": 4.246771879483501,
      "grad_norm": 6.128307342529297,
      "learning_rate": 5.526663484785078e-07,
      "loss": 1.5745,
      "step": 5920
    },
    {
      "epoch": 4.253945480631277,
      "grad_norm": 5.41403865814209,
      "learning_rate": 5.4241236503003e-07,
      "loss": 1.5339,
      "step": 5930
    },
    {
      "epoch": 4.261119081779053,
      "grad_norm": 5.486804962158203,
      "learning_rate": 5.322489409414217e-07,
      "loss": 1.5743,
      "step": 5940
    },
    {
      "epoch": 4.2682926829268295,
      "grad_norm": 4.526071071624756,
      "learning_rate": 5.221762826905457e-07,
      "loss": 1.5375,
      "step": 5950
    },
    {
      "epoch": 4.275466284074605,
      "grad_norm": 4.754627227783203,
      "learning_rate": 5.121945949112906e-07,
      "loss": 1.5177,
      "step": 5960
    },
    {
      "epoch": 4.282639885222381,
      "grad_norm": 4.931293487548828,
      "learning_rate": 5.023040803894064e-07,
      "loss": 1.5616,
      "step": 5970
    },
    {
      "epoch": 4.289813486370158,
      "grad_norm": 4.713810920715332,
      "learning_rate": 4.925049400583881e-07,
      "loss": 1.5494,
      "step": 5980
    },
    {
      "epoch": 4.296987087517934,
      "grad_norm": 4.366137504577637,
      "learning_rate": 4.82797372995395e-07,
      "loss": 1.5724,
      "step": 5990
    },
    {
      "epoch": 4.30416068866571,
      "grad_norm": 4.825933456420898,
      "learning_rate": 4.731815764172004e-07,
      "loss": 1.6041,
      "step": 6000
    },
    {
      "epoch": 4.3113342898134865,
      "grad_norm": 6.756141185760498,
      "learning_rate": 4.6365774567619147e-07,
      "loss": 1.616,
      "step": 6010
    },
    {
      "epoch": 4.318507890961262,
      "grad_norm": 5.259617328643799,
      "learning_rate": 4.54226074256397e-07,
      "loss": 1.6195,
      "step": 6020
    },
    {
      "epoch": 4.325681492109039,
      "grad_norm": 5.099273204803467,
      "learning_rate": 4.448867537695578e-07,
      "loss": 1.6057,
      "step": 6030
    },
    {
      "epoch": 4.332855093256815,
      "grad_norm": 4.967963218688965,
      "learning_rate": 4.3563997395123516e-07,
      "loss": 1.5583,
      "step": 6040
    },
    {
      "epoch": 4.340028694404591,
      "grad_norm": 4.548673629760742,
      "learning_rate": 4.2648592265695175e-07,
      "loss": 1.5588,
      "step": 6050
    },
    {
      "epoch": 4.347202295552368,
      "grad_norm": 7.799607276916504,
      "learning_rate": 4.174247858583785e-07,
      "loss": 1.5248,
      "step": 6060
    },
    {
      "epoch": 4.354375896700144,
      "grad_norm": 5.822380065917969,
      "learning_rate": 4.0845674763955847e-07,
      "loss": 1.5841,
      "step": 6070
    },
    {
      "epoch": 4.3615494978479195,
      "grad_norm": 4.543388366699219,
      "learning_rate": 3.995819901931641e-07,
      "loss": 1.5539,
      "step": 6080
    },
    {
      "epoch": 4.368723098995696,
      "grad_norm": 4.43087100982666,
      "learning_rate": 3.908006938167935e-07,
      "loss": 1.5439,
      "step": 6090
    },
    {
      "epoch": 4.375896700143472,
      "grad_norm": 4.168930530548096,
      "learning_rate": 3.8211303690931457e-07,
      "loss": 1.474,
      "step": 6100
    },
    {
      "epoch": 4.383070301291248,
      "grad_norm": 5.6229424476623535,
      "learning_rate": 3.73519195967233e-07,
      "loss": 1.5646,
      "step": 6110
    },
    {
      "epoch": 4.390243902439025,
      "grad_norm": 4.116856098175049,
      "learning_rate": 3.6501934558111296e-07,
      "loss": 1.595,
      "step": 6120
    },
    {
      "epoch": 4.397417503586801,
      "grad_norm": 4.537616729736328,
      "learning_rate": 3.5661365843202455e-07,
      "loss": 1.5475,
      "step": 6130
    },
    {
      "epoch": 4.4045911047345765,
      "grad_norm": 5.016225814819336,
      "learning_rate": 3.483023052880413e-07,
      "loss": 1.5251,
      "step": 6140
    },
    {
      "epoch": 4.411764705882353,
      "grad_norm": 5.361878395080566,
      "learning_rate": 3.400854550007676e-07,
      "loss": 1.5611,
      "step": 6150
    },
    {
      "epoch": 4.418938307030129,
      "grad_norm": 4.979213237762451,
      "learning_rate": 3.3196327450190634e-07,
      "loss": 1.5408,
      "step": 6160
    },
    {
      "epoch": 4.426111908177905,
      "grad_norm": 4.567112445831299,
      "learning_rate": 3.2393592879987224e-07,
      "loss": 1.6662,
      "step": 6170
    },
    {
      "epoch": 4.433285509325682,
      "grad_norm": 5.373532295227051,
      "learning_rate": 3.160035809764367e-07,
      "loss": 1.5307,
      "step": 6180
    },
    {
      "epoch": 4.440459110473458,
      "grad_norm": 5.461113929748535,
      "learning_rate": 3.081663921834155e-07,
      "loss": 1.5457,
      "step": 6190
    },
    {
      "epoch": 4.4476327116212335,
      "grad_norm": 5.217315196990967,
      "learning_rate": 3.004245216393953e-07,
      "loss": 1.6304,
      "step": 6200
    },
    {
      "epoch": 4.45480631276901,
      "grad_norm": 4.839243412017822,
      "learning_rate": 2.9277812662649784e-07,
      "loss": 1.5147,
      "step": 6210
    },
    {
      "epoch": 4.461979913916786,
      "grad_norm": 5.170962810516357,
      "learning_rate": 2.852273624871832e-07,
      "loss": 1.4946,
      "step": 6220
    },
    {
      "epoch": 4.469153515064562,
      "grad_norm": 5.686546325683594,
      "learning_rate": 2.777723826210993e-07,
      "loss": 1.4707,
      "step": 6230
    },
    {
      "epoch": 4.476327116212339,
      "grad_norm": 5.424300670623779,
      "learning_rate": 2.7041333848196125e-07,
      "loss": 1.6089,
      "step": 6240
    },
    {
      "epoch": 4.483500717360115,
      "grad_norm": 5.544449806213379,
      "learning_rate": 2.6315037957447275e-07,
      "loss": 1.5302,
      "step": 6250
    },
    {
      "epoch": 4.490674318507891,
      "grad_norm": 4.650031089782715,
      "learning_rate": 2.559836534512944e-07,
      "loss": 1.5011,
      "step": 6260
    },
    {
      "epoch": 4.497847919655667,
      "grad_norm": 4.722929000854492,
      "learning_rate": 2.4891330571004066e-07,
      "loss": 1.5914,
      "step": 6270
    },
    {
      "epoch": 4.505021520803443,
      "grad_norm": 5.2164998054504395,
      "learning_rate": 2.4193947999032516e-07,
      "loss": 1.5574,
      "step": 6280
    },
    {
      "epoch": 4.512195121951219,
      "grad_norm": 5.75557804107666,
      "learning_rate": 2.3506231797084278e-07,
      "loss": 1.5442,
      "step": 6290
    },
    {
      "epoch": 4.519368723098996,
      "grad_norm": 6.58354377746582,
      "learning_rate": 2.2828195936648756e-07,
      "loss": 1.523,
      "step": 6300
    },
    {
      "epoch": 4.526542324246772,
      "grad_norm": 4.863304138183594,
      "learning_rate": 2.2159854192551955e-07,
      "loss": 1.543,
      "step": 6310
    },
    {
      "epoch": 4.533715925394548,
      "grad_norm": 5.428411483764648,
      "learning_rate": 2.1501220142676083e-07,
      "loss": 1.6058,
      "step": 6320
    },
    {
      "epoch": 4.540889526542324,
      "grad_norm": 3.772670030593872,
      "learning_rate": 2.085230716768427e-07,
      "loss": 1.5988,
      "step": 6330
    },
    {
      "epoch": 4.5480631276901,
      "grad_norm": 5.76611852645874,
      "learning_rate": 2.0213128450748187e-07,
      "loss": 1.4545,
      "step": 6340
    },
    {
      "epoch": 4.555236728837876,
      "grad_norm": 4.6880903244018555,
      "learning_rate": 1.9583696977280743e-07,
      "loss": 1.6427,
      "step": 6350
    },
    {
      "epoch": 4.562410329985653,
      "grad_norm": 5.376224517822266,
      "learning_rate": 1.8964025534671859e-07,
      "loss": 1.5467,
      "step": 6360
    },
    {
      "epoch": 4.569583931133429,
      "grad_norm": 5.279082775115967,
      "learning_rate": 1.8354126712028853e-07,
      "loss": 1.4995,
      "step": 6370
    },
    {
      "epoch": 4.5767575322812055,
      "grad_norm": 4.825498580932617,
      "learning_rate": 1.7754012899920626e-07,
      "loss": 1.4841,
      "step": 6380
    },
    {
      "epoch": 4.583931133428981,
      "grad_norm": 4.6418352127075195,
      "learning_rate": 1.7163696290126096e-07,
      "loss": 1.5491,
      "step": 6390
    },
    {
      "epoch": 4.591104734576757,
      "grad_norm": 5.144545555114746,
      "learning_rate": 1.658318887538629e-07,
      "loss": 1.6299,
      "step": 6400
    },
    {
      "epoch": 4.598278335724534,
      "grad_norm": 4.674914360046387,
      "learning_rate": 1.6012502449160916e-07,
      "loss": 1.5804,
      "step": 6410
    },
    {
      "epoch": 4.60545193687231,
      "grad_norm": 4.781933307647705,
      "learning_rate": 1.5451648605388614e-07,
      "loss": 1.518,
      "step": 6420
    },
    {
      "epoch": 4.612625538020086,
      "grad_norm": 6.448437690734863,
      "learning_rate": 1.4900638738251262e-07,
      "loss": 1.5381,
      "step": 6430
    },
    {
      "epoch": 4.619799139167863,
      "grad_norm": 5.147371768951416,
      "learning_rate": 1.435948404194304e-07,
      "loss": 1.5747,
      "step": 6440
    },
    {
      "epoch": 4.626972740315638,
      "grad_norm": 5.4571027755737305,
      "learning_rate": 1.3828195510442565e-07,
      "loss": 1.5495,
      "step": 6450
    },
    {
      "epoch": 4.634146341463414,
      "grad_norm": 4.925133228302002,
      "learning_rate": 1.3306783937289404e-07,
      "loss": 1.6238,
      "step": 6460
    },
    {
      "epoch": 4.641319942611191,
      "grad_norm": 4.075822830200195,
      "learning_rate": 1.2795259915365343e-07,
      "loss": 1.5271,
      "step": 6470
    },
    {
      "epoch": 4.648493543758967,
      "grad_norm": 5.278754234313965,
      "learning_rate": 1.22936338366787e-07,
      "loss": 1.602,
      "step": 6480
    },
    {
      "epoch": 4.655667144906743,
      "grad_norm": 4.3164191246032715,
      "learning_rate": 1.1801915892153459e-07,
      "loss": 1.5712,
      "step": 6490
    },
    {
      "epoch": 4.66284074605452,
      "grad_norm": 5.421845436096191,
      "learning_rate": 1.1320116071422027e-07,
      "loss": 1.5725,
      "step": 6500
    },
    {
      "epoch": 4.6700143472022955,
      "grad_norm": 5.4106950759887695,
      "learning_rate": 1.084824416262259e-07,
      "loss": 1.5732,
      "step": 6510
    },
    {
      "epoch": 4.677187948350072,
      "grad_norm": 5.200441837310791,
      "learning_rate": 1.0386309752200008e-07,
      "loss": 1.5423,
      "step": 6520
    },
    {
      "epoch": 4.684361549497848,
      "grad_norm": 5.308101177215576,
      "learning_rate": 9.934322224711023e-08,
      "loss": 1.5925,
      "step": 6530
    },
    {
      "epoch": 4.691535150645624,
      "grad_norm": 6.078038215637207,
      "learning_rate": 9.49229076263386e-08,
      "loss": 1.5483,
      "step": 6540
    },
    {
      "epoch": 4.698708751793401,
      "grad_norm": 4.5854082107543945,
      "learning_rate": 9.060224346181479e-08,
      "loss": 1.5674,
      "step": 6550
    },
    {
      "epoch": 4.705882352941177,
      "grad_norm": 4.320549964904785,
      "learning_rate": 8.638131753119172e-08,
      "loss": 1.5809,
      "step": 6560
    },
    {
      "epoch": 4.7130559540889525,
      "grad_norm": 4.821507453918457,
      "learning_rate": 8.226021558586317e-08,
      "loss": 1.5141,
      "step": 6570
    },
    {
      "epoch": 4.720229555236729,
      "grad_norm": 4.8386993408203125,
      "learning_rate": 7.823902134921957e-08,
      "loss": 1.5517,
      "step": 6580
    },
    {
      "epoch": 4.727403156384505,
      "grad_norm": 4.854640483856201,
      "learning_rate": 7.431781651495052e-08,
      "loss": 1.5652,
      "step": 6590
    },
    {
      "epoch": 4.734576757532281,
      "grad_norm": 5.482609272003174,
      "learning_rate": 7.049668074538107e-08,
      "loss": 1.4974,
      "step": 6600
    },
    {
      "epoch": 4.741750358680058,
      "grad_norm": 5.888434410095215,
      "learning_rate": 6.677569166985754e-08,
      "loss": 1.5129,
      "step": 6610
    },
    {
      "epoch": 4.748923959827834,
      "grad_norm": 4.847434997558594,
      "learning_rate": 6.315492488316588e-08,
      "loss": 1.4825,
      "step": 6620
    },
    {
      "epoch": 4.7560975609756095,
      "grad_norm": 5.54952335357666,
      "learning_rate": 5.963445394400081e-08,
      "loss": 1.5537,
      "step": 6630
    },
    {
      "epoch": 4.763271162123386,
      "grad_norm": 4.192146301269531,
      "learning_rate": 5.6214350373466344e-08,
      "loss": 1.5968,
      "step": 6640
    },
    {
      "epoch": 4.770444763271162,
      "grad_norm": 5.233739376068115,
      "learning_rate": 5.2894683653628154e-08,
      "loss": 1.5972,
      "step": 6650
    },
    {
      "epoch": 4.777618364418938,
      "grad_norm": 4.97914981842041,
      "learning_rate": 4.967552122609631e-08,
      "loss": 1.5565,
      "step": 6660
    },
    {
      "epoch": 4.784791965566715,
      "grad_norm": 5.352022647857666,
      "learning_rate": 4.655692849065974e-08,
      "loss": 1.5332,
      "step": 6670
    },
    {
      "epoch": 4.791965566714491,
      "grad_norm": 5.402764797210693,
      "learning_rate": 4.353896880395614e-08,
      "loss": 1.5475,
      "step": 6680
    },
    {
      "epoch": 4.799139167862267,
      "grad_norm": 4.799407958984375,
      "learning_rate": 4.0621703478183615e-08,
      "loss": 1.5979,
      "step": 6690
    },
    {
      "epoch": 4.806312769010043,
      "grad_norm": 4.489550590515137,
      "learning_rate": 3.7805191779854974e-08,
      "loss": 1.5552,
      "step": 6700
    },
    {
      "epoch": 4.813486370157819,
      "grad_norm": 4.7592668533325195,
      "learning_rate": 3.508949092859759e-08,
      "loss": 1.5344,
      "step": 6710
    },
    {
      "epoch": 4.820659971305595,
      "grad_norm": 5.178126811981201,
      "learning_rate": 3.247465609598599e-08,
      "loss": 1.54,
      "step": 6720
    },
    {
      "epoch": 4.827833572453372,
      "grad_norm": 6.217395305633545,
      "learning_rate": 2.996074040442387e-08,
      "loss": 1.5156,
      "step": 6730
    },
    {
      "epoch": 4.835007173601148,
      "grad_norm": 5.165589809417725,
      "learning_rate": 2.75477949260633e-08,
      "loss": 1.5403,
      "step": 6740
    },
    {
      "epoch": 4.842180774748924,
      "grad_norm": 5.951997756958008,
      "learning_rate": 2.523586868176886e-08,
      "loss": 1.5443,
      "step": 6750
    },
    {
      "epoch": 4.8493543758967,
      "grad_norm": 6.381235599517822,
      "learning_rate": 2.3025008640119585e-08,
      "loss": 1.5253,
      "step": 6760
    },
    {
      "epoch": 4.856527977044476,
      "grad_norm": 5.9743123054504395,
      "learning_rate": 2.0915259716458026e-08,
      "loss": 1.5627,
      "step": 6770
    },
    {
      "epoch": 4.863701578192252,
      "grad_norm": 5.8238525390625,
      "learning_rate": 1.8906664771973782e-08,
      "loss": 1.4988,
      "step": 6780
    },
    {
      "epoch": 4.870875179340029,
      "grad_norm": 4.637222766876221,
      "learning_rate": 1.69992646128353e-08,
      "loss": 1.5541,
      "step": 6790
    },
    {
      "epoch": 4.878048780487805,
      "grad_norm": 3.7741501331329346,
      "learning_rate": 1.519309798936053e-08,
      "loss": 1.5857,
      "step": 6800
    },
    {
      "epoch": 4.885222381635581,
      "grad_norm": 4.047990322113037,
      "learning_rate": 1.3488201595229788e-08,
      "loss": 1.5054,
      "step": 6810
    },
    {
      "epoch": 4.892395982783357,
      "grad_norm": 5.732033729553223,
      "learning_rate": 1.1884610066738578e-08,
      "loss": 1.5874,
      "step": 6820
    },
    {
      "epoch": 4.899569583931133,
      "grad_norm": 3.214785099029541,
      "learning_rate": 1.0382355982095915e-08,
      "loss": 1.4917,
      "step": 6830
    },
    {
      "epoch": 4.906743185078909,
      "grad_norm": 5.24675989151001,
      "learning_rate": 8.981469860763204e-09,
      "loss": 1.5662,
      "step": 6840
    },
    {
      "epoch": 4.913916786226686,
      "grad_norm": 4.3426194190979,
      "learning_rate": 7.681980162830283e-09,
      "loss": 1.5066,
      "step": 6850
    },
    {
      "epoch": 4.921090387374462,
      "grad_norm": 5.073109149932861,
      "learning_rate": 6.4839132884414455e-09,
      "loss": 1.508,
      "step": 6860
    },
    {
      "epoch": 4.928263988522238,
      "grad_norm": 4.572563171386719,
      "learning_rate": 5.387293577257535e-09,
      "loss": 1.5744,
      "step": 6870
    },
    {
      "epoch": 4.9354375896700144,
      "grad_norm": 4.89202356338501,
      "learning_rate": 4.392143307960784e-09,
      "loss": 1.6095,
      "step": 6880
    },
    {
      "epoch": 4.94261119081779,
      "grad_norm": 5.313533782958984,
      "learning_rate": 3.4984826978029518e-09,
      "loss": 1.4975,
      "step": 6890
    },
    {
      "epoch": 4.949784791965567,
      "grad_norm": 5.6334333419799805,
      "learning_rate": 2.7063299021939937e-09,
      "loss": 1.6024,
      "step": 6900
    },
    {
      "epoch": 4.956958393113343,
      "grad_norm": 4.961485862731934,
      "learning_rate": 2.015701014334015e-09,
      "loss": 1.5286,
      "step": 6910
    },
    {
      "epoch": 4.964131994261119,
      "grad_norm": 4.927677154541016,
      "learning_rate": 1.4266100648868687e-09,
      "loss": 1.5679,
      "step": 6920
    },
    {
      "epoch": 4.971305595408896,
      "grad_norm": 4.683927536010742,
      "learning_rate": 9.390690216926069e-10,
      "loss": 1.5469,
      "step": 6930
    },
    {
      "epoch": 4.9784791965566715,
      "grad_norm": 5.903906345367432,
      "learning_rate": 5.530877895271181e-10,
      "loss": 1.6453,
      "step": 6940
    },
    {
      "epoch": 4.985652797704447,
      "grad_norm": 3.654639482498169,
      "learning_rate": 2.6867420989895586e-10,
      "loss": 1.5184,
      "step": 6950
    },
    {
      "epoch": 4.992826398852224,
      "grad_norm": 5.286595344543457,
      "learning_rate": 8.583406089168745e-11,
      "loss": 1.5109,
      "step": 6960
    },
    {
      "epoch": 5.0,
      "grad_norm": 6.265029430389404,
      "learning_rate": 4.5710570445445246e-12,
      "loss": 1.5594,
      "step": 6970
    },
    {
      "epoch": 5.0,
      "step": 6970,
      "total_flos": 2.3256068955144192e+17,
      "train_loss": 1.6362066318178108,
      "train_runtime": 1837.8159,
      "train_samples_per_second": 15.167,
      "train_steps_per_second": 3.793
    }
  ],
  "logging_steps": 10,
  "max_steps": 6970,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500.0,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": false,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.3256068955144192e+17,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
