{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 2970,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.016835016835016835,
      "grad_norm": 1.9351600408554077,
      "learning_rate": 4.999886713385432e-05,
      "loss": 3.4221,
      "step": 10
    },
    {
      "epoch": 0.03367003367003367,
      "grad_norm": 1.9942336082458496,
      "learning_rate": 4.999546863808815e-05,
      "loss": 3.1125,
      "step": 20
    },
    {
      "epoch": 0.050505050505050504,
      "grad_norm": 2.2318530082702637,
      "learning_rate": 4.998903569331016e-05,
      "loss": 2.8342,
      "step": 30
    },
    {
      "epoch": 0.06734006734006734,
      "grad_norm": 3.7640864849090576,
      "learning_rate": 4.9979806779900255e-05,
      "loss": 2.5459,
      "step": 40
    },
    {
      "epoch": 0.08417508417508418,
      "grad_norm": 2.661804437637329,
      "learning_rate": 4.996778293046141e-05,
      "loss": 2.5472,
      "step": 50
    },
    {
      "epoch": 0.10101010101010101,
      "grad_norm": 3.1736788749694824,
      "learning_rate": 4.995296549031585e-05,
      "loss": 2.2898,
      "step": 60
    },
    {
      "epoch": 0.11784511784511785,
      "grad_norm": 3.20106840133667,
      "learning_rate": 4.993535611735463e-05,
      "loss": 2.3689,
      "step": 70
    },
    {
      "epoch": 0.13468013468013468,
      "grad_norm": 3.9566051959991455,
      "learning_rate": 4.991495678185202e-05,
      "loss": 2.249,
      "step": 80
    },
    {
      "epoch": 0.15151515151515152,
      "grad_norm": 3.93385910987854,
      "learning_rate": 4.989176976624511e-05,
      "loss": 2.2512,
      "step": 90
    },
    {
      "epoch": 0.16835016835016836,
      "grad_norm": 2.0667226314544678,
      "learning_rate": 4.9865797664878456e-05,
      "loss": 2.355,
      "step": 100
    },
    {
      "epoch": 0.18518518518518517,
      "grad_norm": 2.8819310665130615,
      "learning_rate": 4.9837043383713753e-05,
      "loss": 2.0975,
      "step": 110
    },
    {
      "epoch": 0.20202020202020202,
      "grad_norm": 5.311926364898682,
      "learning_rate": 4.980551014000474e-05,
      "loss": 2.2198,
      "step": 120
    },
    {
      "epoch": 0.21885521885521886,
      "grad_norm": 2.7744176387786865,
      "learning_rate": 4.977120146193717e-05,
      "loss": 2.1643,
      "step": 130
    },
    {
      "epoch": 0.2356902356902357,
      "grad_norm": 4.199899673461914,
      "learning_rate": 4.973412118823412e-05,
      "loss": 2.1081,
      "step": 140
    },
    {
      "epoch": 0.25252525252525254,
      "grad_norm": 3.3204638957977295,
      "learning_rate": 4.9694273467726426e-05,
      "loss": 2.1071,
      "step": 150
    },
    {
      "epoch": 0.26936026936026936,
      "grad_norm": 3.677358388900757,
      "learning_rate": 4.965166275888854e-05,
      "loss": 2.2366,
      "step": 160
    },
    {
      "epoch": 0.28619528619528617,
      "grad_norm": 3.1014490127563477,
      "learning_rate": 4.9606293829339595e-05,
      "loss": 2.0227,
      "step": 170
    },
    {
      "epoch": 0.30303030303030304,
      "grad_norm": 3.574652671813965,
      "learning_rate": 4.955817175531005e-05,
      "loss": 2.1479,
      "step": 180
    },
    {
      "epoch": 0.31986531986531985,
      "grad_norm": 4.0574750900268555,
      "learning_rate": 4.950730192107368e-05,
      "loss": 2.143,
      "step": 190
    },
    {
      "epoch": 0.3367003367003367,
      "grad_norm": 3.7521677017211914,
      "learning_rate": 4.9453690018345144e-05,
      "loss": 2.1293,
      "step": 200
    },
    {
      "epoch": 0.35353535353535354,
      "grad_norm": 4.88776969909668,
      "learning_rate": 4.939734204564316e-05,
      "loss": 2.1737,
      "step": 210
    },
    {
      "epoch": 0.37037037037037035,
      "grad_norm": 4.180499076843262,
      "learning_rate": 4.933826430761933e-05,
      "loss": 2.1436,
      "step": 220
    },
    {
      "epoch": 0.3872053872053872,
      "grad_norm": 4.208598613739014,
      "learning_rate": 4.9276463414352757e-05,
      "loss": 2.067,
      "step": 230
    },
    {
      "epoch": 0.40404040404040403,
      "grad_norm": 4.210498809814453,
      "learning_rate": 4.921194628061043e-05,
      "loss": 1.8995,
      "step": 240
    },
    {
      "epoch": 0.4208754208754209,
      "grad_norm": 2.889301300048828,
      "learning_rate": 4.9144720125073534e-05,
      "loss": 2.2204,
      "step": 250
    },
    {
      "epoch": 0.4377104377104377,
      "grad_norm": 3.584834337234497,
      "learning_rate": 4.9074792469529815e-05,
      "loss": 2.1572,
      "step": 260
    },
    {
      "epoch": 0.45454545454545453,
      "grad_norm": 3.833918571472168,
      "learning_rate": 4.9002171138031924e-05,
      "loss": 2.0936,
      "step": 270
    },
    {
      "epoch": 0.4713804713804714,
      "grad_norm": 3.2904863357543945,
      "learning_rate": 4.8926864256022074e-05,
      "loss": 2.1792,
      "step": 280
    },
    {
      "epoch": 0.4882154882154882,
      "grad_norm": 4.310300350189209,
      "learning_rate": 4.8848880249422815e-05,
      "loss": 2.0307,
      "step": 290
    },
    {
      "epoch": 0.5050505050505051,
      "grad_norm": 4.05359411239624,
      "learning_rate": 4.8768227843694356e-05,
      "loss": 1.8997,
      "step": 300
    },
    {
      "epoch": 0.5218855218855218,
      "grad_norm": 5.452977657318115,
      "learning_rate": 4.868491606285823e-05,
      "loss": 1.9798,
      "step": 310
    },
    {
      "epoch": 0.5387205387205387,
      "grad_norm": 4.680150508880615,
      "learning_rate": 4.859895422848767e-05,
      "loss": 2.1215,
      "step": 320
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 4.147307872772217,
      "learning_rate": 4.851035195866459e-05,
      "loss": 2.1791,
      "step": 330
    },
    {
      "epoch": 0.5723905723905723,
      "grad_norm": 4.335159778594971,
      "learning_rate": 4.841911916690346e-05,
      "loss": 2.1197,
      "step": 340
    },
    {
      "epoch": 0.5892255892255892,
      "grad_norm": 6.012970924377441,
      "learning_rate": 4.832526606104213e-05,
      "loss": 2.0389,
      "step": 350
    },
    {
      "epoch": 0.6060606060606061,
      "grad_norm": 3.8238539695739746,
      "learning_rate": 4.8228803142099646e-05,
      "loss": 1.8789,
      "step": 360
    },
    {
      "epoch": 0.622895622895623,
      "grad_norm": 4.082944393157959,
      "learning_rate": 4.812974120310134e-05,
      "loss": 2.0145,
      "step": 370
    },
    {
      "epoch": 0.6397306397306397,
      "grad_norm": 6.6342997550964355,
      "learning_rate": 4.802809132787125e-05,
      "loss": 2.1742,
      "step": 380
    },
    {
      "epoch": 0.6565656565656566,
      "grad_norm": 4.439262390136719,
      "learning_rate": 4.792386488979193e-05,
      "loss": 2.1201,
      "step": 390
    },
    {
      "epoch": 0.6734006734006734,
      "grad_norm": 5.378384113311768,
      "learning_rate": 4.781707355053191e-05,
      "loss": 1.9708,
      "step": 400
    },
    {
      "epoch": 0.6902356902356902,
      "grad_norm": 5.224155426025391,
      "learning_rate": 4.770772925874093e-05,
      "loss": 1.9897,
      "step": 410
    },
    {
      "epoch": 0.7070707070707071,
      "grad_norm": 4.198266506195068,
      "learning_rate": 4.759584424871302e-05,
      "loss": 2.0944,
      "step": 420
    },
    {
      "epoch": 0.7239057239057239,
      "grad_norm": 4.056090831756592,
      "learning_rate": 4.74814310390176e-05,
      "loss": 2.0181,
      "step": 430
    },
    {
      "epoch": 0.7407407407407407,
      "grad_norm": 6.250126838684082,
      "learning_rate": 4.7364502431098844e-05,
      "loss": 2.0663,
      "step": 440
    },
    {
      "epoch": 0.7575757575757576,
      "grad_norm": 5.959021091461182,
      "learning_rate": 4.7245071507843354e-05,
      "loss": 1.7896,
      "step": 450
    },
    {
      "epoch": 0.7744107744107744,
      "grad_norm": 5.61577844619751,
      "learning_rate": 4.7123151632116296e-05,
      "loss": 1.9118,
      "step": 460
    },
    {
      "epoch": 0.7912457912457912,
      "grad_norm": 3.036355972290039,
      "learning_rate": 4.6998756445266336e-05,
      "loss": 2.0683,
      "step": 470
    },
    {
      "epoch": 0.8080808080808081,
      "grad_norm": 3.6176133155822754,
      "learning_rate": 4.687189986559925e-05,
      "loss": 1.9694,
      "step": 480
    },
    {
      "epoch": 0.8249158249158249,
      "grad_norm": 5.4774861335754395,
      "learning_rate": 4.6742596086820726e-05,
      "loss": 1.9622,
      "step": 490
    },
    {
      "epoch": 0.8417508417508418,
      "grad_norm": 7.045623779296875,
      "learning_rate": 4.6610859576448176e-05,
      "loss": 2.2727,
      "step": 500
    },
    {
      "epoch": 0.8585858585858586,
      "grad_norm": 5.457861423492432,
      "learning_rate": 4.647670507419206e-05,
      "loss": 2.0935,
      "step": 510
    },
    {
      "epoch": 0.8754208754208754,
      "grad_norm": 6.9665141105651855,
      "learning_rate": 4.634014759030667e-05,
      "loss": 1.8942,
      "step": 520
    },
    {
      "epoch": 0.8922558922558923,
      "grad_norm": 4.721338272094727,
      "learning_rate": 4.620120240391065e-05,
      "loss": 1.7507,
      "step": 530
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 4.766576290130615,
      "learning_rate": 4.605988506127749e-05,
      "loss": 2.0513,
      "step": 540
    },
    {
      "epoch": 0.9259259259259259,
      "grad_norm": 4.273691177368164,
      "learning_rate": 4.591621137409603e-05,
      "loss": 2.0285,
      "step": 550
    },
    {
      "epoch": 0.9427609427609428,
      "grad_norm": 6.440671920776367,
      "learning_rate": 4.5770197417701365e-05,
      "loss": 2.0996,
      "step": 560
    },
    {
      "epoch": 0.9595959595959596,
      "grad_norm": 6.246161460876465,
      "learning_rate": 4.562185952927622e-05,
      "loss": 2.1686,
      "step": 570
    },
    {
      "epoch": 0.9764309764309764,
      "grad_norm": 5.683747291564941,
      "learning_rate": 4.547121430602299e-05,
      "loss": 1.9668,
      "step": 580
    },
    {
      "epoch": 0.9932659932659933,
      "grad_norm": 7.5102081298828125,
      "learning_rate": 4.53182786033067e-05,
      "loss": 2.0976,
      "step": 590
    },
    {
      "epoch": 1.0101010101010102,
      "grad_norm": 5.841650485992432,
      "learning_rate": 4.5163069532769165e-05,
      "loss": 1.9558,
      "step": 600
    },
    {
      "epoch": 1.026936026936027,
      "grad_norm": 4.748441219329834,
      "learning_rate": 4.500560446041432e-05,
      "loss": 1.6833,
      "step": 610
    },
    {
      "epoch": 1.0437710437710437,
      "grad_norm": 6.2728657722473145,
      "learning_rate": 4.4845901004665234e-05,
      "loss": 1.9197,
      "step": 620
    },
    {
      "epoch": 1.0606060606060606,
      "grad_norm": 4.447951793670654,
      "learning_rate": 4.468397703439282e-05,
      "loss": 1.7594,
      "step": 630
    },
    {
      "epoch": 1.0774410774410774,
      "grad_norm": 6.6496076583862305,
      "learning_rate": 4.4519850666916484e-05,
      "loss": 1.9044,
      "step": 640
    },
    {
      "epoch": 1.0942760942760943,
      "grad_norm": 6.893066883087158,
      "learning_rate": 4.4353540265977064e-05,
      "loss": 1.8795,
      "step": 650
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 4.086156845092773,
      "learning_rate": 4.418506443968213e-05,
      "loss": 1.839,
      "step": 660
    },
    {
      "epoch": 1.127946127946128,
      "grad_norm": 7.211981773376465,
      "learning_rate": 4.4014442038423954e-05,
      "loss": 1.9437,
      "step": 670
    },
    {
      "epoch": 1.144781144781145,
      "grad_norm": 6.692904949188232,
      "learning_rate": 4.384169215277041e-05,
      "loss": 1.9157,
      "step": 680
    },
    {
      "epoch": 1.1616161616161615,
      "grad_norm": 9.482995986938477,
      "learning_rate": 4.366683411132895e-05,
      "loss": 1.9634,
      "step": 690
    },
    {
      "epoch": 1.1784511784511784,
      "grad_norm": 4.321144104003906,
      "learning_rate": 4.3489887478583966e-05,
      "loss": 2.0268,
      "step": 700
    },
    {
      "epoch": 1.1952861952861953,
      "grad_norm": 5.905852794647217,
      "learning_rate": 4.331087205270777e-05,
      "loss": 1.9373,
      "step": 710
    },
    {
      "epoch": 1.2121212121212122,
      "grad_norm": 5.254656791687012,
      "learning_rate": 4.312980786334544e-05,
      "loss": 1.9449,
      "step": 720
    },
    {
      "epoch": 1.228956228956229,
      "grad_norm": 5.411879062652588,
      "learning_rate": 4.29467151693737e-05,
      "loss": 1.6988,
      "step": 730
    },
    {
      "epoch": 1.2457912457912457,
      "grad_norm": 7.439530372619629,
      "learning_rate": 4.276161445663423e-05,
      "loss": 2.0845,
      "step": 740
    },
    {
      "epoch": 1.2626262626262625,
      "grad_norm": 3.942002773284912,
      "learning_rate": 4.257452643564155e-05,
      "loss": 1.9471,
      "step": 750
    },
    {
      "epoch": 1.2794612794612794,
      "grad_norm": 5.405117511749268,
      "learning_rate": 4.2385472039265763e-05,
      "loss": 1.9759,
      "step": 760
    },
    {
      "epoch": 1.2962962962962963,
      "grad_norm": 6.889535427093506,
      "learning_rate": 4.219447242039043e-05,
      "loss": 1.9419,
      "step": 770
    },
    {
      "epoch": 1.3131313131313131,
      "grad_norm": 5.009925365447998,
      "learning_rate": 4.200154894954578e-05,
      "loss": 1.8693,
      "step": 780
    },
    {
      "epoch": 1.32996632996633,
      "grad_norm": 3.735913038253784,
      "learning_rate": 4.180672321251765e-05,
      "loss": 1.9619,
      "step": 790
    },
    {
      "epoch": 1.3468013468013469,
      "grad_norm": 7.774379253387451,
      "learning_rate": 4.161001700793231e-05,
      "loss": 1.7933,
      "step": 800
    },
    {
      "epoch": 1.3636363636363638,
      "grad_norm": 6.884219169616699,
      "learning_rate": 4.1411452344817406e-05,
      "loss": 2.0585,
      "step": 810
    },
    {
      "epoch": 1.3804713804713804,
      "grad_norm": 7.734115123748779,
      "learning_rate": 4.121105144013946e-05,
      "loss": 1.9413,
      "step": 820
    },
    {
      "epoch": 1.3973063973063973,
      "grad_norm": 6.02816915512085,
      "learning_rate": 4.100883671631806e-05,
      "loss": 1.9875,
      "step": 830
    },
    {
      "epoch": 1.4141414141414141,
      "grad_norm": 7.747981071472168,
      "learning_rate": 4.080483079871705e-05,
      "loss": 1.8081,
      "step": 840
    },
    {
      "epoch": 1.430976430976431,
      "grad_norm": 6.544225692749023,
      "learning_rate": 4.059905651311304e-05,
      "loss": 1.5942,
      "step": 850
    },
    {
      "epoch": 1.4478114478114479,
      "grad_norm": 5.622461318969727,
      "learning_rate": 4.039153688314145e-05,
      "loss": 1.9285,
      "step": 860
    },
    {
      "epoch": 1.4646464646464645,
      "grad_norm": 8.182441711425781,
      "learning_rate": 4.018229512772052e-05,
      "loss": 1.8828,
      "step": 870
    },
    {
      "epoch": 1.4814814814814814,
      "grad_norm": 7.070849895477295,
      "learning_rate": 3.997135465845331e-05,
      "loss": 1.8246,
      "step": 880
    },
    {
      "epoch": 1.4983164983164983,
      "grad_norm": 6.528000354766846,
      "learning_rate": 3.975873907700825e-05,
      "loss": 1.7039,
      "step": 890
    },
    {
      "epoch": 1.5151515151515151,
      "grad_norm": 5.553628921508789,
      "learning_rate": 3.954447217247844e-05,
      "loss": 1.9482,
      "step": 900
    },
    {
      "epoch": 1.531986531986532,
      "grad_norm": 7.805751323699951,
      "learning_rate": 3.932857791871991e-05,
      "loss": 1.9199,
      "step": 910
    },
    {
      "epoch": 1.5488215488215489,
      "grad_norm": 9.87844181060791,
      "learning_rate": 3.911108047166924e-05,
      "loss": 2.0501,
      "step": 920
    },
    {
      "epoch": 1.5656565656565657,
      "grad_norm": 3.6853933334350586,
      "learning_rate": 3.889200416664078e-05,
      "loss": 1.9586,
      "step": 930
    },
    {
      "epoch": 1.5824915824915826,
      "grad_norm": 8.344951629638672,
      "learning_rate": 3.86713735156039e-05,
      "loss": 1.8876,
      "step": 940
    },
    {
      "epoch": 1.5993265993265995,
      "grad_norm": 8.677186965942383,
      "learning_rate": 3.844921320444031e-05,
      "loss": 2.0682,
      "step": 950
    },
    {
      "epoch": 1.6161616161616161,
      "grad_norm": 9.795968055725098,
      "learning_rate": 3.8225548090182087e-05,
      "loss": 2.0241,
      "step": 960
    },
    {
      "epoch": 1.632996632996633,
      "grad_norm": 7.494648456573486,
      "learning_rate": 3.8000403198230387e-05,
      "loss": 1.7351,
      "step": 970
    },
    {
      "epoch": 1.6498316498316499,
      "grad_norm": 8.197412490844727,
      "learning_rate": 3.7773803719555514e-05,
      "loss": 1.6885,
      "step": 980
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 7.125362396240234,
      "learning_rate": 3.754577500787828e-05,
      "loss": 1.9149,
      "step": 990
    },
    {
      "epoch": 1.6835016835016834,
      "grad_norm": 7.4945454597473145,
      "learning_rate": 3.7316342576833254e-05,
      "loss": 1.6481,
      "step": 1000
    },
    {
      "epoch": 1.7003367003367003,
      "grad_norm": 7.6977081298828125,
      "learning_rate": 3.708553209711409e-05,
      "loss": 1.9842,
      "step": 1010
    },
    {
      "epoch": 1.7171717171717171,
      "grad_norm": 7.0687642097473145,
      "learning_rate": 3.6876645774874876e-05,
      "loss": 1.7009,
      "step": 1020
    },
    {
      "epoch": 1.734006734006734,
      "grad_norm": 7.190761089324951,
      "learning_rate": 3.664328827532799e-05,
      "loss": 1.8958,
      "step": 1030
    },
    {
      "epoch": 1.7508417508417509,
      "grad_norm": 6.6077470779418945,
      "learning_rate": 3.640862803368294e-05,
      "loss": 1.7684,
      "step": 1040
    },
    {
      "epoch": 1.7676767676767677,
      "grad_norm": 5.9207611083984375,
      "learning_rate": 3.61726913055617e-05,
      "loss": 1.8376,
      "step": 1050
    },
    {
      "epoch": 1.7845117845117846,
      "grad_norm": 6.087248802185059,
      "learning_rate": 3.593550448940956e-05,
      "loss": 1.9188,
      "step": 1060
    },
    {
      "epoch": 1.8013468013468015,
      "grad_norm": 7.9521050453186035,
      "learning_rate": 3.569709412354136e-05,
      "loss": 1.8275,
      "step": 1070
    },
    {
      "epoch": 1.8181818181818183,
      "grad_norm": 5.325812816619873,
      "learning_rate": 3.545748688317232e-05,
      "loss": 1.7507,
      "step": 1080
    },
    {
      "epoch": 1.835016835016835,
      "grad_norm": 4.298901557922363,
      "learning_rate": 3.52167095774333e-05,
      "loss": 1.8607,
      "step": 1090
    },
    {
      "epoch": 1.8518518518518519,
      "grad_norm": 5.763392925262451,
      "learning_rate": 3.4974789146371216e-05,
      "loss": 2.0108,
      "step": 1100
    },
    {
      "epoch": 1.8686868686868687,
      "grad_norm": 5.7830491065979,
      "learning_rate": 3.4731752657934794e-05,
      "loss": 2.0541,
      "step": 1110
    },
    {
      "epoch": 1.8855218855218854,
      "grad_norm": 5.188854694366455,
      "learning_rate": 3.448762730494596e-05,
      "loss": 1.8939,
      "step": 1120
    },
    {
      "epoch": 1.9023569023569022,
      "grad_norm": 6.678033351898193,
      "learning_rate": 3.4242440402057353e-05,
      "loss": 1.9459,
      "step": 1130
    },
    {
      "epoch": 1.9191919191919191,
      "grad_norm": 7.730269908905029,
      "learning_rate": 3.399621938269606e-05,
      "loss": 1.6984,
      "step": 1140
    },
    {
      "epoch": 1.936026936026936,
      "grad_norm": 6.1070709228515625,
      "learning_rate": 3.3748991795994266e-05,
      "loss": 2.0175,
      "step": 1150
    },
    {
      "epoch": 1.9528619528619529,
      "grad_norm": 10.151352882385254,
      "learning_rate": 3.3500785303706735e-05,
      "loss": 1.7996,
      "step": 1160
    },
    {
      "epoch": 1.9696969696969697,
      "grad_norm": 7.052657604217529,
      "learning_rate": 3.325162767711583e-05,
      "loss": 1.8741,
      "step": 1170
    },
    {
      "epoch": 1.9865319865319866,
      "grad_norm": 6.718871593475342,
      "learning_rate": 3.3001546793924285e-05,
      "loss": 1.6732,
      "step": 1180
    },
    {
      "epoch": 2.0033670033670035,
      "grad_norm": 9.324509620666504,
      "learning_rate": 3.2750570635135996e-05,
      "loss": 1.8342,
      "step": 1190
    },
    {
      "epoch": 2.0202020202020203,
      "grad_norm": 6.056127548217773,
      "learning_rate": 3.249872728192527e-05,
      "loss": 1.8317,
      "step": 1200
    },
    {
      "epoch": 2.037037037037037,
      "grad_norm": 6.105113983154297,
      "learning_rate": 3.224604491249491e-05,
      "loss": 1.8346,
      "step": 1210
    },
    {
      "epoch": 2.053872053872054,
      "grad_norm": 8.159685134887695,
      "learning_rate": 3.199255179892342e-05,
      "loss": 1.6991,
      "step": 1220
    },
    {
      "epoch": 2.0707070707070705,
      "grad_norm": 11.380085945129395,
      "learning_rate": 3.17382763040017e-05,
      "loss": 1.9616,
      "step": 1230
    },
    {
      "epoch": 2.0875420875420874,
      "grad_norm": 6.695614337921143,
      "learning_rate": 3.148324687805958e-05,
      "loss": 1.7377,
      "step": 1240
    },
    {
      "epoch": 2.1043771043771042,
      "grad_norm": 7.677597999572754,
      "learning_rate": 3.1227492055782565e-05,
      "loss": 1.5805,
      "step": 1250
    },
    {
      "epoch": 2.121212121212121,
      "grad_norm": 8.116870880126953,
      "learning_rate": 3.097104045301922e-05,
      "loss": 1.5124,
      "step": 1260
    },
    {
      "epoch": 2.138047138047138,
      "grad_norm": 7.607891082763672,
      "learning_rate": 3.071392076357934e-05,
      "loss": 1.8855,
      "step": 1270
    },
    {
      "epoch": 2.154882154882155,
      "grad_norm": 9.036705017089844,
      "learning_rate": 3.045616175602344e-05,
      "loss": 1.8334,
      "step": 1280
    },
    {
      "epoch": 2.1717171717171717,
      "grad_norm": 6.182394504547119,
      "learning_rate": 3.0197792270443982e-05,
      "loss": 1.6362,
      "step": 1290
    },
    {
      "epoch": 2.1885521885521886,
      "grad_norm": 12.997217178344727,
      "learning_rate": 2.993884121523849e-05,
      "loss": 1.7044,
      "step": 1300
    },
    {
      "epoch": 2.2053872053872055,
      "grad_norm": 8.415644645690918,
      "learning_rate": 2.967933756387502e-05,
      "loss": 1.7536,
      "step": 1310
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 7.762467384338379,
      "learning_rate": 2.9419310351650392e-05,
      "loss": 1.692,
      "step": 1320
    },
    {
      "epoch": 2.239057239057239,
      "grad_norm": 9.195417404174805,
      "learning_rate": 2.915878867244155e-05,
      "loss": 1.8036,
      "step": 1330
    },
    {
      "epoch": 2.255892255892256,
      "grad_norm": 7.638357162475586,
      "learning_rate": 2.8897801675450238e-05,
      "loss": 1.9723,
      "step": 1340
    },
    {
      "epoch": 2.2727272727272725,
      "grad_norm": 8.022603034973145,
      "learning_rate": 2.8636378561941592e-05,
      "loss": 1.7802,
      "step": 1350
    },
    {
      "epoch": 2.28956228956229,
      "grad_norm": 4.964784622192383,
      "learning_rate": 2.837454858197688e-05,
      "loss": 1.6914,
      "step": 1360
    },
    {
      "epoch": 2.3063973063973062,
      "grad_norm": 7.727941513061523,
      "learning_rate": 2.811234103114077e-05,
      "loss": 1.8201,
      "step": 1370
    },
    {
      "epoch": 2.323232323232323,
      "grad_norm": 10.991243362426758,
      "learning_rate": 2.7849785247263515e-05,
      "loss": 1.6086,
      "step": 1380
    },
    {
      "epoch": 2.34006734006734,
      "grad_norm": 7.4205522537231445,
      "learning_rate": 2.7586910607138393e-05,
      "loss": 1.9782,
      "step": 1390
    },
    {
      "epoch": 2.356902356902357,
      "grad_norm": 11.88499641418457,
      "learning_rate": 2.732374652323481e-05,
      "loss": 1.7479,
      "step": 1400
    },
    {
      "epoch": 2.3737373737373737,
      "grad_norm": 9.063752174377441,
      "learning_rate": 2.706032244040741e-05,
      "loss": 1.6978,
      "step": 1410
    },
    {
      "epoch": 2.3905723905723906,
      "grad_norm": 6.272618293762207,
      "learning_rate": 2.6796667832601546e-05,
      "loss": 1.741,
      "step": 1420
    },
    {
      "epoch": 2.4074074074074074,
      "grad_norm": 8.449617385864258,
      "learning_rate": 2.65328121995555e-05,
      "loss": 1.8539,
      "step": 1430
    },
    {
      "epoch": 2.4242424242424243,
      "grad_norm": 7.275777816772461,
      "learning_rate": 2.626878506349986e-05,
      "loss": 1.6896,
      "step": 1440
    },
    {
      "epoch": 2.441077441077441,
      "grad_norm": 8.660381317138672,
      "learning_rate": 2.6004615965854268e-05,
      "loss": 1.8836,
      "step": 1450
    },
    {
      "epoch": 2.457912457912458,
      "grad_norm": 7.2675461769104,
      "learning_rate": 2.5740334463922172e-05,
      "loss": 1.8448,
      "step": 1460
    },
    {
      "epoch": 2.474747474747475,
      "grad_norm": 7.239981651306152,
      "learning_rate": 2.5475970127583666e-05,
      "loss": 1.6305,
      "step": 1470
    },
    {
      "epoch": 2.4915824915824913,
      "grad_norm": 9.237354278564453,
      "learning_rate": 2.5211552535987026e-05,
      "loss": 1.7776,
      "step": 1480
    },
    {
      "epoch": 2.5084175084175087,
      "grad_norm": 7.33435583114624,
      "learning_rate": 2.4947111274239135e-05,
      "loss": 1.7225,
      "step": 1490
    },
    {
      "epoch": 2.525252525252525,
      "grad_norm": 9.404363632202148,
      "learning_rate": 2.4682675930095263e-05,
      "loss": 1.7033,
      "step": 1500
    },
    {
      "epoch": 2.542087542087542,
      "grad_norm": 6.2729010581970215,
      "learning_rate": 2.4418276090648596e-05,
      "loss": 1.7534,
      "step": 1510
    },
    {
      "epoch": 2.558922558922559,
      "grad_norm": 11.741033554077148,
      "learning_rate": 2.4153941339019758e-05,
      "loss": 1.6855,
      "step": 1520
    },
    {
      "epoch": 2.5757575757575757,
      "grad_norm": 11.013845443725586,
      "learning_rate": 2.388970125104685e-05,
      "loss": 1.6974,
      "step": 1530
    },
    {
      "epoch": 2.5925925925925926,
      "grad_norm": 9.93186092376709,
      "learning_rate": 2.3625585391976255e-05,
      "loss": 1.9069,
      "step": 1540
    },
    {
      "epoch": 2.6094276094276094,
      "grad_norm": 7.268558502197266,
      "learning_rate": 2.3361623313154667e-05,
      "loss": 1.4998,
      "step": 1550
    },
    {
      "epoch": 2.6262626262626263,
      "grad_norm": 8.466529846191406,
      "learning_rate": 2.309784454872262e-05,
      "loss": 1.6081,
      "step": 1560
    },
    {
      "epoch": 2.643097643097643,
      "grad_norm": 7.920555114746094,
      "learning_rate": 2.2834278612310044e-05,
      "loss": 1.7917,
      "step": 1570
    },
    {
      "epoch": 2.65993265993266,
      "grad_norm": 7.8569016456604,
      "learning_rate": 2.2570954993733974e-05,
      "loss": 1.8361,
      "step": 1580
    },
    {
      "epoch": 2.676767676767677,
      "grad_norm": 7.943778038024902,
      "learning_rate": 2.2307903155699027e-05,
      "loss": 1.6581,
      "step": 1590
    },
    {
      "epoch": 2.6936026936026938,
      "grad_norm": 7.023823261260986,
      "learning_rate": 2.2045152530500908e-05,
      "loss": 1.9883,
      "step": 1600
    },
    {
      "epoch": 2.71043771043771,
      "grad_norm": 11.853245735168457,
      "learning_rate": 2.1782732516733274e-05,
      "loss": 1.8445,
      "step": 1610
    },
    {
      "epoch": 2.7272727272727275,
      "grad_norm": 7.591684341430664,
      "learning_rate": 2.1520672475998373e-05,
      "loss": 1.7342,
      "step": 1620
    },
    {
      "epoch": 2.744107744107744,
      "grad_norm": 7.581677436828613,
      "learning_rate": 2.125900172962186e-05,
      "loss": 1.7152,
      "step": 1630
    },
    {
      "epoch": 2.760942760942761,
      "grad_norm": 9.742855072021484,
      "learning_rate": 2.0997749555372125e-05,
      "loss": 1.8942,
      "step": 1640
    },
    {
      "epoch": 2.7777777777777777,
      "grad_norm": 5.88519811630249,
      "learning_rate": 2.0736945184184405e-05,
      "loss": 1.8061,
      "step": 1650
    },
    {
      "epoch": 2.7946127946127945,
      "grad_norm": 8.416365623474121,
      "learning_rate": 2.047661779689024e-05,
      "loss": 1.6805,
      "step": 1660
    },
    {
      "epoch": 2.8114478114478114,
      "grad_norm": 10.872653007507324,
      "learning_rate": 2.0216796520952485e-05,
      "loss": 1.776,
      "step": 1670
    },
    {
      "epoch": 2.8282828282828283,
      "grad_norm": 7.053093910217285,
      "learning_rate": 1.9957510427206295e-05,
      "loss": 1.5224,
      "step": 1680
    },
    {
      "epoch": 2.845117845117845,
      "grad_norm": 7.731687545776367,
      "learning_rate": 1.9698788526606464e-05,
      "loss": 1.5159,
      "step": 1690
    },
    {
      "epoch": 2.861952861952862,
      "grad_norm": 8.087037086486816,
      "learning_rate": 1.944065976698144e-05,
      "loss": 1.7645,
      "step": 1700
    },
    {
      "epoch": 2.878787878787879,
      "grad_norm": 8.534259796142578,
      "learning_rate": 1.918315302979444e-05,
      "loss": 1.7799,
      "step": 1710
    },
    {
      "epoch": 2.8956228956228958,
      "grad_norm": 9.995275497436523,
      "learning_rate": 1.8926297126911933e-05,
      "loss": 1.7421,
      "step": 1720
    },
    {
      "epoch": 2.9124579124579126,
      "grad_norm": 6.562170028686523,
      "learning_rate": 1.8670120797379958e-05,
      "loss": 1.7664,
      "step": 1730
    },
    {
      "epoch": 2.929292929292929,
      "grad_norm": 10.267624855041504,
      "learning_rate": 1.8414652704208583e-05,
      "loss": 1.7402,
      "step": 1740
    },
    {
      "epoch": 2.9461279461279464,
      "grad_norm": 7.58815336227417,
      "learning_rate": 1.8159921431164862e-05,
      "loss": 1.7449,
      "step": 1750
    },
    {
      "epoch": 2.962962962962963,
      "grad_norm": 9.813706398010254,
      "learning_rate": 1.7905955479574633e-05,
      "loss": 1.8668,
      "step": 1760
    },
    {
      "epoch": 2.9797979797979797,
      "grad_norm": 9.183123588562012,
      "learning_rate": 1.7652783265133606e-05,
      "loss": 1.821,
      "step": 1770
    },
    {
      "epoch": 2.9966329966329965,
      "grad_norm": 9.679153442382812,
      "learning_rate": 1.7400433114727967e-05,
      "loss": 1.8144,
      "step": 1780
    },
    {
      "epoch": 3.0134680134680134,
      "grad_norm": 8.182656288146973,
      "learning_rate": 1.7148933263264967e-05,
      "loss": 1.7525,
      "step": 1790
    },
    {
      "epoch": 3.0303030303030303,
      "grad_norm": 8.944281578063965,
      "learning_rate": 1.6898311850513738e-05,
      "loss": 1.5695,
      "step": 1800
    },
    {
      "epoch": 3.047138047138047,
      "grad_norm": 9.481423377990723,
      "learning_rate": 1.664859691795685e-05,
      "loss": 1.6659,
      "step": 1810
    },
    {
      "epoch": 3.063973063973064,
      "grad_norm": 11.901848793029785,
      "learning_rate": 1.639981640565276e-05,
      "loss": 1.726,
      "step": 1820
    },
    {
      "epoch": 3.080808080808081,
      "grad_norm": 11.86637020111084,
      "learning_rate": 1.615199814910971e-05,
      "loss": 1.6769,
      "step": 1830
    },
    {
      "epoch": 3.0976430976430978,
      "grad_norm": 7.784008979797363,
      "learning_rate": 1.5905169876171223e-05,
      "loss": 1.547,
      "step": 1840
    },
    {
      "epoch": 3.1144781144781146,
      "grad_norm": 9.511136054992676,
      "learning_rate": 1.565935920391371e-05,
      "loss": 1.5979,
      "step": 1850
    },
    {
      "epoch": 3.1313131313131315,
      "grad_norm": 10.025003433227539,
      "learning_rate": 1.541459363555652e-05,
      "loss": 1.6993,
      "step": 1860
    },
    {
      "epoch": 3.148148148148148,
      "grad_norm": 10.598386764526367,
      "learning_rate": 1.517090055738454e-05,
      "loss": 1.6776,
      "step": 1870
    },
    {
      "epoch": 3.164983164983165,
      "grad_norm": 9.152552604675293,
      "learning_rate": 1.4928307235684114e-05,
      "loss": 1.4836,
      "step": 1880
    },
    {
      "epoch": 3.1818181818181817,
      "grad_norm": 12.540054321289062,
      "learning_rate": 1.4686840813692224e-05,
      "loss": 1.6403,
      "step": 1890
    },
    {
      "epoch": 3.1986531986531985,
      "grad_norm": 9.056733131408691,
      "learning_rate": 1.4446528308559501e-05,
      "loss": 1.6071,
      "step": 1900
    },
    {
      "epoch": 3.2154882154882154,
      "grad_norm": 7.396214008331299,
      "learning_rate": 1.4207396608327344e-05,
      "loss": 1.863,
      "step": 1910
    },
    {
      "epoch": 3.2323232323232323,
      "grad_norm": 8.274188041687012,
      "learning_rate": 1.3969472468919461e-05,
      "loss": 1.753,
      "step": 1920
    },
    {
      "epoch": 3.249158249158249,
      "grad_norm": 6.352065563201904,
      "learning_rate": 1.3732782511148245e-05,
      "loss": 1.6832,
      "step": 1930
    },
    {
      "epoch": 3.265993265993266,
      "grad_norm": 9.059965133666992,
      "learning_rate": 1.3497353217736176e-05,
      "loss": 1.6165,
      "step": 1940
    },
    {
      "epoch": 3.282828282828283,
      "grad_norm": 8.970723152160645,
      "learning_rate": 1.3263210930352737e-05,
      "loss": 1.4732,
      "step": 1950
    },
    {
      "epoch": 3.2996632996632997,
      "grad_norm": 11.025619506835938,
      "learning_rate": 1.303038184666715e-05,
      "loss": 1.6684,
      "step": 1960
    },
    {
      "epoch": 3.3164983164983166,
      "grad_norm": 12.507800102233887,
      "learning_rate": 1.2798892017417127e-05,
      "loss": 1.634,
      "step": 1970
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 11.249629020690918,
      "learning_rate": 1.256876734349413e-05,
      "loss": 1.6504,
      "step": 1980
    },
    {
      "epoch": 3.3501683501683504,
      "grad_norm": 8.683276176452637,
      "learning_rate": 1.2340033573045359e-05,
      "loss": 1.6495,
      "step": 1990
    },
    {
      "epoch": 3.3670033670033668,
      "grad_norm": 11.7792329788208,
      "learning_rate": 1.2112716298592894e-05,
      "loss": 1.4539,
      "step": 2000
    },
    {
      "epoch": 3.3838383838383836,
      "grad_norm": 10.972962379455566,
      "learning_rate": 1.1886840954170142e-05,
      "loss": 1.784,
      "step": 2010
    },
    {
      "epoch": 3.4006734006734005,
      "grad_norm": 9.004392623901367,
      "learning_rate": 1.1662432812476118e-05,
      "loss": 1.6403,
      "step": 2020
    },
    {
      "epoch": 3.4175084175084174,
      "grad_norm": 10.815110206604004,
      "learning_rate": 1.1439516982047727e-05,
      "loss": 1.5597,
      "step": 2030
    },
    {
      "epoch": 3.4343434343434343,
      "grad_norm": 9.899258613586426,
      "learning_rate": 1.1218118404450422e-05,
      "loss": 1.685,
      "step": 2040
    },
    {
      "epoch": 3.451178451178451,
      "grad_norm": 10.685590744018555,
      "learning_rate": 1.0998261851487557e-05,
      "loss": 1.7797,
      "step": 2050
    },
    {
      "epoch": 3.468013468013468,
      "grad_norm": 9.22020149230957,
      "learning_rate": 1.0779971922428711e-05,
      "loss": 1.5132,
      "step": 2060
    },
    {
      "epoch": 3.484848484848485,
      "grad_norm": 8.398581504821777,
      "learning_rate": 1.0563273041257332e-05,
      "loss": 1.7018,
      "step": 2070
    },
    {
      "epoch": 3.5016835016835017,
      "grad_norm": 11.449586868286133,
      "learning_rate": 1.0348189453938018e-05,
      "loss": 1.7965,
      "step": 2080
    },
    {
      "epoch": 3.5185185185185186,
      "grad_norm": 10.282944679260254,
      "learning_rate": 1.0134745225703638e-05,
      "loss": 1.5314,
      "step": 2090
    },
    {
      "epoch": 3.5353535353535355,
      "grad_norm": 9.445771217346191,
      "learning_rate": 9.922964238362762e-06,
      "loss": 1.6492,
      "step": 2100
    },
    {
      "epoch": 3.5521885521885523,
      "grad_norm": 10.630270957946777,
      "learning_rate": 9.712870187627574e-06,
      "loss": 1.5527,
      "step": 2110
    },
    {
      "epoch": 3.569023569023569,
      "grad_norm": 7.706526756286621,
      "learning_rate": 9.504486580462595e-06,
      "loss": 1.805,
      "step": 2120
    },
    {
      "epoch": 3.5858585858585856,
      "grad_norm": 8.090899467468262,
      "learning_rate": 9.297836732454564e-06,
      "loss": 1.69,
      "step": 2130
    },
    {
      "epoch": 3.602693602693603,
      "grad_norm": 8.639691352844238,
      "learning_rate": 9.092943765203683e-06,
      "loss": 1.677,
      "step": 2140
    },
    {
      "epoch": 3.6195286195286194,
      "grad_norm": 9.372014045715332,
      "learning_rate": 8.889830603736604e-06,
      "loss": 1.8671,
      "step": 2150
    },
    {
      "epoch": 3.6363636363636362,
      "grad_norm": 12.574713706970215,
      "learning_rate": 8.6885199739414e-06,
      "loss": 1.6287,
      "step": 2160
    },
    {
      "epoch": 3.653198653198653,
      "grad_norm": 8.895698547363281,
      "learning_rate": 8.489034400024812e-06,
      "loss": 1.7373,
      "step": 2170
    },
    {
      "epoch": 3.67003367003367,
      "grad_norm": 7.549201965332031,
      "learning_rate": 8.291396201992063e-06,
      "loss": 1.6845,
      "step": 2180
    },
    {
      "epoch": 3.686868686868687,
      "grad_norm": 9.444300651550293,
      "learning_rate": 8.09562749314952e-06,
      "loss": 1.6234,
      "step": 2190
    },
    {
      "epoch": 3.7037037037037037,
      "grad_norm": 7.815389156341553,
      "learning_rate": 7.901750177630493e-06,
      "loss": 1.6349,
      "step": 2200
    },
    {
      "epoch": 3.7205387205387206,
      "grad_norm": 9.617527961730957,
      "learning_rate": 7.709785947944411e-06,
      "loss": 1.7319,
      "step": 2210
    },
    {
      "epoch": 3.7373737373737375,
      "grad_norm": 10.505480766296387,
      "learning_rate": 7.519756282549734e-06,
      "loss": 1.7762,
      "step": 2220
    },
    {
      "epoch": 3.7542087542087543,
      "grad_norm": 9.371417999267578,
      "learning_rate": 7.3316824434507185e-06,
      "loss": 1.6844,
      "step": 2230
    },
    {
      "epoch": 3.771043771043771,
      "grad_norm": 8.02890682220459,
      "learning_rate": 7.145585473818503e-06,
      "loss": 1.6914,
      "step": 2240
    },
    {
      "epoch": 3.787878787878788,
      "grad_norm": 15.429818153381348,
      "learning_rate": 6.961486195636613e-06,
      "loss": 1.6273,
      "step": 2250
    },
    {
      "epoch": 3.8047138047138045,
      "grad_norm": 7.854650974273682,
      "learning_rate": 6.7794052073712475e-06,
      "loss": 1.6874,
      "step": 2260
    },
    {
      "epoch": 3.821548821548822,
      "grad_norm": 11.608465194702148,
      "learning_rate": 6.599362881666546e-06,
      "loss": 1.6335,
      "step": 2270
    },
    {
      "epoch": 3.8383838383838382,
      "grad_norm": 11.066922187805176,
      "learning_rate": 6.421379363065142e-06,
      "loss": 1.5851,
      "step": 2280
    },
    {
      "epoch": 3.855218855218855,
      "grad_norm": 15.113936424255371,
      "learning_rate": 6.245474565754261e-06,
      "loss": 1.6718,
      "step": 2290
    },
    {
      "epoch": 3.872053872053872,
      "grad_norm": 15.739283561706543,
      "learning_rate": 6.071668171337522e-06,
      "loss": 1.5805,
      "step": 2300
    },
    {
      "epoch": 3.888888888888889,
      "grad_norm": 10.04202651977539,
      "learning_rate": 5.899979626632834e-06,
      "loss": 1.6817,
      "step": 2310
    },
    {
      "epoch": 3.9057239057239057,
      "grad_norm": 13.551682472229004,
      "learning_rate": 5.730428141496522e-06,
      "loss": 1.6755,
      "step": 2320
    },
    {
      "epoch": 3.9225589225589226,
      "grad_norm": 8.994790077209473,
      "learning_rate": 5.563032686673986e-06,
      "loss": 1.6546,
      "step": 2330
    },
    {
      "epoch": 3.9393939393939394,
      "grad_norm": 9.936976432800293,
      "learning_rate": 5.397811991677107e-06,
      "loss": 1.6863,
      "step": 2340
    },
    {
      "epoch": 3.9562289562289563,
      "grad_norm": 10.62367057800293,
      "learning_rate": 5.234784542688631e-06,
      "loss": 1.5814,
      "step": 2350
    },
    {
      "epoch": 3.973063973063973,
      "grad_norm": 9.182181358337402,
      "learning_rate": 5.0739685804938016e-06,
      "loss": 1.7338,
      "step": 2360
    },
    {
      "epoch": 3.98989898989899,
      "grad_norm": 8.511890411376953,
      "learning_rate": 4.915382098439436e-06,
      "loss": 1.5623,
      "step": 2370
    },
    {
      "epoch": 4.006734006734007,
      "grad_norm": 9.945953369140625,
      "learning_rate": 4.759042840420682e-06,
      "loss": 1.5468,
      "step": 2380
    },
    {
      "epoch": 4.023569023569023,
      "grad_norm": 11.818568229675293,
      "learning_rate": 4.604968298895703e-06,
      "loss": 1.597,
      "step": 2390
    },
    {
      "epoch": 4.040404040404041,
      "grad_norm": 10.017897605895996,
      "learning_rate": 4.453175712928476e-06,
      "loss": 1.5631,
      "step": 2400
    },
    {
      "epoch": 4.057239057239057,
      "grad_norm": 8.691032409667969,
      "learning_rate": 4.3036820662599594e-06,
      "loss": 1.5715,
      "step": 2410
    },
    {
      "epoch": 4.074074074074074,
      "grad_norm": 10.04669189453125,
      "learning_rate": 4.156504085407806e-06,
      "loss": 1.6189,
      "step": 2420
    },
    {
      "epoch": 4.090909090909091,
      "grad_norm": 10.519609451293945,
      "learning_rate": 4.011658237794877e-06,
      "loss": 1.6175,
      "step": 2430
    },
    {
      "epoch": 4.107744107744108,
      "grad_norm": 10.489120483398438,
      "learning_rate": 3.869160729906757e-06,
      "loss": 1.5758,
      "step": 2440
    },
    {
      "epoch": 4.124579124579125,
      "grad_norm": 11.625679016113281,
      "learning_rate": 3.729027505478405e-06,
      "loss": 1.4749,
      "step": 2450
    },
    {
      "epoch": 4.141414141414141,
      "grad_norm": 10.188380241394043,
      "learning_rate": 3.591274243710277e-06,
      "loss": 1.7645,
      "step": 2460
    },
    {
      "epoch": 4.158249158249158,
      "grad_norm": 7.030267238616943,
      "learning_rate": 3.4559163575139997e-06,
      "loss": 1.7482,
      "step": 2470
    },
    {
      "epoch": 4.175084175084175,
      "grad_norm": 11.049137115478516,
      "learning_rate": 3.3229689917878644e-06,
      "loss": 1.5977,
      "step": 2480
    },
    {
      "epoch": 4.191919191919192,
      "grad_norm": 10.272364616394043,
      "learning_rate": 3.1924470217222835e-06,
      "loss": 1.534,
      "step": 2490
    },
    {
      "epoch": 4.2087542087542085,
      "grad_norm": 9.50283145904541,
      "learning_rate": 3.0643650511354484e-06,
      "loss": 1.4801,
      "step": 2500
    },
    {
      "epoch": 4.225589225589226,
      "grad_norm": 11.843816757202148,
      "learning_rate": 2.9387374108393485e-06,
      "loss": 1.5567,
      "step": 2510
    },
    {
      "epoch": 4.242424242424242,
      "grad_norm": 10.063736915588379,
      "learning_rate": 2.8155781570363026e-06,
      "loss": 1.7143,
      "step": 2520
    },
    {
      "epoch": 4.2592592592592595,
      "grad_norm": 12.098738670349121,
      "learning_rate": 2.6949010697462582e-06,
      "loss": 1.5225,
      "step": 2530
    },
    {
      "epoch": 4.276094276094276,
      "grad_norm": 13.707754135131836,
      "learning_rate": 2.5767196512649723e-06,
      "loss": 1.6896,
      "step": 2540
    },
    {
      "epoch": 4.292929292929293,
      "grad_norm": 11.407543182373047,
      "learning_rate": 2.461047124653279e-06,
      "loss": 1.5618,
      "step": 2550
    },
    {
      "epoch": 4.30976430976431,
      "grad_norm": 8.580541610717773,
      "learning_rate": 2.3478964322575596e-06,
      "loss": 1.7722,
      "step": 2560
    },
    {
      "epoch": 4.326599326599327,
      "grad_norm": 11.025402069091797,
      "learning_rate": 2.2372802342616877e-06,
      "loss": 1.6317,
      "step": 2570
    },
    {
      "epoch": 4.343434343434343,
      "grad_norm": 9.22443962097168,
      "learning_rate": 2.129210907270496e-06,
      "loss": 1.5378,
      "step": 2580
    },
    {
      "epoch": 4.36026936026936,
      "grad_norm": 8.897051811218262,
      "learning_rate": 2.023700542924972e-06,
      "loss": 1.661,
      "step": 2590
    },
    {
      "epoch": 4.377104377104377,
      "grad_norm": 9.592473983764648,
      "learning_rate": 1.920760946549366e-06,
      "loss": 1.6353,
      "step": 2600
    },
    {
      "epoch": 4.393939393939394,
      "grad_norm": 9.055866241455078,
      "learning_rate": 1.8204036358303173e-06,
      "loss": 1.6968,
      "step": 2610
    },
    {
      "epoch": 4.410774410774411,
      "grad_norm": 11.303881645202637,
      "learning_rate": 1.7226398395281534e-06,
      "loss": 1.7348,
      "step": 2620
    },
    {
      "epoch": 4.427609427609427,
      "grad_norm": 10.665999412536621,
      "learning_rate": 1.6274804962205486e-06,
      "loss": 1.7017,
      "step": 2630
    },
    {
      "epoch": 4.444444444444445,
      "grad_norm": 8.247533798217773,
      "learning_rate": 1.534936253078606e-06,
      "loss": 1.5715,
      "step": 2640
    },
    {
      "epoch": 4.461279461279461,
      "grad_norm": 13.769998550415039,
      "learning_rate": 1.4450174646755994e-06,
      "loss": 1.7664,
      "step": 2650
    },
    {
      "epoch": 4.478114478114478,
      "grad_norm": 11.838301658630371,
      "learning_rate": 1.3577341918283914e-06,
      "loss": 1.6374,
      "step": 2660
    },
    {
      "epoch": 4.494949494949495,
      "grad_norm": 9.390249252319336,
      "learning_rate": 1.2730962004717685e-06,
      "loss": 1.5129,
      "step": 2670
    },
    {
      "epoch": 4.511784511784512,
      "grad_norm": 10.70655345916748,
      "learning_rate": 1.1911129605657489e-06,
      "loss": 1.632,
      "step": 2680
    },
    {
      "epoch": 4.5286195286195285,
      "grad_norm": 8.239924430847168,
      "learning_rate": 1.1117936450360078e-06,
      "loss": 1.6631,
      "step": 2690
    },
    {
      "epoch": 4.545454545454545,
      "grad_norm": 10.199871063232422,
      "learning_rate": 1.0351471287475406e-06,
      "loss": 1.5046,
      "step": 2700
    },
    {
      "epoch": 4.562289562289562,
      "grad_norm": 9.748342514038086,
      "learning_rate": 9.611819875116723e-07,
      "loss": 1.5607,
      "step": 2710
    },
    {
      "epoch": 4.57912457912458,
      "grad_norm": 10.384041786193848,
      "learning_rate": 8.899064971265276e-07,
      "loss": 1.6533,
      "step": 2720
    },
    {
      "epoch": 4.595959595959596,
      "grad_norm": 11.684048652648926,
      "learning_rate": 8.213286324510738e-07,
      "loss": 1.5284,
      "step": 2730
    },
    {
      "epoch": 4.6127946127946124,
      "grad_norm": 10.703577995300293,
      "learning_rate": 7.554560665128246e-07,
      "loss": 1.5145,
      "step": 2740
    },
    {
      "epoch": 4.62962962962963,
      "grad_norm": 5.70272159576416,
      "learning_rate": 6.922961696493286e-07,
      "loss": 1.5763,
      "step": 2750
    },
    {
      "epoch": 4.646464646464646,
      "grad_norm": 8.393756866455078,
      "learning_rate": 6.318560086835179e-07,
      "loss": 1.8288,
      "step": 2760
    },
    {
      "epoch": 4.6632996632996635,
      "grad_norm": 8.008843421936035,
      "learning_rate": 5.741423461330081e-07,
      "loss": 1.6122,
      "step": 2770
    },
    {
      "epoch": 4.68013468013468,
      "grad_norm": 9.10508918762207,
      "learning_rate": 5.191616394534693e-07,
      "loss": 1.4375,
      "step": 2780
    },
    {
      "epoch": 4.696969696969697,
      "grad_norm": 10.586594581604004,
      "learning_rate": 4.669200403160989e-07,
      "loss": 1.5535,
      "step": 2790
    },
    {
      "epoch": 4.713804713804714,
      "grad_norm": 11.951582908630371,
      "learning_rate": 4.1742339391934173e-07,
      "loss": 1.5842,
      "step": 2800
    },
    {
      "epoch": 4.730639730639731,
      "grad_norm": 7.801820755004883,
      "learning_rate": 3.7067723833486836e-07,
      "loss": 1.6817,
      "step": 2810
    },
    {
      "epoch": 4.747474747474747,
      "grad_norm": 13.025071144104004,
      "learning_rate": 3.266868038879434e-07,
      "loss": 1.5662,
      "step": 2820
    },
    {
      "epoch": 4.764309764309765,
      "grad_norm": 14.460515975952148,
      "learning_rate": 2.8545701257221e-07,
      "loss": 1.672,
      "step": 2830
    },
    {
      "epoch": 4.781144781144781,
      "grad_norm": 12.313004493713379,
      "learning_rate": 2.4699247749897515e-07,
      "loss": 1.7596,
      "step": 2840
    },
    {
      "epoch": 4.797979797979798,
      "grad_norm": 10.291306495666504,
      "learning_rate": 2.1129750238107203e-07,
      "loss": 1.5375,
      "step": 2850
    },
    {
      "epoch": 4.814814814814815,
      "grad_norm": 6.266113758087158,
      "learning_rate": 1.7837608105131242e-07,
      "loss": 1.5453,
      "step": 2860
    },
    {
      "epoch": 4.831649831649831,
      "grad_norm": 12.137284278869629,
      "learning_rate": 1.482318970156382e-07,
      "loss": 1.6078,
      "step": 2870
    },
    {
      "epoch": 4.848484848484849,
      "grad_norm": 10.706008911132812,
      "learning_rate": 1.2086832304096797e-07,
      "loss": 1.5187,
      "step": 2880
    },
    {
      "epoch": 4.865319865319865,
      "grad_norm": 13.570531845092773,
      "learning_rate": 9.628842077783473e-08,
      "loss": 1.6435,
      "step": 2890
    },
    {
      "epoch": 4.882154882154882,
      "grad_norm": 10.062174797058105,
      "learning_rate": 7.449494041782967e-08,
      "loss": 1.5533,
      "step": 2900
    },
    {
      "epoch": 4.898989898989899,
      "grad_norm": 13.20865249633789,
      "learning_rate": 5.54903203858731e-08,
      "loss": 1.5958,
      "step": 2910
    },
    {
      "epoch": 4.915824915824916,
      "grad_norm": 11.387255668640137,
      "learning_rate": 3.9276687067399395e-08,
      "loss": 1.6818,
      "step": 2920
    },
    {
      "epoch": 4.9326599326599325,
      "grad_norm": 12.698760032653809,
      "learning_rate": 2.5855854570430715e-08,
      "loss": 1.6905,
      "step": 2930
    },
    {
      "epoch": 4.94949494949495,
      "grad_norm": 11.168657302856445,
      "learning_rate": 1.522932452260595e-08,
      "loss": 1.6261,
      "step": 2940
    },
    {
      "epoch": 4.966329966329966,
      "grad_norm": 11.721102714538574,
      "learning_rate": 7.3982859031707095e-09,
      "loss": 1.697,
      "step": 2950
    },
    {
      "epoch": 4.983164983164983,
      "grad_norm": 11.20834732055664,
      "learning_rate": 2.3636149099282024e-09,
      "loss": 1.6948,
      "step": 2960
    },
    {
      "epoch": 5.0,
      "grad_norm": 8.794071197509766,
      "learning_rate": 1.2587486122317416e-10,
      "loss": 1.4156,
      "step": 2970
    },
    {
      "epoch": 5.0,
      "step": 2970,
      "total_flos": 5.122040301531955e+16,
      "train_loss": 1.811927077425048,
      "train_runtime": 582.303,
      "train_samples_per_second": 20.393,
      "train_steps_per_second": 5.1
    }
  ],
  "logging_steps": 10,
  "max_steps": 2970,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500.0,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": false,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 5.122040301531955e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
