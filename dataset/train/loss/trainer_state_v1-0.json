{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 1425,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.03508771929824561,
      "grad_norm": 1.719846487045288,
      "learning_rate": 9.998784954797474e-05,
      "loss": 3.1493,
      "step": 10
    },
    {
      "epoch": 0.07017543859649122,
      "grad_norm": 3.045808792114258,
      "learning_rate": 9.995614150494293e-05,
      "loss": 2.6169,
      "step": 20
    },
    {
      "epoch": 0.10526315789473684,
      "grad_norm": 2.769902467727661,
      "learning_rate": 9.989784536373726e-05,
      "loss": 2.3922,
      "step": 30
    },
    {
      "epoch": 0.14035087719298245,
      "grad_norm": 4.130541801452637,
      "learning_rate": 9.981529796748134e-05,
      "loss": 2.3987,
      "step": 40
    },
    {
      "epoch": 0.17543859649122806,
      "grad_norm": 3.5327138900756836,
      "learning_rate": 9.97085394357023e-05,
      "loss": 2.1804,
      "step": 50
    },
    {
      "epoch": 0.21052631578947367,
      "grad_norm": 3.813915967941284,
      "learning_rate": 9.957762165497686e-05,
      "loss": 2.1467,
      "step": 60
    },
    {
      "epoch": 0.24561403508771928,
      "grad_norm": 3.805263042449951,
      "learning_rate": 9.942260825371358e-05,
      "loss": 2.1852,
      "step": 70
    },
    {
      "epoch": 0.2807017543859649,
      "grad_norm": 3.19567608833313,
      "learning_rate": 9.924357457122828e-05,
      "loss": 2.0915,
      "step": 80
    },
    {
      "epoch": 0.3157894736842105,
      "grad_norm": 2.8541414737701416,
      "learning_rate": 9.904060762112777e-05,
      "loss": 2.2044,
      "step": 90
    },
    {
      "epoch": 0.3508771929824561,
      "grad_norm": 4.158815383911133,
      "learning_rate": 9.881380604901964e-05,
      "loss": 2.0589,
      "step": 100
    },
    {
      "epoch": 0.38596491228070173,
      "grad_norm": 3.381164789199829,
      "learning_rate": 9.856328008456872e-05,
      "loss": 2.0521,
      "step": 110
    },
    {
      "epoch": 0.42105263157894735,
      "grad_norm": 3.9986066818237305,
      "learning_rate": 9.828915148792352e-05,
      "loss": 1.9709,
      "step": 120
    },
    {
      "epoch": 0.45614035087719296,
      "grad_norm": 2.4225213527679443,
      "learning_rate": 9.799155349053851e-05,
      "loss": 1.9797,
      "step": 130
    },
    {
      "epoch": 0.49122807017543857,
      "grad_norm": 3.7709991931915283,
      "learning_rate": 9.76706307304213e-05,
      "loss": 1.9366,
      "step": 140
    },
    {
      "epoch": 0.5263157894736842,
      "grad_norm": 3.8311586380004883,
      "learning_rate": 9.732653918183592e-05,
      "loss": 2.0103,
      "step": 150
    },
    {
      "epoch": 0.5614035087719298,
      "grad_norm": 4.786016941070557,
      "learning_rate": 9.695944607949649e-05,
      "loss": 2.2607,
      "step": 160
    },
    {
      "epoch": 0.5964912280701754,
      "grad_norm": 5.7684807777404785,
      "learning_rate": 9.65695298372882e-05,
      "loss": 1.9588,
      "step": 170
    },
    {
      "epoch": 0.631578947368421,
      "grad_norm": 6.4565815925598145,
      "learning_rate": 9.61569799615548e-05,
      "loss": 1.9836,
      "step": 180
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 4.659782886505127,
      "learning_rate": 9.572199695899522e-05,
      "loss": 1.9693,
      "step": 190
    },
    {
      "epoch": 0.7017543859649122,
      "grad_norm": 5.347527503967285,
      "learning_rate": 9.526479223921366e-05,
      "loss": 1.9596,
      "step": 200
    },
    {
      "epoch": 0.7368421052631579,
      "grad_norm": 3.669649362564087,
      "learning_rate": 9.478558801197065e-05,
      "loss": 1.9652,
      "step": 210
    },
    {
      "epoch": 0.7719298245614035,
      "grad_norm": 6.528237819671631,
      "learning_rate": 9.428461717918511e-05,
      "loss": 1.878,
      "step": 220
    },
    {
      "epoch": 0.8070175438596491,
      "grad_norm": 4.638526916503906,
      "learning_rate": 9.376212322173985e-05,
      "loss": 1.9212,
      "step": 230
    },
    {
      "epoch": 0.8421052631578947,
      "grad_norm": 3.820744276046753,
      "learning_rate": 9.321836008114539e-05,
      "loss": 1.8141,
      "step": 240
    },
    {
      "epoch": 0.8771929824561403,
      "grad_norm": 5.184341907501221,
      "learning_rate": 9.265359203611987e-05,
      "loss": 1.8724,
      "step": 250
    },
    {
      "epoch": 0.9122807017543859,
      "grad_norm": 5.6075286865234375,
      "learning_rate": 9.206809357414474e-05,
      "loss": 2.0228,
      "step": 260
    },
    {
      "epoch": 0.9473684210526315,
      "grad_norm": 4.489683151245117,
      "learning_rate": 9.146214925805891e-05,
      "loss": 1.8267,
      "step": 270
    },
    {
      "epoch": 0.9824561403508771,
      "grad_norm": 4.8996195793151855,
      "learning_rate": 9.083605358775612e-05,
      "loss": 1.9237,
      "step": 280
    },
    {
      "epoch": 1.0175438596491229,
      "grad_norm": 5.299293518066406,
      "learning_rate": 9.019011085705253e-05,
      "loss": 1.8797,
      "step": 290
    },
    {
      "epoch": 1.0526315789473684,
      "grad_norm": 4.156127452850342,
      "learning_rate": 8.95246350057946e-05,
      "loss": 1.848,
      "step": 300
    },
    {
      "epoch": 1.087719298245614,
      "grad_norm": 4.640390872955322,
      "learning_rate": 8.883994946727849e-05,
      "loss": 1.8041,
      "step": 310
    },
    {
      "epoch": 1.1228070175438596,
      "grad_norm": 5.9081034660339355,
      "learning_rate": 8.813638701105573e-05,
      "loss": 1.6932,
      "step": 320
    },
    {
      "epoch": 1.1578947368421053,
      "grad_norm": 4.403779029846191,
      "learning_rate": 8.741428958120118e-05,
      "loss": 1.6325,
      "step": 330
    },
    {
      "epoch": 1.1929824561403508,
      "grad_norm": 4.3029608726501465,
      "learning_rate": 8.6674008130122e-05,
      "loss": 1.6818,
      "step": 340
    },
    {
      "epoch": 1.2280701754385965,
      "grad_norm": 5.7832560539245605,
      "learning_rate": 8.591590244798844e-05,
      "loss": 1.8058,
      "step": 350
    },
    {
      "epoch": 1.263157894736842,
      "grad_norm": 3.725045919418335,
      "learning_rate": 8.514034098786933e-05,
      "loss": 1.9477,
      "step": 360
    },
    {
      "epoch": 1.2982456140350878,
      "grad_norm": 6.202881336212158,
      "learning_rate": 8.434770068665723e-05,
      "loss": 1.6796,
      "step": 370
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 5.372885227203369,
      "learning_rate": 8.353836678187027e-05,
      "loss": 1.7978,
      "step": 380
    },
    {
      "epoch": 1.368421052631579,
      "grad_norm": 5.834847927093506,
      "learning_rate": 8.271273262441975e-05,
      "loss": 1.7206,
      "step": 390
    },
    {
      "epoch": 1.4035087719298245,
      "grad_norm": 6.51511287689209,
      "learning_rate": 8.18711994874345e-05,
      "loss": 1.7111,
      "step": 400
    },
    {
      "epoch": 1.4385964912280702,
      "grad_norm": 6.013103008270264,
      "learning_rate": 8.101417637123484e-05,
      "loss": 1.6802,
      "step": 410
    },
    {
      "epoch": 1.4736842105263157,
      "grad_norm": 5.622183322906494,
      "learning_rate": 8.01420798045511e-05,
      "loss": 1.6446,
      "step": 420
    },
    {
      "epoch": 1.5087719298245614,
      "grad_norm": 4.8114776611328125,
      "learning_rate": 7.925533364208309e-05,
      "loss": 1.6732,
      "step": 430
    },
    {
      "epoch": 1.543859649122807,
      "grad_norm": 3.3873331546783447,
      "learning_rate": 7.835436885849902e-05,
      "loss": 1.7302,
      "step": 440
    },
    {
      "epoch": 1.5789473684210527,
      "grad_norm": 6.759139537811279,
      "learning_rate": 7.743962333897405e-05,
      "loss": 1.8348,
      "step": 450
    },
    {
      "epoch": 1.6140350877192984,
      "grad_norm": 7.1612548828125,
      "learning_rate": 7.651154166637025e-05,
      "loss": 1.6064,
      "step": 460
    },
    {
      "epoch": 1.6491228070175439,
      "grad_norm": 5.620690822601318,
      "learning_rate": 7.557057490516111e-05,
      "loss": 1.7255,
      "step": 470
    },
    {
      "epoch": 1.6842105263157894,
      "grad_norm": 6.32211446762085,
      "learning_rate": 7.461718038220621e-05,
      "loss": 1.9079,
      "step": 480
    },
    {
      "epoch": 1.719298245614035,
      "grad_norm": 5.472750186920166,
      "learning_rate": 7.365182146448205e-05,
      "loss": 1.8288,
      "step": 490
    },
    {
      "epoch": 1.7543859649122808,
      "grad_norm": 6.57025671005249,
      "learning_rate": 7.267496733387731e-05,
      "loss": 1.7209,
      "step": 500
    },
    {
      "epoch": 1.7894736842105263,
      "grad_norm": 6.543961524963379,
      "learning_rate": 7.1687092759162e-05,
      "loss": 1.6965,
      "step": 510
    },
    {
      "epoch": 1.8245614035087718,
      "grad_norm": 6.190191268920898,
      "learning_rate": 7.068867786524116e-05,
      "loss": 1.525,
      "step": 520
    },
    {
      "epoch": 1.8596491228070176,
      "grad_norm": 6.411980628967285,
      "learning_rate": 6.968020789980562e-05,
      "loss": 1.7758,
      "step": 530
    },
    {
      "epoch": 1.8947368421052633,
      "grad_norm": 5.482910633087158,
      "learning_rate": 6.86621729974927e-05,
      "loss": 1.8769,
      "step": 540
    },
    {
      "epoch": 1.9298245614035088,
      "grad_norm": 5.804032802581787,
      "learning_rate": 6.763506794167208e-05,
      "loss": 1.7164,
      "step": 550
    },
    {
      "epoch": 1.9649122807017543,
      "grad_norm": 7.566220760345459,
      "learning_rate": 6.659939192397192e-05,
      "loss": 1.7058,
      "step": 560
    },
    {
      "epoch": 2.0,
      "grad_norm": 8.38500690460205,
      "learning_rate": 6.555564830166293e-05,
      "loss": 1.7952,
      "step": 570
    },
    {
      "epoch": 2.0350877192982457,
      "grad_norm": 6.883004665374756,
      "learning_rate": 6.450434435301751e-05,
      "loss": 1.594,
      "step": 580
    },
    {
      "epoch": 2.0701754385964914,
      "grad_norm": 7.969598293304443,
      "learning_rate": 6.344599103076329e-05,
      "loss": 1.7371,
      "step": 590
    },
    {
      "epoch": 2.1052631578947367,
      "grad_norm": 5.469300746917725,
      "learning_rate": 6.238110271375102e-05,
      "loss": 1.6038,
      "step": 600
    },
    {
      "epoch": 2.1403508771929824,
      "grad_norm": 6.575092792510986,
      "learning_rate": 6.131019695695702e-05,
      "loss": 1.5611,
      "step": 610
    },
    {
      "epoch": 2.175438596491228,
      "grad_norm": 7.4674787521362305,
      "learning_rate": 6.023379423994214e-05,
      "loss": 1.6817,
      "step": 620
    },
    {
      "epoch": 2.2105263157894735,
      "grad_norm": 8.313907623291016,
      "learning_rate": 5.915241771388931e-05,
      "loss": 1.2925,
      "step": 630
    },
    {
      "epoch": 2.245614035087719,
      "grad_norm": 7.526777267456055,
      "learning_rate": 5.8066592947342555e-05,
      "loss": 1.5613,
      "step": 640
    },
    {
      "epoch": 2.280701754385965,
      "grad_norm": 9.671422004699707,
      "learning_rate": 5.697684767077125e-05,
      "loss": 1.6217,
      "step": 650
    },
    {
      "epoch": 2.3157894736842106,
      "grad_norm": 7.488387584686279,
      "learning_rate": 5.588371152008349e-05,
      "loss": 1.6419,
      "step": 660
    },
    {
      "epoch": 2.3508771929824563,
      "grad_norm": 8.7188720703125,
      "learning_rate": 5.478771577921351e-05,
      "loss": 1.3701,
      "step": 670
    },
    {
      "epoch": 2.3859649122807016,
      "grad_norm": 8.101195335388184,
      "learning_rate": 5.368939312190808e-05,
      "loss": 1.5725,
      "step": 680
    },
    {
      "epoch": 2.4210526315789473,
      "grad_norm": 4.9318413734436035,
      "learning_rate": 5.258927735283748e-05,
      "loss": 1.649,
      "step": 690
    },
    {
      "epoch": 2.456140350877193,
      "grad_norm": 8.414766311645508,
      "learning_rate": 5.148790314815663e-05,
      "loss": 1.5895,
      "step": 700
    },
    {
      "epoch": 2.4912280701754383,
      "grad_norm": 7.010472774505615,
      "learning_rate": 5.038580579564298e-05,
      "loss": 1.4524,
      "step": 710
    },
    {
      "epoch": 2.526315789473684,
      "grad_norm": 6.470371723175049,
      "learning_rate": 4.9283520934536904e-05,
      "loss": 1.5695,
      "step": 720
    },
    {
      "epoch": 2.56140350877193,
      "grad_norm": 5.362852096557617,
      "learning_rate": 4.818158429521129e-05,
      "loss": 1.6807,
      "step": 730
    },
    {
      "epoch": 2.5964912280701755,
      "grad_norm": 8.079692840576172,
      "learning_rate": 4.708053143879701e-05,
      "loss": 1.5093,
      "step": 740
    },
    {
      "epoch": 2.6315789473684212,
      "grad_norm": 6.042057991027832,
      "learning_rate": 4.598089749689041e-05,
      "loss": 1.548,
      "step": 750
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 7.591445446014404,
      "learning_rate": 4.488321691146975e-05,
      "loss": 1.6705,
      "step": 760
    },
    {
      "epoch": 2.7017543859649122,
      "grad_norm": 8.62905502319336,
      "learning_rate": 4.3788023175146747e-05,
      "loss": 1.4379,
      "step": 770
    },
    {
      "epoch": 2.736842105263158,
      "grad_norm": 7.921586036682129,
      "learning_rate": 4.269584857187943e-05,
      "loss": 1.441,
      "step": 780
    },
    {
      "epoch": 2.7719298245614032,
      "grad_norm": 6.374785900115967,
      "learning_rate": 4.160722391827262e-05,
      "loss": 1.6861,
      "step": 790
    },
    {
      "epoch": 2.807017543859649,
      "grad_norm": 7.16132926940918,
      "learning_rate": 4.05226783055914e-05,
      "loss": 1.4198,
      "step": 800
    },
    {
      "epoch": 2.8421052631578947,
      "grad_norm": 9.97480297088623,
      "learning_rate": 3.944273884261322e-05,
      "loss": 1.5267,
      "step": 810
    },
    {
      "epoch": 2.8771929824561404,
      "grad_norm": 5.691163063049316,
      "learning_rate": 3.836793039944349e-05,
      "loss": 1.6473,
      "step": 820
    },
    {
      "epoch": 2.912280701754386,
      "grad_norm": 11.115808486938477,
      "learning_rate": 3.7298775352419206e-05,
      "loss": 1.4313,
      "step": 830
    },
    {
      "epoch": 2.9473684210526314,
      "grad_norm": 7.59100866317749,
      "learning_rate": 3.6235793330224635e-05,
      "loss": 1.5585,
      "step": 840
    },
    {
      "epoch": 2.982456140350877,
      "grad_norm": 7.718577861785889,
      "learning_rate": 3.517950096134232e-05,
      "loss": 1.5492,
      "step": 850
    },
    {
      "epoch": 3.017543859649123,
      "grad_norm": 7.534000873565674,
      "learning_rate": 3.413041162296241e-05,
      "loss": 1.5134,
      "step": 860
    },
    {
      "epoch": 3.0526315789473686,
      "grad_norm": 9.30507755279541,
      "learning_rate": 3.308903519147194e-05,
      "loss": 1.3024,
      "step": 870
    },
    {
      "epoch": 3.087719298245614,
      "grad_norm": 10.87876033782959,
      "learning_rate": 3.205587779464576e-05,
      "loss": 1.5014,
      "step": 880
    },
    {
      "epoch": 3.1228070175438596,
      "grad_norm": 6.01002836227417,
      "learning_rate": 3.1031441565659235e-05,
      "loss": 1.397,
      "step": 890
    },
    {
      "epoch": 3.1578947368421053,
      "grad_norm": 8.913203239440918,
      "learning_rate": 3.0016224399042515e-05,
      "loss": 1.3077,
      "step": 900
    },
    {
      "epoch": 3.192982456140351,
      "grad_norm": 8.530435562133789,
      "learning_rate": 2.9010719708694722e-05,
      "loss": 1.4256,
      "step": 910
    },
    {
      "epoch": 3.2280701754385963,
      "grad_norm": 6.991862773895264,
      "learning_rate": 2.8015416188075893e-05,
      "loss": 1.2795,
      "step": 920
    },
    {
      "epoch": 3.263157894736842,
      "grad_norm": 9.03817081451416,
      "learning_rate": 2.703079757269319e-05,
      "loss": 1.5504,
      "step": 930
    },
    {
      "epoch": 3.2982456140350878,
      "grad_norm": 13.501176834106445,
      "learning_rate": 2.6057342404996522e-05,
      "loss": 1.3467,
      "step": 940
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 8.239439010620117,
      "learning_rate": 2.5095523801798495e-05,
      "loss": 1.4218,
      "step": 950
    },
    {
      "epoch": 3.3684210526315788,
      "grad_norm": 8.352029800415039,
      "learning_rate": 2.4145809224330896e-05,
      "loss": 1.4901,
      "step": 960
    },
    {
      "epoch": 3.4035087719298245,
      "grad_norm": 9.084619522094727,
      "learning_rate": 2.3208660251050158e-05,
      "loss": 1.5302,
      "step": 970
    },
    {
      "epoch": 3.43859649122807,
      "grad_norm": 7.4268622398376465,
      "learning_rate": 2.2284532353301953e-05,
      "loss": 1.4462,
      "step": 980
    },
    {
      "epoch": 3.473684210526316,
      "grad_norm": 9.125428199768066,
      "learning_rate": 2.1373874673953685e-05,
      "loss": 1.5127,
      "step": 990
    },
    {
      "epoch": 3.5087719298245617,
      "grad_norm": 4.854528427124023,
      "learning_rate": 2.0477129809103147e-05,
      "loss": 1.4875,
      "step": 1000
    },
    {
      "epoch": 3.543859649122807,
      "grad_norm": 8.155963897705078,
      "learning_rate": 1.9594733592968733e-05,
      "loss": 1.3685,
      "step": 1010
    },
    {
      "epoch": 3.5789473684210527,
      "grad_norm": 7.805800437927246,
      "learning_rate": 1.872711488606609e-05,
      "loss": 1.335,
      "step": 1020
    },
    {
      "epoch": 3.6140350877192984,
      "grad_norm": 9.136666297912598,
      "learning_rate": 1.787469536677419e-05,
      "loss": 1.434,
      "step": 1030
    },
    {
      "epoch": 3.6491228070175437,
      "grad_norm": 8.002681732177734,
      "learning_rate": 1.703788932639202e-05,
      "loss": 1.5685,
      "step": 1040
    },
    {
      "epoch": 3.6842105263157894,
      "grad_norm": 7.156036853790283,
      "learning_rate": 1.6217103467785484e-05,
      "loss": 1.4419,
      "step": 1050
    },
    {
      "epoch": 3.719298245614035,
      "grad_norm": 7.870306968688965,
      "learning_rate": 1.5412736707722537e-05,
      "loss": 1.3552,
      "step": 1060
    },
    {
      "epoch": 3.754385964912281,
      "grad_norm": 7.358731746673584,
      "learning_rate": 1.4625179982992321e-05,
      "loss": 1.2955,
      "step": 1070
    },
    {
      "epoch": 3.7894736842105265,
      "grad_norm": 7.213303565979004,
      "learning_rate": 1.385481606040287e-05,
      "loss": 1.2698,
      "step": 1080
    },
    {
      "epoch": 3.824561403508772,
      "grad_norm": 9.164271354675293,
      "learning_rate": 1.3102019350749528e-05,
      "loss": 1.4334,
      "step": 1090
    },
    {
      "epoch": 3.8596491228070176,
      "grad_norm": 5.654627323150635,
      "learning_rate": 1.2367155726844492e-05,
      "loss": 1.331,
      "step": 1100
    },
    {
      "epoch": 3.8947368421052633,
      "grad_norm": 9.55091381072998,
      "learning_rate": 1.1650582345696088e-05,
      "loss": 1.4102,
      "step": 1110
    },
    {
      "epoch": 3.9298245614035086,
      "grad_norm": 8.020398139953613,
      "learning_rate": 1.095264747492391e-05,
      "loss": 1.3949,
      "step": 1120
    },
    {
      "epoch": 3.9649122807017543,
      "grad_norm": 8.223581314086914,
      "learning_rate": 1.0340722563656107e-05,
      "loss": 1.4066,
      "step": 1130
    },
    {
      "epoch": 4.0,
      "grad_norm": 10.651467323303223,
      "learning_rate": 9.679127763401152e-06,
      "loss": 1.4892,
      "step": 1140
    },
    {
      "epoch": 4.035087719298246,
      "grad_norm": 6.060051918029785,
      "learning_rate": 9.037129636095309e-06,
      "loss": 1.4171,
      "step": 1150
    },
    {
      "epoch": 4.0701754385964914,
      "grad_norm": 9.337297439575195,
      "learning_rate": 8.415040204436426e-06,
      "loss": 1.4098,
      "step": 1160
    },
    {
      "epoch": 4.105263157894737,
      "grad_norm": 9.200929641723633,
      "learning_rate": 7.813161815136294e-06,
      "loss": 1.2652,
      "step": 1170
    },
    {
      "epoch": 4.140350877192983,
      "grad_norm": 8.479249000549316,
      "learning_rate": 7.2317869919746705e-06,
      "loss": 1.3643,
      "step": 1180
    },
    {
      "epoch": 4.175438596491228,
      "grad_norm": 6.3367156982421875,
      "learning_rate": 6.671198293627479e-06,
      "loss": 1.4249,
      "step": 1190
    },
    {
      "epoch": 4.2105263157894735,
      "grad_norm": 7.483198165893555,
      "learning_rate": 6.131668176338118e-06,
      "loss": 1.3143,
      "step": 1200
    },
    {
      "epoch": 4.245614035087719,
      "grad_norm": 7.042283058166504,
      "learning_rate": 5.613458861498832e-06,
      "loss": 1.3659,
      "step": 1210
    },
    {
      "epoch": 4.280701754385965,
      "grad_norm": 8.78304672241211,
      "learning_rate": 5.116822208206396e-06,
      "loss": 1.2082,
      "step": 1220
    },
    {
      "epoch": 4.315789473684211,
      "grad_norm": 8.48442554473877,
      "learning_rate": 4.64199959085398e-06,
      "loss": 1.2134,
      "step": 1230
    },
    {
      "epoch": 4.350877192982456,
      "grad_norm": 8.072980880737305,
      "learning_rate": 4.189221781818914e-06,
      "loss": 1.4617,
      "step": 1240
    },
    {
      "epoch": 4.385964912280702,
      "grad_norm": 7.608723163604736,
      "learning_rate": 3.75870883930306e-06,
      "loss": 1.3342,
      "step": 1250
    },
    {
      "epoch": 4.421052631578947,
      "grad_norm": 10.453292846679688,
      "learning_rate": 3.35067000038059e-06,
      "loss": 1.2991,
      "step": 1260
    },
    {
      "epoch": 4.456140350877193,
      "grad_norm": 7.50430965423584,
      "learning_rate": 2.965303579304973e-06,
      "loss": 1.3286,
      "step": 1270
    },
    {
      "epoch": 4.491228070175438,
      "grad_norm": 8.323973655700684,
      "learning_rate": 2.602796871124663e-06,
      "loss": 1.4207,
      "step": 1280
    },
    {
      "epoch": 4.526315789473684,
      "grad_norm": 8.637736320495605,
      "learning_rate": 2.263326060654336e-06,
      "loss": 1.2143,
      "step": 1290
    },
    {
      "epoch": 4.56140350877193,
      "grad_norm": 7.235519886016846,
      "learning_rate": 1.9470561368458485e-06,
      "loss": 1.2845,
      "step": 1300
    },
    {
      "epoch": 4.5964912280701755,
      "grad_norm": 7.421191692352295,
      "learning_rate": 1.6541408126006463e-06,
      "loss": 1.3874,
      "step": 1310
    },
    {
      "epoch": 4.631578947368421,
      "grad_norm": 7.797518730163574,
      "learning_rate": 1.3847224500625256e-06,
      "loss": 1.2602,
      "step": 1320
    },
    {
      "epoch": 4.666666666666667,
      "grad_norm": 9.04797649383545,
      "learning_rate": 1.138931991427028e-06,
      "loss": 1.2629,
      "step": 1330
    },
    {
      "epoch": 4.701754385964913,
      "grad_norm": 8.498790740966797,
      "learning_rate": 9.168888953011989e-07,
      "loss": 1.2864,
      "step": 1340
    },
    {
      "epoch": 4.7368421052631575,
      "grad_norm": 8.701374053955078,
      "learning_rate": 7.187010786445181e-07,
      "loss": 1.3633,
      "step": 1350
    },
    {
      "epoch": 4.771929824561403,
      "grad_norm": 9.323623657226562,
      "learning_rate": 5.444648643193051e-07,
      "loss": 1.3554,
      "step": 1360
    },
    {
      "epoch": 4.807017543859649,
      "grad_norm": 6.792618274688721,
      "learning_rate": 3.9426493427611177e-07,
      "loss": 1.2895,
      "step": 1370
    },
    {
      "epoch": 4.842105263157895,
      "grad_norm": 8.779854774475098,
      "learning_rate": 2.6817428839668315e-07,
      "loss": 1.3718,
      "step": 1380
    },
    {
      "epoch": 4.87719298245614,
      "grad_norm": 8.328659057617188,
      "learning_rate": 1.662542090147712e-07,
      "loss": 1.358,
      "step": 1390
    },
    {
      "epoch": 4.912280701754386,
      "grad_norm": 7.100942611694336,
      "learning_rate": 8.855423113177664e-08,
      "loss": 1.3713,
      "step": 1400
    },
    {
      "epoch": 4.947368421052632,
      "grad_norm": 7.508010387420654,
      "learning_rate": 3.511211834184014e-08,
      "loss": 1.3713,
      "step": 1410
    },
    {
      "epoch": 4.982456140350877,
      "grad_norm": 8.044100761413574,
      "learning_rate": 5.953844478068238e-09,
      "loss": 1.4092,
      "step": 1420
    },
    {
      "epoch": 5.0,
      "step": 1425,
      "total_flos": 2.193394169413632e+16,
      "train_loss": 1.6289425334595797,
      "train_runtime": 273.3993,
      "train_samples_per_second": 20.83,
      "train_steps_per_second": 5.212
    }
  ],
  "logging_steps": 10,
  "max_steps": 1425,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500.0,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": false,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.193394169413632e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
