{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 2970,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.016835016835016835,
      "grad_norm": 2.9000630378723145,
      "learning_rate": 9.999773426770865e-05,
      "loss": 3.3528,
      "step": 10
    },
    {
      "epoch": 0.03367003367003367,
      "grad_norm": 1.8130184412002563,
      "learning_rate": 9.998990237032888e-05,
      "loss": 2.7967,
      "step": 20
    },
    {
      "epoch": 0.050505050505050504,
      "grad_norm": 3.1725528240203857,
      "learning_rate": 9.997647721194235e-05,
      "loss": 2.4292,
      "step": 30
    },
    {
      "epoch": 0.06734006734006734,
      "grad_norm": 4.351349353790283,
      "learning_rate": 9.995746029466071e-05,
      "loss": 2.2963,
      "step": 40
    },
    {
      "epoch": 0.08417508417508418,
      "grad_norm": 2.712670087814331,
      "learning_rate": 9.993285374624531e-05,
      "loss": 2.4128,
      "step": 50
    },
    {
      "epoch": 0.10101010101010101,
      "grad_norm": 3.171586036682129,
      "learning_rate": 9.990266031986909e-05,
      "loss": 2.1699,
      "step": 60
    },
    {
      "epoch": 0.11784511784511785,
      "grad_norm": 3.339716911315918,
      "learning_rate": 9.986688339380862e-05,
      "loss": 2.261,
      "step": 70
    },
    {
      "epoch": 0.13468013468013468,
      "grad_norm": 4.0818047523498535,
      "learning_rate": 9.982552697106601e-05,
      "loss": 2.1363,
      "step": 80
    },
    {
      "epoch": 0.15151515151515152,
      "grad_norm": 4.749594688415527,
      "learning_rate": 9.97785956789211e-05,
      "loss": 2.1491,
      "step": 90
    },
    {
      "epoch": 0.16835016835016836,
      "grad_norm": 2.228301763534546,
      "learning_rate": 9.972609476841367e-05,
      "loss": 2.2699,
      "step": 100
    },
    {
      "epoch": 0.18518518518518517,
      "grad_norm": 2.987699508666992,
      "learning_rate": 9.966803011375594e-05,
      "loss": 2.0061,
      "step": 110
    },
    {
      "epoch": 0.20202020202020202,
      "grad_norm": 5.830697536468506,
      "learning_rate": 9.960440821167531e-05,
      "loss": 2.1525,
      "step": 120
    },
    {
      "epoch": 0.21885521885521886,
      "grad_norm": 2.928910732269287,
      "learning_rate": 9.953523618068749e-05,
      "loss": 2.0891,
      "step": 130
    },
    {
      "epoch": 0.2356902356902357,
      "grad_norm": 4.471485137939453,
      "learning_rate": 9.946052176029994e-05,
      "loss": 2.046,
      "step": 140
    },
    {
      "epoch": 0.25252525252525254,
      "grad_norm": 3.4521384239196777,
      "learning_rate": 9.938027331014601e-05,
      "loss": 2.0606,
      "step": 150
    },
    {
      "epoch": 0.26936026936026936,
      "grad_norm": 3.6639907360076904,
      "learning_rate": 9.929449980904952e-05,
      "loss": 2.1718,
      "step": 160
    },
    {
      "epoch": 0.28619528619528617,
      "grad_norm": 2.64402174949646,
      "learning_rate": 9.920321085402022e-05,
      "loss": 1.9626,
      "step": 170
    },
    {
      "epoch": 0.30303030303030304,
      "grad_norm": 3.499401330947876,
      "learning_rate": 9.910641665917992e-05,
      "loss": 2.0941,
      "step": 180
    },
    {
      "epoch": 0.31986531986531985,
      "grad_norm": 4.093149662017822,
      "learning_rate": 9.900412805461967e-05,
      "loss": 2.0696,
      "step": 190
    },
    {
      "epoch": 0.3367003367003367,
      "grad_norm": 4.084297180175781,
      "learning_rate": 9.889635648518809e-05,
      "loss": 2.0761,
      "step": 200
    },
    {
      "epoch": 0.35353535353535354,
      "grad_norm": 4.75815486907959,
      "learning_rate": 9.878311400921072e-05,
      "loss": 2.1058,
      "step": 210
    },
    {
      "epoch": 0.37037037037037035,
      "grad_norm": 4.334584712982178,
      "learning_rate": 9.866441329714088e-05,
      "loss": 2.0957,
      "step": 220
    },
    {
      "epoch": 0.3872053872053872,
      "grad_norm": 3.4615371227264404,
      "learning_rate": 9.854026763014204e-05,
      "loss": 2.0148,
      "step": 230
    },
    {
      "epoch": 0.40404040404040403,
      "grad_norm": 4.054864883422852,
      "learning_rate": 9.841069089860176e-05,
      "loss": 1.8365,
      "step": 240
    },
    {
      "epoch": 0.4208754208754209,
      "grad_norm": 2.7588202953338623,
      "learning_rate": 9.827569760057755e-05,
      "loss": 2.1503,
      "step": 250
    },
    {
      "epoch": 0.4377104377104377,
      "grad_norm": 3.3782155513763428,
      "learning_rate": 9.813530284017474e-05,
      "loss": 2.1137,
      "step": 260
    },
    {
      "epoch": 0.45454545454545453,
      "grad_norm": 3.843691110610962,
      "learning_rate": 9.798952232585645e-05,
      "loss": 2.0512,
      "step": 270
    },
    {
      "epoch": 0.4713804713804714,
      "grad_norm": 2.964019298553467,
      "learning_rate": 9.783837236868609e-05,
      "loss": 2.1015,
      "step": 280
    },
    {
      "epoch": 0.4882154882154882,
      "grad_norm": 4.0633158683776855,
      "learning_rate": 9.768186988050228e-05,
      "loss": 1.9671,
      "step": 290
    },
    {
      "epoch": 0.5050505050505051,
      "grad_norm": 3.6476211547851562,
      "learning_rate": 9.752003237202662e-05,
      "loss": 1.842,
      "step": 300
    },
    {
      "epoch": 0.5218855218855218,
      "grad_norm": 4.757910251617432,
      "learning_rate": 9.735287795090455e-05,
      "loss": 1.9281,
      "step": 310
    },
    {
      "epoch": 0.5387205387205387,
      "grad_norm": 4.547556400299072,
      "learning_rate": 9.719790845697533e-05,
      "loss": 2.0795,
      "step": 320
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 3.742764949798584,
      "learning_rate": 9.702070391732919e-05,
      "loss": 2.1449,
      "step": 330
    },
    {
      "epoch": 0.5723905723905723,
      "grad_norm": 3.9334418773651123,
      "learning_rate": 9.683823833380692e-05,
      "loss": 2.0669,
      "step": 340
    },
    {
      "epoch": 0.5892255892255892,
      "grad_norm": 5.506468772888184,
      "learning_rate": 9.665053212208426e-05,
      "loss": 1.9863,
      "step": 350
    },
    {
      "epoch": 0.6060606060606061,
      "grad_norm": 3.3560328483581543,
      "learning_rate": 9.645760628419929e-05,
      "loss": 1.8244,
      "step": 360
    },
    {
      "epoch": 0.622895622895623,
      "grad_norm": 3.686779737472534,
      "learning_rate": 9.625948240620269e-05,
      "loss": 1.9549,
      "step": 370
    },
    {
      "epoch": 0.6397306397306397,
      "grad_norm": 5.835128307342529,
      "learning_rate": 9.60561826557425e-05,
      "loss": 2.1339,
      "step": 380
    },
    {
      "epoch": 0.6565656565656566,
      "grad_norm": 3.899601459503174,
      "learning_rate": 9.584772977958386e-05,
      "loss": 2.0714,
      "step": 390
    },
    {
      "epoch": 0.6734006734006734,
      "grad_norm": 4.960414886474609,
      "learning_rate": 9.563414710106382e-05,
      "loss": 1.9076,
      "step": 400
    },
    {
      "epoch": 0.6902356902356902,
      "grad_norm": 4.620044231414795,
      "learning_rate": 9.541545851748186e-05,
      "loss": 1.9361,
      "step": 410
    },
    {
      "epoch": 0.7070707070707071,
      "grad_norm": 3.7291998863220215,
      "learning_rate": 9.519168849742604e-05,
      "loss": 2.0462,
      "step": 420
    },
    {
      "epoch": 0.7239057239057239,
      "grad_norm": 3.538034200668335,
      "learning_rate": 9.49628620780352e-05,
      "loss": 1.9669,
      "step": 430
    },
    {
      "epoch": 0.7407407407407407,
      "grad_norm": 5.364833354949951,
      "learning_rate": 9.472900486219769e-05,
      "loss": 2.0228,
      "step": 440
    },
    {
      "epoch": 0.7575757575757576,
      "grad_norm": 5.041627883911133,
      "learning_rate": 9.449014301568671e-05,
      "loss": 1.7348,
      "step": 450
    },
    {
      "epoch": 0.7744107744107744,
      "grad_norm": 4.997405529022217,
      "learning_rate": 9.424630326423259e-05,
      "loss": 1.8545,
      "step": 460
    },
    {
      "epoch": 0.7912457912457912,
      "grad_norm": 2.6067373752593994,
      "learning_rate": 9.399751289053267e-05,
      "loss": 2.0126,
      "step": 470
    },
    {
      "epoch": 0.8080808080808081,
      "grad_norm": 3.1079647541046143,
      "learning_rate": 9.37437997311985e-05,
      "loss": 1.9199,
      "step": 480
    },
    {
      "epoch": 0.8249158249158249,
      "grad_norm": 5.038483142852783,
      "learning_rate": 9.348519217364145e-05,
      "loss": 1.8994,
      "step": 490
    },
    {
      "epoch": 0.8417508417508418,
      "grad_norm": 5.688510894775391,
      "learning_rate": 9.322171915289635e-05,
      "loss": 2.2316,
      "step": 500
    },
    {
      "epoch": 0.8585858585858586,
      "grad_norm": 4.461010456085205,
      "learning_rate": 9.295341014838412e-05,
      "loss": 2.0387,
      "step": 510
    },
    {
      "epoch": 0.8754208754208754,
      "grad_norm": 5.8227362632751465,
      "learning_rate": 9.268029518061334e-05,
      "loss": 1.8414,
      "step": 520
    },
    {
      "epoch": 0.8922558922558923,
      "grad_norm": 3.8748457431793213,
      "learning_rate": 9.24024048078213e-05,
      "loss": 1.6605,
      "step": 530
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 4.194492340087891,
      "learning_rate": 9.211977012255498e-05,
      "loss": 1.9772,
      "step": 540
    },
    {
      "epoch": 0.9259259259259259,
      "grad_norm": 3.5794293880462646,
      "learning_rate": 9.183242274819205e-05,
      "loss": 1.9885,
      "step": 550
    },
    {
      "epoch": 0.9427609427609428,
      "grad_norm": 5.171054840087891,
      "learning_rate": 9.154039483540273e-05,
      "loss": 2.0576,
      "step": 560
    },
    {
      "epoch": 0.9595959595959596,
      "grad_norm": 5.419192790985107,
      "learning_rate": 9.124371905855244e-05,
      "loss": 2.1108,
      "step": 570
    },
    {
      "epoch": 0.9764309764309764,
      "grad_norm": 4.6464362144470215,
      "learning_rate": 9.094242861204599e-05,
      "loss": 1.9026,
      "step": 580
    },
    {
      "epoch": 0.9932659932659933,
      "grad_norm": 5.7736029624938965,
      "learning_rate": 9.06365572066134e-05,
      "loss": 2.0602,
      "step": 590
    },
    {
      "epoch": 1.0101010101010102,
      "grad_norm": 4.716272354125977,
      "learning_rate": 9.032613906553833e-05,
      "loss": 1.84,
      "step": 600
    },
    {
      "epoch": 1.026936026936027,
      "grad_norm": 3.874558448791504,
      "learning_rate": 9.001120892082864e-05,
      "loss": 1.5363,
      "step": 610
    },
    {
      "epoch": 1.0437710437710437,
      "grad_norm": 5.56102180480957,
      "learning_rate": 8.969180200933047e-05,
      "loss": 1.844,
      "step": 620
    },
    {
      "epoch": 1.0606060606060606,
      "grad_norm": 3.6827306747436523,
      "learning_rate": 8.936795406878564e-05,
      "loss": 1.6759,
      "step": 630
    },
    {
      "epoch": 1.0774410774410774,
      "grad_norm": 5.514259338378906,
      "learning_rate": 8.903970133383297e-05,
      "loss": 1.7829,
      "step": 640
    },
    {
      "epoch": 1.0942760942760943,
      "grad_norm": 6.218451499938965,
      "learning_rate": 8.870708053195413e-05,
      "loss": 1.7874,
      "step": 650
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 3.614187240600586,
      "learning_rate": 8.837012887936426e-05,
      "loss": 1.7428,
      "step": 660
    },
    {
      "epoch": 1.127946127946128,
      "grad_norm": 6.251280784606934,
      "learning_rate": 8.802888407684791e-05,
      "loss": 1.8554,
      "step": 670
    },
    {
      "epoch": 1.144781144781145,
      "grad_norm": 5.510988235473633,
      "learning_rate": 8.768338430554082e-05,
      "loss": 1.8001,
      "step": 680
    },
    {
      "epoch": 1.1616161616161615,
      "grad_norm": 8.674945831298828,
      "learning_rate": 8.73336682226579e-05,
      "loss": 1.8848,
      "step": 690
    },
    {
      "epoch": 1.1784511784511784,
      "grad_norm": 4.106324195861816,
      "learning_rate": 8.697977495716793e-05,
      "loss": 1.9261,
      "step": 700
    },
    {
      "epoch": 1.1952861952861953,
      "grad_norm": 5.137358665466309,
      "learning_rate": 8.662174410541555e-05,
      "loss": 1.8415,
      "step": 710
    },
    {
      "epoch": 1.2121212121212122,
      "grad_norm": 4.780930519104004,
      "learning_rate": 8.625961572669088e-05,
      "loss": 1.8504,
      "step": 720
    },
    {
      "epoch": 1.228956228956229,
      "grad_norm": 4.5608110427856445,
      "learning_rate": 8.58934303387474e-05,
      "loss": 1.6073,
      "step": 730
    },
    {
      "epoch": 1.2457912457912457,
      "grad_norm": 6.230374813079834,
      "learning_rate": 8.552322891326846e-05,
      "loss": 1.9928,
      "step": 740
    },
    {
      "epoch": 1.2626262626262625,
      "grad_norm": 3.372148275375366,
      "learning_rate": 8.51490528712831e-05,
      "loss": 1.8528,
      "step": 750
    },
    {
      "epoch": 1.2794612794612794,
      "grad_norm": 4.421151638031006,
      "learning_rate": 8.477094407853153e-05,
      "loss": 1.8673,
      "step": 760
    },
    {
      "epoch": 1.2962962962962963,
      "grad_norm": 6.284719944000244,
      "learning_rate": 8.438894484078086e-05,
      "loss": 1.8469,
      "step": 770
    },
    {
      "epoch": 1.3131313131313131,
      "grad_norm": 4.316601276397705,
      "learning_rate": 8.400309789909156e-05,
      "loss": 1.7644,
      "step": 780
    },
    {
      "epoch": 1.32996632996633,
      "grad_norm": 3.073948860168457,
      "learning_rate": 8.36134464250353e-05,
      "loss": 1.8665,
      "step": 790
    },
    {
      "epoch": 1.3468013468013469,
      "grad_norm": 6.140207290649414,
      "learning_rate": 8.322003401586462e-05,
      "loss": 1.6866,
      "step": 800
    },
    {
      "epoch": 1.3636363636363638,
      "grad_norm": 5.514103889465332,
      "learning_rate": 8.282290468963481e-05,
      "loss": 1.9401,
      "step": 810
    },
    {
      "epoch": 1.3804713804713804,
      "grad_norm": 6.389695644378662,
      "learning_rate": 8.242210288027893e-05,
      "loss": 1.8484,
      "step": 820
    },
    {
      "epoch": 1.3973063973063973,
      "grad_norm": 5.197141647338867,
      "learning_rate": 8.201767343263612e-05,
      "loss": 1.8826,
      "step": 830
    },
    {
      "epoch": 1.4141414141414141,
      "grad_norm": 6.3362135887146,
      "learning_rate": 8.16096615974341e-05,
      "loss": 1.7133,
      "step": 840
    },
    {
      "epoch": 1.430976430976431,
      "grad_norm": 5.861750602722168,
      "learning_rate": 8.119811302622608e-05,
      "loss": 1.4722,
      "step": 850
    },
    {
      "epoch": 1.4478114478114479,
      "grad_norm": 4.762773036956787,
      "learning_rate": 8.07830737662829e-05,
      "loss": 1.8461,
      "step": 860
    },
    {
      "epoch": 1.4646464646464645,
      "grad_norm": 6.76863431930542,
      "learning_rate": 8.036459025544105e-05,
      "loss": 1.7795,
      "step": 870
    },
    {
      "epoch": 1.4814814814814814,
      "grad_norm": 5.871642589569092,
      "learning_rate": 7.994270931690662e-05,
      "loss": 1.7398,
      "step": 880
    },
    {
      "epoch": 1.4983164983164983,
      "grad_norm": 5.272466659545898,
      "learning_rate": 7.95174781540165e-05,
      "loss": 1.6109,
      "step": 890
    },
    {
      "epoch": 1.5151515151515151,
      "grad_norm": 4.588648319244385,
      "learning_rate": 7.908894434495689e-05,
      "loss": 1.8439,
      "step": 900
    },
    {
      "epoch": 1.531986531986532,
      "grad_norm": 6.138535022735596,
      "learning_rate": 7.865715583743982e-05,
      "loss": 1.8017,
      "step": 910
    },
    {
      "epoch": 1.5488215488215489,
      "grad_norm": 7.962610721588135,
      "learning_rate": 7.822216094333847e-05,
      "loss": 1.9258,
      "step": 920
    },
    {
      "epoch": 1.5656565656565657,
      "grad_norm": 3.413698196411133,
      "learning_rate": 7.778400833328156e-05,
      "loss": 1.8661,
      "step": 930
    },
    {
      "epoch": 1.5824915824915826,
      "grad_norm": 7.47783088684082,
      "learning_rate": 7.73427470312078e-05,
      "loss": 1.7909,
      "step": 940
    },
    {
      "epoch": 1.5993265993265995,
      "grad_norm": 7.033029079437256,
      "learning_rate": 7.689842640888063e-05,
      "loss": 1.9955,
      "step": 950
    },
    {
      "epoch": 1.6161616161616161,
      "grad_norm": 7.569550037384033,
      "learning_rate": 7.645109618036417e-05,
      "loss": 1.92,
      "step": 960
    },
    {
      "epoch": 1.632996632996633,
      "grad_norm": 6.084757328033447,
      "learning_rate": 7.600080639646077e-05,
      "loss": 1.6276,
      "step": 970
    },
    {
      "epoch": 1.6498316498316499,
      "grad_norm": 7.842527866363525,
      "learning_rate": 7.554760743911103e-05,
      "loss": 1.5795,
      "step": 980
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 5.937313556671143,
      "learning_rate": 7.509155001575656e-05,
      "loss": 1.8099,
      "step": 990
    },
    {
      "epoch": 1.6835016835016834,
      "grad_norm": 6.3635711669921875,
      "learning_rate": 7.463268515366651e-05,
      "loss": 1.5458,
      "step": 1000
    },
    {
      "epoch": 1.7003367003367003,
      "grad_norm": 6.182557106018066,
      "learning_rate": 7.417106419422819e-05,
      "loss": 1.8714,
      "step": 1010
    },
    {
      "epoch": 1.7171717171717171,
      "grad_norm": 5.63146448135376,
      "learning_rate": 7.37067387872027e-05,
      "loss": 1.6281,
      "step": 1020
    },
    {
      "epoch": 1.734006734006734,
      "grad_norm": 5.923813819885254,
      "learning_rate": 7.32397608849458e-05,
      "loss": 1.7839,
      "step": 1030
    },
    {
      "epoch": 1.7508417508417509,
      "grad_norm": 5.0235595703125,
      "learning_rate": 7.277018273659517e-05,
      "loss": 1.6649,
      "step": 1040
    },
    {
      "epoch": 1.7676767676767677,
      "grad_norm": 4.771322250366211,
      "learning_rate": 7.229805688222432e-05,
      "loss": 1.7509,
      "step": 1050
    },
    {
      "epoch": 1.7845117845117846,
      "grad_norm": 5.0504374504089355,
      "learning_rate": 7.182343614696412e-05,
      "loss": 1.8259,
      "step": 1060
    },
    {
      "epoch": 1.8013468013468015,
      "grad_norm": 6.048926830291748,
      "learning_rate": 7.13463736350921e-05,
      "loss": 1.7285,
      "step": 1070
    },
    {
      "epoch": 1.8181818181818183,
      "grad_norm": 4.323897838592529,
      "learning_rate": 7.08669227240909e-05,
      "loss": 1.6468,
      "step": 1080
    },
    {
      "epoch": 1.835016835016835,
      "grad_norm": 3.390502691268921,
      "learning_rate": 7.038513705867592e-05,
      "loss": 1.7575,
      "step": 1090
    },
    {
      "epoch": 1.8518518518518519,
      "grad_norm": 4.692629337310791,
      "learning_rate": 6.990107054479312e-05,
      "loss": 1.9013,
      "step": 1100
    },
    {
      "epoch": 1.8686868686868687,
      "grad_norm": 4.662871837615967,
      "learning_rate": 6.941477734358762e-05,
      "loss": 1.9329,
      "step": 1110
    },
    {
      "epoch": 1.8855218855218854,
      "grad_norm": 4.749701976776123,
      "learning_rate": 6.89263118653437e-05,
      "loss": 1.7826,
      "step": 1120
    },
    {
      "epoch": 1.9023569023569022,
      "grad_norm": 5.770020008087158,
      "learning_rate": 6.843572876339705e-05,
      "loss": 1.8622,
      "step": 1130
    },
    {
      "epoch": 1.9191919191919191,
      "grad_norm": 5.359264373779297,
      "learning_rate": 6.794308292801954e-05,
      "loss": 1.5485,
      "step": 1140
    },
    {
      "epoch": 1.936026936026936,
      "grad_norm": 5.067996978759766,
      "learning_rate": 6.744842948027786e-05,
      "loss": 1.9019,
      "step": 1150
    },
    {
      "epoch": 1.9528619528619529,
      "grad_norm": 7.9194655418396,
      "learning_rate": 6.695182376586603e-05,
      "loss": 1.6901,
      "step": 1160
    },
    {
      "epoch": 1.9696969696969697,
      "grad_norm": 5.651750564575195,
      "learning_rate": 6.645332134891296e-05,
      "loss": 1.7666,
      "step": 1170
    },
    {
      "epoch": 1.9865319865319866,
      "grad_norm": 5.251967430114746,
      "learning_rate": 6.59529780057654e-05,
      "loss": 1.5681,
      "step": 1180
    },
    {
      "epoch": 2.0033670033670035,
      "grad_norm": 6.56718635559082,
      "learning_rate": 6.545084971874738e-05,
      "loss": 1.7149,
      "step": 1190
    },
    {
      "epoch": 2.0202020202020203,
      "grad_norm": 4.939560413360596,
      "learning_rate": 6.494699266989635e-05,
      "loss": 1.6773,
      "step": 1200
    },
    {
      "epoch": 2.037037037037037,
      "grad_norm": 4.887939453125,
      "learning_rate": 6.44414632346772e-05,
      "loss": 1.6773,
      "step": 1210
    },
    {
      "epoch": 2.053872053872054,
      "grad_norm": 6.329192638397217,
      "learning_rate": 6.39343179756744e-05,
      "loss": 1.5325,
      "step": 1220
    },
    {
      "epoch": 2.0707070707070705,
      "grad_norm": 9.916997909545898,
      "learning_rate": 6.342561363626348e-05,
      "loss": 1.7961,
      "step": 1230
    },
    {
      "epoch": 2.0875420875420874,
      "grad_norm": 5.777223110198975,
      "learning_rate": 6.291540713426206e-05,
      "loss": 1.5734,
      "step": 1240
    },
    {
      "epoch": 2.1043771043771042,
      "grad_norm": 7.053924560546875,
      "learning_rate": 6.240375555556145e-05,
      "loss": 1.4136,
      "step": 1250
    },
    {
      "epoch": 2.121212121212121,
      "grad_norm": 6.793748378753662,
      "learning_rate": 6.189071614773941e-05,
      "loss": 1.356,
      "step": 1260
    },
    {
      "epoch": 2.138047138047138,
      "grad_norm": 6.218673229217529,
      "learning_rate": 6.137634631365491e-05,
      "loss": 1.699,
      "step": 1270
    },
    {
      "epoch": 2.154882154882155,
      "grad_norm": 7.758062362670898,
      "learning_rate": 6.0860703605025395e-05,
      "loss": 1.6594,
      "step": 1280
    },
    {
      "epoch": 2.1717171717171717,
      "grad_norm": 5.516324520111084,
      "learning_rate": 6.0343845715987436e-05,
      "loss": 1.4833,
      "step": 1290
    },
    {
      "epoch": 2.1885521885521886,
      "grad_norm": 9.076506614685059,
      "learning_rate": 5.982583047664151e-05,
      "loss": 1.5538,
      "step": 1300
    },
    {
      "epoch": 2.2053872053872055,
      "grad_norm": 6.558327674865723,
      "learning_rate": 5.9306715846581506e-05,
      "loss": 1.5596,
      "step": 1310
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 5.533405303955078,
      "learning_rate": 5.878655990840971e-05,
      "loss": 1.5295,
      "step": 1320
    },
    {
      "epoch": 2.239057239057239,
      "grad_norm": 7.087987899780273,
      "learning_rate": 5.826542086123812e-05,
      "loss": 1.6561,
      "step": 1330
    },
    {
      "epoch": 2.255892255892256,
      "grad_norm": 6.008196830749512,
      "learning_rate": 5.7743357014176624e-05,
      "loss": 1.7758,
      "step": 1340
    },
    {
      "epoch": 2.2727272727272725,
      "grad_norm": 6.076262474060059,
      "learning_rate": 5.7220426779809e-05,
      "loss": 1.5965,
      "step": 1350
    },
    {
      "epoch": 2.28956228956229,
      "grad_norm": 3.613360643386841,
      "learning_rate": 5.669668866765717e-05,
      "loss": 1.542,
      "step": 1360
    },
    {
      "epoch": 2.3063973063973062,
      "grad_norm": 6.3600754737854,
      "learning_rate": 5.617220127763474e-05,
      "loss": 1.6351,
      "step": 1370
    },
    {
      "epoch": 2.323232323232323,
      "grad_norm": 8.790763854980469,
      "learning_rate": 5.5647023293490405e-05,
      "loss": 1.4704,
      "step": 1380
    },
    {
      "epoch": 2.34006734006734,
      "grad_norm": 6.171703815460205,
      "learning_rate": 5.5121213476241895e-05,
      "loss": 1.7967,
      "step": 1390
    },
    {
      "epoch": 2.356902356902357,
      "grad_norm": 9.384291648864746,
      "learning_rate": 5.4594830657601384e-05,
      "loss": 1.5835,
      "step": 1400
    },
    {
      "epoch": 2.3737373737373737,
      "grad_norm": 7.17268180847168,
      "learning_rate": 5.4067933733392915e-05,
      "loss": 1.5497,
      "step": 1410
    },
    {
      "epoch": 2.3905723905723906,
      "grad_norm": 5.120451927185059,
      "learning_rate": 5.3540581656962706e-05,
      "loss": 1.5433,
      "step": 1420
    },
    {
      "epoch": 2.4074074074074074,
      "grad_norm": 5.923940658569336,
      "learning_rate": 5.301283343258293e-05,
      "loss": 1.7148,
      "step": 1430
    },
    {
      "epoch": 2.4242424242424243,
      "grad_norm": 5.726093292236328,
      "learning_rate": 5.248474810884988e-05,
      "loss": 1.5024,
      "step": 1440
    },
    {
      "epoch": 2.441077441077441,
      "grad_norm": 6.538606643676758,
      "learning_rate": 5.195638477207722e-05,
      "loss": 1.7293,
      "step": 1450
    },
    {
      "epoch": 2.457912457912458,
      "grad_norm": 6.038503646850586,
      "learning_rate": 5.142780253968481e-05,
      "loss": 1.6689,
      "step": 1460
    },
    {
      "epoch": 2.474747474747475,
      "grad_norm": 5.799315452575684,
      "learning_rate": 5.089906055358432e-05,
      "loss": 1.4949,
      "step": 1470
    },
    {
      "epoch": 2.4915824915824913,
      "grad_norm": 7.005345344543457,
      "learning_rate": 5.0370217973561896e-05,
      "loss": 1.5766,
      "step": 1480
    },
    {
      "epoch": 2.5084175084175087,
      "grad_norm": 6.131919860839844,
      "learning_rate": 4.984133397065889e-05,
      "loss": 1.5672,
      "step": 1490
    },
    {
      "epoch": 2.525252525252525,
      "grad_norm": 7.548540115356445,
      "learning_rate": 4.931246772055141e-05,
      "loss": 1.5532,
      "step": 1500
    },
    {
      "epoch": 2.542087542087542,
      "grad_norm": 5.111536502838135,
      "learning_rate": 4.878367839692922e-05,
      "loss": 1.5928,
      "step": 1510
    },
    {
      "epoch": 2.558922558922559,
      "grad_norm": 7.641093730926514,
      "learning_rate": 4.825502516487497e-05,
      "loss": 1.5451,
      "step": 1520
    },
    {
      "epoch": 2.5757575757575757,
      "grad_norm": 9.059408187866211,
      "learning_rate": 4.772656717424428e-05,
      "loss": 1.4964,
      "step": 1530
    },
    {
      "epoch": 2.5925925925925926,
      "grad_norm": 8.070810317993164,
      "learning_rate": 4.7198363553047656e-05,
      "loss": 1.7449,
      "step": 1540
    },
    {
      "epoch": 2.6094276094276094,
      "grad_norm": 6.4887285232543945,
      "learning_rate": 4.667047340083481e-05,
      "loss": 1.3535,
      "step": 1550
    },
    {
      "epoch": 2.6262626262626263,
      "grad_norm": 6.896193027496338,
      "learning_rate": 4.614295578208202e-05,
      "loss": 1.4267,
      "step": 1560
    },
    {
      "epoch": 2.643097643097643,
      "grad_norm": 5.983852386474609,
      "learning_rate": 4.561586971958364e-05,
      "loss": 1.6279,
      "step": 1570
    },
    {
      "epoch": 2.65993265993266,
      "grad_norm": 6.656632900238037,
      "learning_rate": 4.508927418784815e-05,
      "loss": 1.649,
      "step": 1580
    },
    {
      "epoch": 2.676767676767677,
      "grad_norm": 6.072704792022705,
      "learning_rate": 4.456322810649959e-05,
      "loss": 1.4655,
      "step": 1590
    },
    {
      "epoch": 2.6936026936026938,
      "grad_norm": 5.997891902923584,
      "learning_rate": 4.403779033368521e-05,
      "loss": 1.8187,
      "step": 1600
    },
    {
      "epoch": 2.71043771043771,
      "grad_norm": 9.374914169311523,
      "learning_rate": 4.351301965948991e-05,
      "loss": 1.6871,
      "step": 1610
    },
    {
      "epoch": 2.7272727272727275,
      "grad_norm": 6.586580753326416,
      "learning_rate": 4.298897479935847e-05,
      "loss": 1.5373,
      "step": 1620
    },
    {
      "epoch": 2.744107744107744,
      "grad_norm": 6.265583038330078,
      "learning_rate": 4.246571438752585e-05,
      "loss": 1.5547,
      "step": 1630
    },
    {
      "epoch": 2.760942760942761,
      "grad_norm": 8.23583698272705,
      "learning_rate": 4.19432969704568e-05,
      "loss": 1.7207,
      "step": 1640
    },
    {
      "epoch": 2.7777777777777777,
      "grad_norm": 5.0888352394104,
      "learning_rate": 4.1421781000295254e-05,
      "loss": 1.6238,
      "step": 1650
    },
    {
      "epoch": 2.7946127946127945,
      "grad_norm": 6.845096111297607,
      "learning_rate": 4.0901224828324225e-05,
      "loss": 1.4966,
      "step": 1660
    },
    {
      "epoch": 2.8114478114478114,
      "grad_norm": 8.343280792236328,
      "learning_rate": 4.038168669843697e-05,
      "loss": 1.6188,
      "step": 1670
    },
    {
      "epoch": 2.8282828282828283,
      "grad_norm": 5.750819206237793,
      "learning_rate": 3.986322474062025e-05,
      "loss": 1.3636,
      "step": 1680
    },
    {
      "epoch": 2.845117845117845,
      "grad_norm": 6.268388271331787,
      "learning_rate": 3.934589696445032e-05,
      "loss": 1.3439,
      "step": 1690
    },
    {
      "epoch": 2.861952861952862,
      "grad_norm": 6.997115135192871,
      "learning_rate": 3.882976125260229e-05,
      "loss": 1.5924,
      "step": 1700
    },
    {
      "epoch": 2.878787878787879,
      "grad_norm": 6.892436504364014,
      "learning_rate": 3.8314875354373804e-05,
      "loss": 1.626,
      "step": 1710
    },
    {
      "epoch": 2.8956228956228958,
      "grad_norm": 7.43792200088501,
      "learning_rate": 3.780129687922359e-05,
      "loss": 1.5883,
      "step": 1720
    },
    {
      "epoch": 2.9124579124579126,
      "grad_norm": 5.633615493774414,
      "learning_rate": 3.728908329032567e-05,
      "loss": 1.612,
      "step": 1730
    },
    {
      "epoch": 2.929292929292929,
      "grad_norm": 8.489230155944824,
      "learning_rate": 3.67782918981399e-05,
      "loss": 1.5789,
      "step": 1740
    },
    {
      "epoch": 2.9461279461279464,
      "grad_norm": 5.623019695281982,
      "learning_rate": 3.6268979853999675e-05,
      "loss": 1.6161,
      "step": 1750
    },
    {
      "epoch": 2.962962962962963,
      "grad_norm": 7.306431293487549,
      "learning_rate": 3.5761204143717385e-05,
      "loss": 1.6814,
      "step": 1760
    },
    {
      "epoch": 2.9797979797979797,
      "grad_norm": 7.618320465087891,
      "learning_rate": 3.525502158120836e-05,
      "loss": 1.5759,
      "step": 1770
    },
    {
      "epoch": 2.9966329966329965,
      "grad_norm": 7.48720645904541,
      "learning_rate": 3.475048880213407e-05,
      "loss": 1.6016,
      "step": 1780
    },
    {
      "epoch": 3.0134680134680134,
      "grad_norm": 7.253063678741455,
      "learning_rate": 3.424766225756537e-05,
      "loss": 1.4955,
      "step": 1790
    },
    {
      "epoch": 3.0303030303030303,
      "grad_norm": 6.845301628112793,
      "learning_rate": 3.374659820766625e-05,
      "loss": 1.3355,
      "step": 1800
    },
    {
      "epoch": 3.047138047138047,
      "grad_norm": 7.639352798461914,
      "learning_rate": 3.324735271539899e-05,
      "loss": 1.4229,
      "step": 1810
    },
    {
      "epoch": 3.063973063973064,
      "grad_norm": 10.282553672790527,
      "learning_rate": 3.274998164025148e-05,
      "loss": 1.5092,
      "step": 1820
    },
    {
      "epoch": 3.080808080808081,
      "grad_norm": 10.014019012451172,
      "learning_rate": 3.2254540631987115e-05,
      "loss": 1.4272,
      "step": 1830
    },
    {
      "epoch": 3.0976430976430978,
      "grad_norm": 7.387519359588623,
      "learning_rate": 3.176108512441839e-05,
      "loss": 1.3351,
      "step": 1840
    },
    {
      "epoch": 3.1144781144781146,
      "grad_norm": 8.109609603881836,
      "learning_rate": 3.12696703292044e-05,
      "loss": 1.3623,
      "step": 1850
    },
    {
      "epoch": 3.1313131313131315,
      "grad_norm": 7.776191711425781,
      "learning_rate": 3.078035122967342e-05,
      "loss": 1.4865,
      "step": 1860
    },
    {
      "epoch": 3.148148148148148,
      "grad_norm": 8.434733390808105,
      "learning_rate": 3.0293182574670903e-05,
      "loss": 1.4666,
      "step": 1870
    },
    {
      "epoch": 3.164983164983165,
      "grad_norm": 6.92057991027832,
      "learning_rate": 2.9808218872433767e-05,
      "loss": 1.2797,
      "step": 1880
    },
    {
      "epoch": 3.1818181818181817,
      "grad_norm": 9.266314506530762,
      "learning_rate": 2.9325514384491576e-05,
      "loss": 1.4424,
      "step": 1890
    },
    {
      "epoch": 3.1986531986531985,
      "grad_norm": 8.057415962219238,
      "learning_rate": 2.884512311959532e-05,
      "loss": 1.3554,
      "step": 1900
    },
    {
      "epoch": 3.2154882154882154,
      "grad_norm": 6.089616775512695,
      "learning_rate": 2.8367098827674578e-05,
      "loss": 1.6114,
      "step": 1910
    },
    {
      "epoch": 3.2323232323232323,
      "grad_norm": 6.915341377258301,
      "learning_rate": 2.7891494993823448e-05,
      "loss": 1.4805,
      "step": 1920
    },
    {
      "epoch": 3.249158249158249,
      "grad_norm": 5.5031938552856445,
      "learning_rate": 2.7418364832316224e-05,
      "loss": 1.4595,
      "step": 1930
    },
    {
      "epoch": 3.265993265993266,
      "grad_norm": 7.89310359954834,
      "learning_rate": 2.694776128065345e-05,
      "loss": 1.4193,
      "step": 1940
    },
    {
      "epoch": 3.282828282828283,
      "grad_norm": 6.689288139343262,
      "learning_rate": 2.6479736993638814e-05,
      "loss": 1.2656,
      "step": 1950
    },
    {
      "epoch": 3.2996632996632997,
      "grad_norm": 10.075740814208984,
      "learning_rate": 2.6014344337487707e-05,
      "loss": 1.4449,
      "step": 1960
    },
    {
      "epoch": 3.3164983164983166,
      "grad_norm": 9.881210327148438,
      "learning_rate": 2.5551635383968065e-05,
      "loss": 1.4022,
      "step": 1970
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 8.06873893737793,
      "learning_rate": 2.509166190457425e-05,
      "loss": 1.3983,
      "step": 1980
    },
    {
      "epoch": 3.3501683501683504,
      "grad_norm": 6.64177131652832,
      "learning_rate": 2.4634475364734357e-05,
      "loss": 1.4472,
      "step": 1990
    },
    {
      "epoch": 3.3670033670033668,
      "grad_norm": 8.789559364318848,
      "learning_rate": 2.418012691805191e-05,
      "loss": 1.209,
      "step": 2000
    },
    {
      "epoch": 3.3838383838383836,
      "grad_norm": 8.624828338623047,
      "learning_rate": 2.3728667400582387e-05,
      "loss": 1.5748,
      "step": 2010
    },
    {
      "epoch": 3.4006734006734005,
      "grad_norm": 9.258682250976562,
      "learning_rate": 2.3280147325145285e-05,
      "loss": 1.394,
      "step": 2020
    },
    {
      "epoch": 3.4175084175084174,
      "grad_norm": 9.381258964538574,
      "learning_rate": 2.283461687567236e-05,
      "loss": 1.342,
      "step": 2030
    },
    {
      "epoch": 3.4343434343434343,
      "grad_norm": 8.492450714111328,
      "learning_rate": 2.2392125901592613e-05,
      "loss": 1.4187,
      "step": 2040
    },
    {
      "epoch": 3.451178451178451,
      "grad_norm": 8.460450172424316,
      "learning_rate": 2.1952723912254854e-05,
      "loss": 1.5793,
      "step": 2050
    },
    {
      "epoch": 3.468013468013468,
      "grad_norm": 7.902731895446777,
      "learning_rate": 2.1516460071388062e-05,
      "loss": 1.3078,
      "step": 2060
    },
    {
      "epoch": 3.484848484848485,
      "grad_norm": 7.137658596038818,
      "learning_rate": 2.1083383191600674e-05,
      "loss": 1.4719,
      "step": 2070
    },
    {
      "epoch": 3.5016835016835017,
      "grad_norm": 8.714941024780273,
      "learning_rate": 2.0653541728919e-05,
      "loss": 1.5328,
      "step": 2080
    },
    {
      "epoch": 3.5185185185185186,
      "grad_norm": 8.673201560974121,
      "learning_rate": 2.0226983777365604e-05,
      "loss": 1.3057,
      "step": 2090
    },
    {
      "epoch": 3.5353535353535355,
      "grad_norm": 7.966537952423096,
      "learning_rate": 1.9803757063578145e-05,
      "loss": 1.4254,
      "step": 2100
    },
    {
      "epoch": 3.5521885521885523,
      "grad_norm": 9.389535903930664,
      "learning_rate": 1.938390894146938e-05,
      "loss": 1.3431,
      "step": 2110
    },
    {
      "epoch": 3.569023569023569,
      "grad_norm": 6.817751407623291,
      "learning_rate": 1.8967486386928817e-05,
      "loss": 1.5868,
      "step": 2120
    },
    {
      "epoch": 3.5858585858585856,
      "grad_norm": 6.646559715270996,
      "learning_rate": 1.8554535992566685e-05,
      "loss": 1.4272,
      "step": 2130
    },
    {
      "epoch": 3.602693602693603,
      "grad_norm": 6.708581924438477,
      "learning_rate": 1.8145103962500792e-05,
      "loss": 1.4346,
      "step": 2140
    },
    {
      "epoch": 3.6195286195286194,
      "grad_norm": 8.11893367767334,
      "learning_rate": 1.773923610718686e-05,
      "loss": 1.6106,
      "step": 2150
    },
    {
      "epoch": 3.6363636363636362,
      "grad_norm": 12.631006240844727,
      "learning_rate": 1.7336977838292866e-05,
      "loss": 1.4016,
      "step": 2160
    },
    {
      "epoch": 3.653198653198653,
      "grad_norm": 7.965667724609375,
      "learning_rate": 1.6938374163618054e-05,
      "loss": 1.5037,
      "step": 2170
    },
    {
      "epoch": 3.67003367003367,
      "grad_norm": 6.443951606750488,
      "learning_rate": 1.6543469682057106e-05,
      "loss": 1.4456,
      "step": 2180
    },
    {
      "epoch": 3.686868686868687,
      "grad_norm": 7.6936821937561035,
      "learning_rate": 1.6152308578610036e-05,
      "loss": 1.3936,
      "step": 2190
    },
    {
      "epoch": 3.7037037037037037,
      "grad_norm": 6.109166622161865,
      "learning_rate": 1.5764934619438517e-05,
      "loss": 1.4156,
      "step": 2200
    },
    {
      "epoch": 3.7205387205387206,
      "grad_norm": 8.118368148803711,
      "learning_rate": 1.5381391146968866e-05,
      "loss": 1.5048,
      "step": 2210
    },
    {
      "epoch": 3.7373737373737375,
      "grad_norm": 8.76032829284668,
      "learning_rate": 1.5001721075042618e-05,
      "loss": 1.5727,
      "step": 2220
    },
    {
      "epoch": 3.7542087542087543,
      "grad_norm": 8.14488697052002,
      "learning_rate": 1.4625966884114995e-05,
      "loss": 1.4121,
      "step": 2230
    },
    {
      "epoch": 3.771043771043771,
      "grad_norm": 6.195301055908203,
      "learning_rate": 1.4254170616501827e-05,
      "loss": 1.4428,
      "step": 2240
    },
    {
      "epoch": 3.787878787878788,
      "grad_norm": 9.524380683898926,
      "learning_rate": 1.3886373871675568e-05,
      "loss": 1.4075,
      "step": 2250
    },
    {
      "epoch": 3.8047138047138045,
      "grad_norm": 7.195601463317871,
      "learning_rate": 1.3522617801610765e-05,
      "loss": 1.4619,
      "step": 2260
    },
    {
      "epoch": 3.821548821548822,
      "grad_norm": 10.130084037780762,
      "learning_rate": 1.3162943106179749e-05,
      "loss": 1.4318,
      "step": 2270
    },
    {
      "epoch": 3.8383838383838382,
      "grad_norm": 10.153435707092285,
      "learning_rate": 1.2807390028598715e-05,
      "loss": 1.3672,
      "step": 2280
    },
    {
      "epoch": 3.855218855218855,
      "grad_norm": 11.816568374633789,
      "learning_rate": 1.245599835092504e-05,
      "loss": 1.4293,
      "step": 2290
    },
    {
      "epoch": 3.872053872053872,
      "grad_norm": 13.034727096557617,
      "learning_rate": 1.2108807389606158e-05,
      "loss": 1.3861,
      "step": 2300
    },
    {
      "epoch": 3.888888888888889,
      "grad_norm": 8.562345504760742,
      "learning_rate": 1.1765855991080533e-05,
      "loss": 1.4212,
      "step": 2310
    },
    {
      "epoch": 3.9057239057239057,
      "grad_norm": 10.676769256591797,
      "learning_rate": 1.1427182527431191e-05,
      "loss": 1.4439,
      "step": 2320
    },
    {
      "epoch": 3.9225589225589226,
      "grad_norm": 7.595232963562012,
      "learning_rate": 1.1092824892092373e-05,
      "loss": 1.4097,
      "step": 2330
    },
    {
      "epoch": 3.9393939393939394,
      "grad_norm": 8.294414520263672,
      "learning_rate": 1.076282049560972e-05,
      "loss": 1.4714,
      "step": 2340
    },
    {
      "epoch": 3.9562289562289563,
      "grad_norm": 7.921667575836182,
      "learning_rate": 1.0437206261454485e-05,
      "loss": 1.3476,
      "step": 2350
    },
    {
      "epoch": 3.973063973063973,
      "grad_norm": 7.450766086578369,
      "learning_rate": 1.0116018621892237e-05,
      "loss": 1.5314,
      "step": 2360
    },
    {
      "epoch": 3.98989898989899,
      "grad_norm": 6.315002918243408,
      "learning_rate": 9.830764196878872e-06,
      "loss": 1.321,
      "step": 2370
    },
    {
      "epoch": 4.006734006734007,
      "grad_norm": 8.043960571289062,
      "learning_rate": 9.518085680841365e-06,
      "loss": 1.2904,
      "step": 2380
    },
    {
      "epoch": 4.023569023569023,
      "grad_norm": 9.943821907043457,
      "learning_rate": 9.209936597791407e-06,
      "loss": 1.2939,
      "step": 2390
    },
    {
      "epoch": 4.040404040404041,
      "grad_norm": 8.007933616638184,
      "learning_rate": 8.906351425856952e-06,
      "loss": 1.2967,
      "step": 2400
    },
    {
      "epoch": 4.057239057239057,
      "grad_norm": 7.228787899017334,
      "learning_rate": 8.607364132519919e-06,
      "loss": 1.3297,
      "step": 2410
    },
    {
      "epoch": 4.074074074074074,
      "grad_norm": 8.932138442993164,
      "learning_rate": 8.313008170815612e-06,
      "loss": 1.3595,
      "step": 2420
    },
    {
      "epoch": 4.090909090909091,
      "grad_norm": 9.039047241210938,
      "learning_rate": 8.023316475589754e-06,
      "loss": 1.3622,
      "step": 2430
    },
    {
      "epoch": 4.107744107744108,
      "grad_norm": 9.390512466430664,
      "learning_rate": 7.738321459813513e-06,
      "loss": 1.3331,
      "step": 2440
    },
    {
      "epoch": 4.124579124579125,
      "grad_norm": 8.676645278930664,
      "learning_rate": 7.45805501095681e-06,
      "loss": 1.169,
      "step": 2450
    },
    {
      "epoch": 4.141414141414141,
      "grad_norm": 7.758623123168945,
      "learning_rate": 7.182548487420554e-06,
      "loss": 1.5099,
      "step": 2460
    },
    {
      "epoch": 4.158249158249158,
      "grad_norm": 6.341349124908447,
      "learning_rate": 6.911832715027999e-06,
      "loss": 1.507,
      "step": 2470
    },
    {
      "epoch": 4.175084175084175,
      "grad_norm": 7.741669178009033,
      "learning_rate": 6.645937983575729e-06,
      "loss": 1.3111,
      "step": 2480
    },
    {
      "epoch": 4.191919191919192,
      "grad_norm": 7.870728969573975,
      "learning_rate": 6.384894043444567e-06,
      "loss": 1.2748,
      "step": 2490
    },
    {
      "epoch": 4.2087542087542085,
      "grad_norm": 8.257475852966309,
      "learning_rate": 6.128730102270897e-06,
      "loss": 1.2472,
      "step": 2500
    },
    {
      "epoch": 4.225589225589226,
      "grad_norm": 8.990983009338379,
      "learning_rate": 5.877474821678697e-06,
      "loss": 1.3064,
      "step": 2510
    },
    {
      "epoch": 4.242424242424242,
      "grad_norm": 7.860503196716309,
      "learning_rate": 5.631156314072605e-06,
      "loss": 1.4658,
      "step": 2520
    },
    {
      "epoch": 4.2592592592592595,
      "grad_norm": 9.644444465637207,
      "learning_rate": 5.3898021394925165e-06,
      "loss": 1.2531,
      "step": 2530
    },
    {
      "epoch": 4.276094276094276,
      "grad_norm": 11.13022518157959,
      "learning_rate": 5.153439302529945e-06,
      "loss": 1.4485,
      "step": 2540
    },
    {
      "epoch": 4.292929292929293,
      "grad_norm": 9.415964126586914,
      "learning_rate": 4.922094249306558e-06,
      "loss": 1.3118,
      "step": 2550
    },
    {
      "epoch": 4.30976430976431,
      "grad_norm": 6.407840728759766,
      "learning_rate": 4.695792864515119e-06,
      "loss": 1.5043,
      "step": 2560
    },
    {
      "epoch": 4.326599326599327,
      "grad_norm": 8.742469787597656,
      "learning_rate": 4.474560468523375e-06,
      "loss": 1.364,
      "step": 2570
    },
    {
      "epoch": 4.343434343434343,
      "grad_norm": 7.465949058532715,
      "learning_rate": 4.258421814540992e-06,
      "loss": 1.2875,
      "step": 2580
    },
    {
      "epoch": 4.36026936026936,
      "grad_norm": 7.098089218139648,
      "learning_rate": 4.047401085849944e-06,
      "loss": 1.3822,
      "step": 2590
    },
    {
      "epoch": 4.377104377104377,
      "grad_norm": 7.307161808013916,
      "learning_rate": 3.841521893098732e-06,
      "loss": 1.3506,
      "step": 2600
    },
    {
      "epoch": 4.393939393939394,
      "grad_norm": 7.338278770446777,
      "learning_rate": 3.6408072716606346e-06,
      "loss": 1.4424,
      "step": 2610
    },
    {
      "epoch": 4.410774410774411,
      "grad_norm": 10.28330135345459,
      "learning_rate": 3.445279679056307e-06,
      "loss": 1.4255,
      "step": 2620
    },
    {
      "epoch": 4.427609427609427,
      "grad_norm": 8.609440803527832,
      "learning_rate": 3.254960992441097e-06,
      "loss": 1.3949,
      "step": 2630
    },
    {
      "epoch": 4.444444444444445,
      "grad_norm": 7.79900598526001,
      "learning_rate": 3.069872506157212e-06,
      "loss": 1.3153,
      "step": 2640
    },
    {
      "epoch": 4.461279461279461,
      "grad_norm": 10.621522903442383,
      "learning_rate": 2.8900349293511987e-06,
      "loss": 1.4709,
      "step": 2650
    },
    {
      "epoch": 4.478114478114478,
      "grad_norm": 10.61673355102539,
      "learning_rate": 2.715468383656783e-06,
      "loss": 1.3688,
      "step": 2660
    },
    {
      "epoch": 4.494949494949495,
      "grad_norm": 9.04110050201416,
      "learning_rate": 2.546192400943537e-06,
      "loss": 1.2608,
      "step": 2670
    },
    {
      "epoch": 4.511784511784512,
      "grad_norm": 10.162277221679688,
      "learning_rate": 2.3822259211314978e-06,
      "loss": 1.37,
      "step": 2680
    },
    {
      "epoch": 4.5286195286195285,
      "grad_norm": 7.943367958068848,
      "learning_rate": 2.2235872900720156e-06,
      "loss": 1.3611,
      "step": 2690
    },
    {
      "epoch": 4.545454545454545,
      "grad_norm": 7.070958614349365,
      "learning_rate": 2.070294257495081e-06,
      "loss": 1.2394,
      "step": 2700
    },
    {
      "epoch": 4.562289562289562,
      "grad_norm": 8.838357925415039,
      "learning_rate": 1.9223639750233445e-06,
      "loss": 1.283,
      "step": 2710
    },
    {
      "epoch": 4.57912457912458,
      "grad_norm": 8.233187675476074,
      "learning_rate": 1.7798129942530551e-06,
      "loss": 1.4356,
      "step": 2720
    },
    {
      "epoch": 4.595959595959596,
      "grad_norm": 9.070026397705078,
      "learning_rate": 1.6426572649021476e-06,
      "loss": 1.2717,
      "step": 2730
    },
    {
      "epoch": 4.6127946127946124,
      "grad_norm": 9.794612884521484,
      "learning_rate": 1.5109121330256493e-06,
      "loss": 1.2425,
      "step": 2740
    },
    {
      "epoch": 4.62962962962963,
      "grad_norm": 5.1212873458862305,
      "learning_rate": 1.3845923392986571e-06,
      "loss": 1.3396,
      "step": 2750
    },
    {
      "epoch": 4.646464646464646,
      "grad_norm": 6.653864860534668,
      "learning_rate": 1.2637120173670358e-06,
      "loss": 1.5403,
      "step": 2760
    },
    {
      "epoch": 4.6632996632996635,
      "grad_norm": 7.150104999542236,
      "learning_rate": 1.1482846922660162e-06,
      "loss": 1.3843,
      "step": 2770
    },
    {
      "epoch": 4.68013468013468,
      "grad_norm": 7.374744415283203,
      "learning_rate": 1.0383232789069385e-06,
      "loss": 1.1784,
      "step": 2780
    },
    {
      "epoch": 4.696969696969697,
      "grad_norm": 8.733792304992676,
      "learning_rate": 9.338400806321978e-07,
      "loss": 1.2751,
      "step": 2790
    },
    {
      "epoch": 4.713804713804714,
      "grad_norm": 10.706399917602539,
      "learning_rate": 8.348467878386835e-07,
      "loss": 1.3369,
      "step": 2800
    },
    {
      "epoch": 4.730639730639731,
      "grad_norm": 7.042067527770996,
      "learning_rate": 7.413544766697367e-07,
      "loss": 1.4321,
      "step": 2810
    },
    {
      "epoch": 4.747474747474747,
      "grad_norm": 10.98060417175293,
      "learning_rate": 6.533736077758868e-07,
      "loss": 1.3126,
      "step": 2820
    },
    {
      "epoch": 4.764309764309765,
      "grad_norm": 11.159470558166504,
      "learning_rate": 5.7091402514442e-07,
      "loss": 1.43,
      "step": 2830
    },
    {
      "epoch": 4.781144781144781,
      "grad_norm": 9.863370895385742,
      "learning_rate": 4.939849549979503e-07,
      "loss": 1.4916,
      "step": 2840
    },
    {
      "epoch": 4.797979797979798,
      "grad_norm": 8.65966510772705,
      "learning_rate": 4.2259500476214407e-07,
      "loss": 1.2774,
      "step": 2850
    },
    {
      "epoch": 4.814814814814815,
      "grad_norm": 5.909444332122803,
      "learning_rate": 3.5675216210262484e-07,
      "loss": 1.3041,
      "step": 2860
    },
    {
      "epoch": 4.831649831649831,
      "grad_norm": 9.644746780395508,
      "learning_rate": 2.964637940312764e-07,
      "loss": 1.3325,
      "step": 2870
    },
    {
      "epoch": 4.848484848484849,
      "grad_norm": 9.014927864074707,
      "learning_rate": 2.4173664608193593e-07,
      "loss": 1.2733,
      "step": 2880
    },
    {
      "epoch": 4.865319865319865,
      "grad_norm": 11.867165565490723,
      "learning_rate": 1.9257684155566947e-07,
      "loss": 1.3784,
      "step": 2890
    },
    {
      "epoch": 4.882154882154882,
      "grad_norm": 8.332916259765625,
      "learning_rate": 1.4898988083565933e-07,
      "loss": 1.2529,
      "step": 2900
    },
    {
      "epoch": 4.898989898989899,
      "grad_norm": 9.89289379119873,
      "learning_rate": 1.109806407717462e-07,
      "loss": 1.3352,
      "step": 2910
    },
    {
      "epoch": 4.915824915824916,
      "grad_norm": 10.827881813049316,
      "learning_rate": 7.855337413479879e-08,
      "loss": 1.4172,
      "step": 2920
    },
    {
      "epoch": 4.9326599326599325,
      "grad_norm": 10.352798461914062,
      "learning_rate": 5.171170914086143e-08,
      "loss": 1.4065,
      "step": 2930
    },
    {
      "epoch": 4.94949494949495,
      "grad_norm": 8.986812591552734,
      "learning_rate": 3.04586490452119e-08,
      "loss": 1.3712,
      "step": 2940
    },
    {
      "epoch": 4.966329966329966,
      "grad_norm": 9.956207275390625,
      "learning_rate": 1.4796571806341419e-08,
      "loss": 1.431,
      "step": 2950
    },
    {
      "epoch": 4.983164983164983,
      "grad_norm": 9.657472610473633,
      "learning_rate": 4.727229819856405e-09,
      "loss": 1.4129,
      "step": 2960
    },
    {
      "epoch": 5.0,
      "grad_norm": 7.223636150360107,
      "learning_rate": 2.5174972244634833e-10,
      "loss": 1.1881,
      "step": 2970
    },
    {
      "epoch": 5.0,
      "step": 2970,
      "total_flos": 5.122040301531955e+16,
      "train_loss": 1.6439159518540507,
      "train_runtime": 580.8044,
      "train_samples_per_second": 20.446,
      "train_steps_per_second": 5.114
    }
  ],
  "logging_steps": 10,
  "max_steps": 2970,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500.0,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": false,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 5.122040301531955e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
